{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *“数据分析 ”至“模型融合”为参考代码*\n",
    "\n",
    "参考出处：https://tianchi.aliyun.com/forum/postDetail?spm=5176.12586969.1002.21.3cf2170epYn0xP&postId=188681"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRsT7Gicm0Ub"
   },
   "source": [
    "*赛题：心电图心跳信号多分类预测*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-vCh1pU9wO9"
   },
   "source": [
    "# *Load Google Drive*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34489,
     "status": "ok",
     "timestamp": 1620872024266,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "aDVfAsg3x0aU",
    "outputId": "aa17f9d6-a82a-482b-94c3-b4b2dfd179de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EC3AiZtP0fk"
   },
   "source": [
    "# 问题分析（Task_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEtSY1IUeNSV"
   },
   "source": [
    "典型的单变量时间序列分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTT1V2Z1m0Ur"
   },
   "source": [
    "# 数据分析（Task_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wk5mPc5Cm0Us"
   },
   "source": [
    "## 通用流程\n",
    "\n",
    "1. 载入数据科学和可视化的包：\n",
    "    * pandas, numpy, scipy, matplotlib, seaborn\n",
    "2. 加载数据：\n",
    "    * 加载train和test数据集，简略观察数据（.head()和.shape）\n",
    "3. 数据总览：\n",
    "    * 通过.describe()熟悉数据相关的统计量， 通过.info()熟悉数据类型\n",
    "4. 判断数据缺失和异常：\n",
    "    * 按列查看nan值存在情况，异常值检测\n",
    "5. 了解预测值的分布：\n",
    "    * 总体分布，查看skewness和kurtosis，预测值的具体频数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RosBCVBbm0Ut"
   },
   "source": [
    "必要的配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5068,
     "status": "ok",
     "timestamp": 1618089016618,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "m2DSFYA0m0Uu",
    "outputId": "1dfc1b62-eb3b-4e26-b2bd-494121150f76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: missingno in /usr/local/lib/python3.7/dist-packages (0.4.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from missingno) (1.19.5)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from missingno) (3.2.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from missingno) (0.11.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from missingno) (1.4.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->missingno) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->missingno) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->missingno) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->missingno) (2.8.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->missingno) (1.1.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->missingno) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->missingno) (2018.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install missingno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2COT4_nm0Uw"
   },
   "source": [
    "## 2. 代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EXLLxSYsKqv"
   },
   "source": [
    "### 1 载入数据科学库和可视化库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pxdjxr6Vm0Uw"
   },
   "outputs": [],
   "source": [
    "### 1 载入数据科学库和可视化库\n",
    "# coding:utf-8\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # 过滤并忽略警告语句\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import missingno as msno\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUoSHe31sTU_"
   },
   "source": [
    "### 2 载入train和test数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15624,
     "status": "ok",
     "timestamp": 1618548239219,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "v_qYmeq8m0Ux",
    "outputId": "14c35bc3-130f-4b30-f426-fd19ee04ca11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train首尾数据及shape：\n",
      "           id                                  heartbeat_signals  label\n",
      "0          0  0.9912297987616655,0.9435330436439665,0.764677...    0.0\n",
      "1          1  0.9714822034884503,0.9289687459588268,0.572932...    0.0\n",
      "2          2  1.0,0.9591487564065292,0.7013782792997189,0.23...    2.0\n",
      "3          3  0.9757952826275774,0.9340884687738161,0.659636...    0.0\n",
      "4          4  0.0,0.055816398940721094,0.26129357194994196,0...    2.0\n",
      "99995  99995  1.0,0.677705342021188,0.22239242747868546,0.25...    0.0\n",
      "99996  99996  0.9268571578157265,0.9063471198026871,0.636993...    2.0\n",
      "99997  99997  0.9258351628306013,0.5873839035878395,0.633226...    3.0\n",
      "99998  99998  1.0,0.9947621698382489,0.8297017704865509,0.45...    2.0\n",
      "99999  99999  0.9259994004527861,0.916476635326053,0.4042900...    0.0 \n",
      " (100000, 3)\n",
      "Test首尾数据及shape：\n",
      "            id                                  heartbeat_signals\n",
      "0      100000  0.9915713654170097,1.0,0.6318163407681274,0.13...\n",
      "1      100001  0.6075533139615096,0.5417083883163654,0.340694...\n",
      "2      100002  0.9752726292239277,0.6710965234906665,0.686758...\n",
      "3      100003  0.9956348033996116,0.9170249621481004,0.521096...\n",
      "4      100004  1.0,0.8879490481178918,0.745564725322326,0.531...\n",
      "19995  119995  1.0,0.8330283177934747,0.6340472606311671,0.63...\n",
      "19996  119996  1.0,0.8259705825857048,0.4521053488322387,0.08...\n",
      "19997  119997  0.951744840752379,0.9162611283848351,0.6675251...\n",
      "19998  119998  0.9276692903808186,0.6771898159607004,0.242906...\n",
      "19999  119999  0.6653212231837624,0.527064114047737,0.5166625... \n",
      " (20000, 2)\n"
     ]
    }
   ],
   "source": [
    "### 2 载入train和test数据集\n",
    "#----------------------- 本地加载 -----------------------\n",
    "Train_data = pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/train.csv')\n",
    "Test_data = pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/testA.csv')\n",
    "\n",
    "\n",
    "#----------------------- Web端加载 -----------------------\n",
    "# 从tmp网盘加载文件进行查看数据（限本地jupyter notebook）\n",
    "import csv\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from urllib import request\n",
    "import pandas as pd\n",
    "\n",
    "# 基于tmp.link分享链接直接加载文件\n",
    "def get_data_from_tmp_web(url):\n",
    "  file_mark = url[-13:]\n",
    "  base_url = 'https://tmplinkapp-connect.vx-cdn.com/connect-tz6rhexhflovcrsiq7w5-'\n",
    "  download_link = base_url + file_mark\n",
    "\n",
    "  s = request.urlopen(download_link).read().decode('utf8')  # 1 读取数据串\n",
    "\n",
    "  dfile = StringIO(s)      # 2 将字符串转换为 StringIO对象，使其具有文件属性 \n",
    "  creader = csv.reader(dfile)  # 3 将流 转换为可迭代的 reader（csv row）\n",
    "  dlists=[rw for rw in creader]  # 4 其他转换、操作\n",
    "\n",
    "  temp = []\n",
    "  for i in range(len(dlists)):\n",
    "    row_int = [float(j) for j in dlists[i][0].split( )]\n",
    "    temp.append(row_int)\n",
    "\n",
    "  df = pd.DataFrame(temp, columns=['voltage_input', 'voltage_output', 'value_tachometer'])\n",
    "  # print(df.head())\n",
    "\n",
    "  return df\n",
    "\n",
    "Train_data = get_data_from_tmp_web('http://tmp.link/f/60b15fc318681')\n",
    "Test_data = get_data_from_tmp_web('http://tmp.link/f/60b15fc193212')\n",
    "\n",
    "\n",
    "# 观察首尾数据\n",
    "print('Train首尾数据及shape：\\n', Train_data.head().append(Train_data.tail()),'\\n',  Train_data.shape)\n",
    "\n",
    "print('Test首尾数据及shape：\\n', Test_data.head().append(Test_data.tail()), '\\n', Test_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JQFgq57sZjf"
   },
   "source": [
    "### 3 总览数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 794,
     "status": "ok",
     "timestamp": 1618548243299,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "VuTDTqeVm0Uz",
    "outputId": "c0dc9590-ebc2-4c64-fec8-26240b4cdae2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据的统计量：\n",
      "                   id          label\n",
      "count  100000.000000  100000.000000\n",
      "mean    49999.500000       0.856960\n",
      "std     28867.657797       1.217084\n",
      "min         0.000000       0.000000\n",
      "25%     24999.750000       0.000000\n",
      "50%     49999.500000       0.000000\n",
      "75%     74999.250000       2.000000\n",
      "max     99999.000000       3.000000\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   id                 100000 non-null  int64  \n",
      " 1   heartbeat_signals  100000 non-null  object \n",
      " 2   label              100000 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 2.3+ MB\n",
      "数据类型信息：\n",
      " None\n",
      "数据的统计量：\n",
      "                   id\n",
      "count   20000.000000\n",
      "mean   109999.500000\n",
      "std      5773.647028\n",
      "min    100000.000000\n",
      "25%    104999.750000\n",
      "50%    109999.500000\n",
      "75%    114999.250000\n",
      "max    119999.000000\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   id                 20000 non-null  int64 \n",
      " 1   heartbeat_signals  20000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 312.6+ KB\n",
      "数据类型信息：\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "### 3 总览数据（ data.describe() --> 获取数据的统计量， data.info() --> 获取数据类型\n",
    "print('数据的统计量：\\n', Train_data.describe())\n",
    "print('数据类型信息：\\n', Train_data.info())\n",
    "\n",
    "print('数据的统计量：\\n', Test_data.describe())\n",
    "print('数据类型信息：\\n', Test_data.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23pXzysHsc-1"
   },
   "source": [
    "### 4 判断数据缺失和异常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 911,
     "status": "ok",
     "timestamp": 1618548247956,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "j7QlmdH-m0U0",
    "outputId": "26d3915c-def5-4003-dd60-904bd3af6294"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据缺失与异常情况：\n",
      " id                   0\n",
      "heartbeat_signals    0\n",
      "label                0\n",
      "dtype: int64\n",
      "数据缺失与异常情况：\n",
      " id                   0\n",
      "heartbeat_signals    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "### 4 判断数据缺失和异常（data.isnull().sun() --> 按列查看nan存在情况）\n",
    "print('数据缺失与异常情况：\\n', Train_data.isnull().sum())\n",
    "print('数据缺失与异常情况：\\n', Test_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIaHJhyvsgX8"
   },
   "source": [
    "### 5 了解预测值的分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 10644,
     "status": "ok",
     "timestamp": 1618548278707,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "X6lGxxRtm0U1",
    "outputId": "aa0be59d-61af-4583-ef5a-dfc26ac819e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    64327\n",
      "3.0    17912\n",
      "2.0    14199\n",
      "1.0     3562\n",
      "Name: label, dtype: int64\n",
      "Skewness: 0.871005\n",
      "Kurtness: -1.009573\n",
      "偏度：\n",
      " id       0.000000\n",
      "label    0.871005\n",
      "dtype: float64 \n",
      "峰度：\n",
      " id      -1.200000\n",
      "label   -1.009573\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZAklEQVR4nO3de7SddX3n8fdHws0LghCRktRQTbXYWVw8xThOXV4QAl5Cpw6jtSbDosbxVqsdLVqnINpWx1UvdJRVFCVYR8QrkQFjRCwzbbmcCOUqJVpZJAUSDTehgsB3/ti/M2zDSbLz5Ox9cjjv11p77ef5/n7Ps38PBD557qkqJEnq4nHTPQBJ0sxliEiSOjNEJEmdGSKSpM4MEUlSZ4aIJKkzQ0QSAEnOSvLB6R6HZhZDRAKS/DjJkUP+je8l+XmS+X21I5P8eJi/Kw2TISKN1r3Af5+KFSXZZSrWI+0IQ0TaiiS7J/l4kn9tn48n2b2v/d1Jbm1tf5CkkjxzK6s8DXhtkmds4fd+o+2x3JnkuiSv6ms7K8npSS5Ici/w4rYH9a4kVye5N8mZSfZPcmGSe5J8J8k+fev4cpLbktyV5JIkz5mCf0yaxQwRaev+FFgEHAocAhwBvA8gyWLgncCRwDOBFw2wvvXAp4H3b96QZFfgm8C3gacCbwO+kORZfd1+D/hz4EnA/2213wVeBvw68ErgQuC9wFx6/43/Yd/yFwIL2/q/D3xhgDFLW2SISFv3OuDUqtpQVRvp/c//9a3teOBzVXVdVd0HnDLgOv8SeOUkewGLgCcCH6qqB6rqu8D5wGv7+pxXVX9fVQ9X1c9b7a+r6vaqWg/8H+CyqrqytX8dOGxi4ar6bFXdU1X3t/EekuTJA45behRDRNq6XwFu7pu/udUm2m7pa+uf3qIWRv8TOHWS37qlqh7e7PcO3MZv3N43/W+TzD8ReudQknwoyQ+T3A38uPXZb5BxS5MxRKSt+1fg6X3zv9pqALcC8/ra5jO4jwAvBp672W/NT9L/3+Wv0jsENmFHHrv9e8ASeoffngwsaPXswDo1yxki0iN2TbJH32cO8EXgfUnmJtkP+DPgb1v/c4ET2snwx7MdV11V1Z3AXwHv7itfBtwHvDvJrkleRO8cxzk7vGU9TwLuB34KPB74iylar2YxQ0R6xAX0Dv9MfE4BPgiMA1cD19A7Gf1BgKq6kN7VVhcDa4FL23ruH/D3PgE8NDFTVQ/QC41jgJ8AnwKWVtUPdmCb+p1N7/DYeuD6vvFKncWXUklTI8lvANcCu1fVg9M9HmkU3BORdkCS32n3kuwDfBj4pgGi2cQQkXbMG4ENwA/pHZp60/QORxotD2dJkjpzT0SS1Nmc6R7AqO233361YMGC6R6GJM0Ya9as+UlVzZ2sbdaFyIIFCxgfH5/uYUjSjJHk5i21eThLktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSpM0NEktTZrLtjXTNQpvDtrT5wVJpS7olIkjozRCRJnRkikqTODBFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjobaogk2TvJV5L8IMkNSZ6f5ClJVie5qX3v0/omyWlJ1ia5OsnhfetZ1vrflGRZX/25Sa5py5yWTOWtzZKkbRn2nsgngG9V1bOBQ4AbgJOAi6pqIXBRmwc4BljYPsuB0wGSPAU4GXgecARw8kTwtD5v6Ftu8ZC3R5LUZ2ghkuTJwAuBMwGq6oGquhNYAqxo3VYAx7XpJcDZ1XMpsHeSA4CjgdVVtamq7gBWA4tb215VdWlVFXB237okSSMwzD2Rg4CNwOeSXJnkM0meAOxfVbe2PrcB+7fpA4Fb+pZf12pbq6+bpC5JGpFhhsgc4HDg9Ko6DLiXRw5dAdD2IIb+WNUky5OMJxnfuHHjsH9OkmaNYYbIOmBdVV3W5r9CL1Rub4eiaN8bWvt6YH7f8vNabWv1eZPUH6Wqzqiqsaoamzt37g5tlCTpEUMLkaq6DbglybNa6aXA9cBKYOIKq2XAeW16JbC0XaW1CLirHfZaBRyVZJ92Qv0oYFVruzvJonZV1tK+dUmSRmDYL6V6G/CFJLsBPwJOoBdc5yY5EbgZOL71vQA4FlgL3Nf6UlWbknwAuKL1O7WqNrXpNwNnAXsCF7aPJGlEUrPsTW9jY2M1Pj4+3cPQ9vDNhtK0SrKmqsYma/OOdUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM4MEUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM4MEUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM6GGiJJfpzkmiRXJRlvtackWZ3kpva9T6snyWlJ1ia5OsnhfetZ1vrflGRZX/25bf1r27IZ5vZIkn7ZKPZEXlxVh1bVWJs/CbioqhYCF7V5gGOAhe2zHDgdeqEDnAw8DzgCOHkieFqfN/Qtt3j4myNJmjAdh7OWACva9ArguL762dVzKbB3kgOAo4HVVbWpqu4AVgOLW9teVXVpVRVwdt+6JEkjMOwQKeDbSdYkWd5q+1fVrW36NmD/Nn0gcEvfsutabWv1dZPUHyXJ8iTjScY3bty4I9sjSeozZ8jr/w9VtT7JU4HVSX7Q31hVlaSGPAaq6gzgDICxsbGh/54kzRZD3ROpqvXtewPwdXrnNG5vh6Jo3xta9/XA/L7F57Xa1urzJqlLkkZkaCGS5AlJnjQxDRwFXAusBCausFoGnNemVwJL21Vai4C72mGvVcBRSfZpJ9SPAla1truTLGpXZS3tW5ckaQSGeThrf+Dr7arbOcD/qqpvJbkCODfJicDNwPGt/wXAscBa4D7gBICq2pTkA8AVrd+pVbWpTb8ZOAvYE7iwfSRJI5LehU2zx9jYWI2Pj0/3MLQ9pvL2n1n2512aCknW9N2m8Uu8Y12S1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM4MEUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM4MEUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM4MEUlSZ0MPkSS7JLkyyflt/qAklyVZm+RLSXZr9d3b/NrWvqBvHe9p9RuTHN1XX9xqa5OcNOxtkST9slHsibwduKFv/sPAx6rqmcAdwImtfiJwR6t/rPUjycHAa4DnAIuBT7Vg2gX4JHAMcDDw2tZXkjQiQw2RJPOAlwOfafMBXgJ8pXVZARzXppe0eVr7S1v/JcA5VXV/Vf0LsBY4on3WVtWPquoB4JzWV5I0IsPeE/k48G7g4Ta/L3BnVT3Y5tcBB7bpA4FbAFr7Xa3//69vtsyW6o+SZHmS8STjGzdu3NFtkiQ1QwuRJK8ANlTVmmH9xqCq6oyqGquqsblz5073cCTpMWPOENf9AuBVSY4F9gD2Aj4B7J1kTtvbmAesb/3XA/OBdUnmAE8GftpXn9C/zJbqkqQRGNqeSFW9p6rmVdUCeifGv1tVrwMuBl7dui0DzmvTK9s8rf27VVWt/pp29dZBwELgcuAKYGG72mu39hsrh7U9kqRHGyhEknwtycuTTEXo/AnwziRr6Z3zOLPVzwT2bfV3AicBVNV1wLnA9cC3gLdU1UNtT+atwCp6V3+d2/pKkkYkvb/sb6NTciRwArAI+DLwuaq6cchjG4qxsbEaHx+f7mFoeyRTt64B/rxL+mVJ1lTV2GRtA+1ZVNV32qGow4EfA99J8g9JTkiy69QNVZI0kwx8eCrJvsB/Af4AuJLeSfLDgdVDGZkkaac30NVZSb4OPAv4PPDKqrq1NX0piceGJGmWGvQS309X1QX9hSS7t7vIJz1OJkl67Bv0cNYHJ6n941QORJI082x1TyTJ0+g9SmTPJIcBE5fJ7AU8fshjkyTt5LZ1OOtoeifT5wEf7avfA7x3SGOSJM0QWw2RqloBrEjyu1X11RGNSZI0Q2zrcNbvV9XfAguSvHPz9qr66CSLSZJmiW0dznpC+37isAciSZp5tnU462/a9/tHMxxJ0kwy6AMY/0eSvZLsmuSiJBuT/P6wBydJ2rkNep/IUVV1N/AKes/OeibwrmENSpI0MwwaIhOHvV4OfLmq7hrSeCRJM8igjz05P8kPgH8D3pRkLvDz4Q1LkjQTDPoo+JOAfw+MVdUvgHuBJcMcmCRp57c971h/Nr37RfqXOXuKxyNJmkEGfRT854FnAFcBD7VyYYhI0qw26J7IGHBwDfIuXUnSrDHo1VnXAk8b5kAkSTPPoHsi+wHXJ7kcuH+iWFWvGsqoJEkzwqAhcsowByFJmpkGCpGq+rskTwcWVtV3kjwe2GW4Q5Mk7ewGfXbWG4CvAH/TSgcC39jGMnskuTzJPyW5Lsn7W/2gJJclWZvkS0l2a/Xd2/za1r6gb13vafUbkxzdV1/camuTnLQ9Gy5J2nGDnlh/C/AC4G6AqroJeOo2lrkfeElVHQIcCixOsgj4MPCxqnomcAdwYut/InBHq3+s9SPJwcBrgOcAi4FPJdklyS7AJ4FjgIOB17a+kqQRGTRE7q+qByZm2g2HW73ct3p+1mZ3bZ8CXkJvrwZgBXBcm17S5mntL02SVj+nqu6vqn8B1gJHtM/aqvpRG9s5eBe9JI3UoCHyd0neC+yZ5GXAl4FvbmuhtsdwFbABWA38ELizqh5sXdbROzRG+74FoLXfBezbX99smS3VJxvH8iTjScY3btw4wOZKkgYxaIicBGwErgHeCFwAvG9bC1XVQ1V1KDCP3p7DszuOc4dU1RlVNVZVY3Pnzp2OIUjSY9KgV2c9nOQbwDeqarv/Kl9Vdya5GHg+sHeSOW1vYx6wvnVbD8wH1rXDZU8GftpXn9C/zJbqkqQR2OqeSHpOSfIT4EbgxvZWwz/b1oqTzE2yd5veE3gZcANwMfDq1m0ZcF6bXtnmae3fbY9ZWQm8pl29dRCwELgcuAJY2K722o3eyfeVg264JGnHbWtP5B30rsr6rXZSmyS/Bpye5B1V9bGtLHsAsKJdRfU44NyqOj/J9cA5ST4IXAmc2fqfCXw+yVpgE71QoKquS3IucD3wIPCWqnqojeWtwCp696x8tqqu287tlyTtgGztmYpJrgReVlU/2aw+F/h2VR025PFNubGxsRofH5/uYWh7JFO3Lp8hKm23JGuqamyytm2dWN918wABaOdFdp2KwUmSZq5thcgDHdskSbPAts6JHJLk7knqAfYYwngkSTPIVkOkqnzIoiRpiwa92VCSpEcxRCRJnRkikqTODBFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTOhhYiSeYnuTjJ9UmuS/L2Vn9KktVJbmrf+7R6kpyWZG2Sq5Mc3reuZa3/TUmW9dWfm+SatsxpSTKs7ZEkPdow90QeBP64qg4GFgFvSXIwcBJwUVUtBC5q8wDHAAvbZzlwOvRCBzgZeB5wBHDyRPC0Pm/oW27xELdHkrSZoYVIVd1aVd9v0/cANwAHAkuAFa3bCuC4Nr0EOLt6LgX2TnIAcDSwuqo2VdUdwGpgcWvbq6ouraoCzu5blyRpBEZyTiTJAuAw4DJg/6q6tTXdBuzfpg8EbulbbF2rba2+bpL6ZL+/PMl4kvGNGzfu0LZIkh4x9BBJ8kTgq8AfVdXd/W1tD6KGPYaqOqOqxqpqbO7cucP+OUmaNYYaIkl2pRcgX6iqr7Xy7e1QFO17Q6uvB+b3LT6v1bZWnzdJXZI0IsO8OivAmcANVfXRvqaVwMQVVsuA8/rqS9tVWouAu9phr1XAUUn2aSfUjwJWtba7kyxqv7W0b12SpBGYM8R1vwB4PXBNkqta7b3Ah4Bzk5wI3Awc39ouAI4F1gL3AScAVNWmJB8Armj9Tq2qTW36zcBZwJ7Ahe0jSRqR9E5LzB5jY2M1Pj4+3cPQ9pjK239m2Z93aSokWVNVY5O1ece6JKmzYR7OkqTZYar2lmfgnrJ7IpKkzgwRSVJnhogkqTNDRJLUmSEiSerMEJEkdWaISJI6M0QkSZ0ZIpKkzgwRSVJnhogkqTOfnSVpuHwK82OaeyKSpM4MEUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM6GFiJJPptkQ5Jr+2pPSbI6yU3te59WT5LTkqxNcnWSw/uWWdb635RkWV/9uUmuacuclkzlbbGSpEEMc0/kLGDxZrWTgIuqaiFwUZsHOAZY2D7LgdOhFzrAycDzgCOAkyeCp/V5Q99ym/+WJGnIhhYiVXUJsGmz8hJgRZteARzXVz+7ei4F9k5yAHA0sLqqNlXVHcBqYHFr26uqLq2qAs7uW5ckaURGfU5k/6q6tU3fBuzfpg8Ebunrt67VtlZfN0ldkjRC03Zive1BjOSRnEmWJxlPMr5x48ZR/KQkzQqjDpHb26Eo2veGVl8PzO/rN6/VtlafN0l9UlV1RlWNVdXY3Llzd3gjJEk9ow6RlcDEFVbLgPP66kvbVVqLgLvaYa9VwFFJ9mkn1I8CVrW2u5MsaldlLe1blyRpRIb2UqokXwReBOyXZB29q6w+BJyb5ETgZuD41v0C4FhgLXAfcAJAVW1K8gHgitbv1KqaOFn/ZnpXgO0JXNg+kqQRSs2yN4WNjY3V+Pj4dA9D28M3481ss+Hf31Rt4066fUnWVNXYZG3esS5J6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSps6G9lErSgGbD+zb0mOWeiCSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmczPkSSLE5yY5K1SU6a7vFI0mwyo0MkyS7AJ4FjgIOB1yY5eHpHJUmzR2oGPyYhyfOBU6rq6Db/HoCq+sstLTM2Nlbj4+MjGuHscPlf/Weec8eq4f3AVP4RncInjEwZt29wO+P2wdRt4xC3764Hd+NXPrSh07JJ1lTV2GRtM/3ZWQcCt/TNrwOet3mnJMuB5W32Z0luHMHYhmE/4CfTPYhp4HbPPrN124e73R/unFJP31LDTA+RgVTVGcAZ0z2OHZVkfEt/G3gsc7tnn9m67TNxu2f0ORFgPTC/b35eq0mSRmCmh8gVwMIkByXZDXgNsHKaxyRJs8aMPpxVVQ8meSuwCtgF+GxVXTfNwxqmGX9IriO3e/aZrds+47Z7Rl+dJUmaXjP9cJYkaRoZIpKkzgyRGSTJf0pyXZKHk8yoywC7mo2PtUny2SQbklw73WMZpSTzk1yc5Pr25/zt0z2mUUmyR5LLk/xT2/b3T/eYBmWIzCzXAv8RuGS6BzIKs/ixNmcBi6d7ENPgQeCPq+pgYBHwllny7xvgfuAlVXUIcCiwOMmiaR7TQAyRGaSqbqiqmXq3fRdHAGur6kdV9QBwDrBkmsc0dFV1CbBpuscxalV1a1V9v03fA9xA76kUj3nV87M2u2v7zIirngwR7cwme6zNrPifymyXZAFwGHDZ9I5kdJLskuQqYAOwuqpmxLbP6PtEHouSfAd42iRNf1pV5416PNKoJXki8FXgj6rq7ukez6hU1UPAoUn2Br6e5Deraqc/L2aI7GSq6sjpHsNOxMfazDJJdqUXIF+oqq9N93imQ1XdmeRieufFdvoQ8XCWdmY+1mYWSRLgTOCGqvrodI9nlJLMbXsgJNkTeBnwg+kd1WAMkRkkye8kWQc8H/jfSYb4Eo/pV1UPAhOPtbkBOPcx/lgbAJJ8EfhH4FlJ1iU5cbrHNCIvAF4PvCTJVe1z7HQPakQOAC5OcjW9vzytrqrzp3lMA/GxJ5KkztwTkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiNRBkp/1TR+b5J+TPH07lj9uFj1cUI9hhoi0A5K8FDgNOKaqbh5wmTnAcfSeTCzNaIaI1FGSFwKfBl5RVT9MsqD/HSBJ/luSU9r095J8PMk48CfAq4CPtBvqntHaP9zeKfHPSX67LbdLko8kuSLJ1Une2OoHJLmkLX9tkt9ufc9q89ckeceo/5lo9vHZWVI3uwPfAF5UVYM+nmK3qhoDSLIQOL+qvtLmAeZU1RHtLu2TgSOBE4G7quq3kuwO/H2Sb9N7r8yqqvrz9t6Vx9N7D8WBVfWbbZ17T9XGSlvinojUzS+Af6D3P/lBfWkb7RMPHFwDLGjTRwFL2yPCLwP2BRbSezTGCW1P59+192/8CPi1JH+dZDEwa56Aq+ljiEjdPAwcDxyR5L2t9iC//N/UHpstc+821nl/+36IR44SBHhbVR3aPgdV1bfbi6teSO+pxmclWVpVdwCHAN8D/ivwmQ7bJW0XQ0TqqKruA14OvK49JPF24KlJ9m2Hnl6xlcXvAZ40wM+sAt7UHpFOkl9P8oR2JdjtVfVpemFxeJL9gMdV1VeB9wGHd944aUCeE5F2QFVtaoeOLgE2AqcCl9PbQ9jauZJzgE8n+UPg1Vvp9xl6h7a+3x6VvpHelV0vAt6V5BfAz4Cl9N76+LkkE385fE/HzZIG5lN8JUmdeThLktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmf/D7JdKZ2dADjQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 5 了解预测值的分布\n",
    "print(Train_data['label'].value_counts())\n",
    "\n",
    "# 1)总体分布概况（无界约翰逊分布等）\n",
    "import scipy.stats as st\n",
    "y = Train_data['label']\n",
    "plt.figure(1)\n",
    "plt.title('Default')\n",
    "sns.distplot(y, rug=True, bins=20)\n",
    "plt.figure(2)\n",
    "plt.title('Normal')\n",
    "sns.distplot(y, kde=False, fit=st.norm)\n",
    "plt.figure(3)\n",
    "plt.title('Log Normal')\n",
    "sns.distplot(y, kde=False, fit=st.lognorm)\n",
    "\n",
    "# 2) 查看skewness and kurtosis --> 偏度和峰度\n",
    "# 偏度系数等于0为正态，大于零为右偏（负偏），小于零为左偏（负偏）；\n",
    "# 峰度系数等于3为正态，大于3为尖峰，小于3为平顶；\n",
    "sns.distplot(y)\n",
    "print('Skewness: %f' % y.skew())\n",
    "print('Kurtness: %f' % y.kurt())\n",
    "\n",
    "print('偏度：\\n', Train_data.skew(), '\\n峰度：\\n', Train_data.kurt())\n",
    "\n",
    "sns.distplot(y.kurt(), color='orange', axlabel='Kurtness')\n",
    "\n",
    "# 3) 查看预测值的具体频数\n",
    "plt.hist(y, orientation='vertical', histtype='bar', color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVXAqPJ5sjwt"
   },
   "source": [
    "### 6 用pandas_profiling做EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14042,
     "status": "ok",
     "timestamp": 1618548301939,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "BiFqqTaUzByQ",
    "outputId": "e7b1f824-5a7d-4006-9868-8d0b31c3f10e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas_profiling\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/12/e2870750c5320116efe7bebd4ae1709cd7e35e3bc23ac8039864b05b9497/pandas_profiling-2.11.0-py2.py3-none-any.whl (243kB)\n",
      "\r",
      "\u001b[K     |█▍                              | 10kB 17.8MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 20kB 22.8MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 30kB 12.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████▍                          | 40kB 9.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 51kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 61kB 9.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 71kB 8.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▊                     | 81kB 8.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 92kB 8.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 102kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 112kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 122kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 133kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 143kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 153kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▌          | 163kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▉         | 174kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 184kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 194kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 204kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 215kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 225kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 235kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 245kB 8.3MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: matplotlib>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (3.2.2)\n",
      "Collecting requests>=2.24.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 6.3MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: jinja2>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (2.11.3)\n",
      "Collecting tangled-up-in-unicode>=0.0.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/d1/58bfbe263494741a47140049b989ad42a8941854e8d34f1af90640c6c9f9/tangled_up_in_unicode-0.0.7-py3-none-any.whl (3.1MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 13.3MB/s \n",
      "\u001b[?25hCollecting tqdm>=4.48.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/8a/34efae5cf9924328a8f34eeb2fdaae14c011462d9f0e3fcded48e1266d1c/tqdm-4.60.0-py2.py3-none-any.whl (75kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 8.7MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: seaborn>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (0.11.1)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=19.3.0 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (20.3.0)\n",
      "Requirement already satisfied, skipping upgrade: missingno>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (0.4.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (1.0.1)\n",
      "Collecting phik>=0.10.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/ce/193e8ddf62d4be643b9b4b20e8e9c63b2f6a20f92778c0410c629f89bdaa/phik-0.11.2.tar.gz (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 35.8MB/s \n",
      "\u001b[?25hCollecting htmlmin>=0.1.12\n",
      "  Downloading https://files.pythonhosted.org/packages/b3/e7/fcd59e12169de19f0131ff2812077f964c6b960e7c09804d30a7bf2ab461/htmlmin-0.1.12.tar.gz\n",
      "Collecting confuse>=1.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/6d/55/b4726d81e5d6509fa3441f770f8a9524612627dc1b2a7d6209d1d20083fe/confuse-1.4.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: ipywidgets>=7.5.1 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (7.6.3)\n",
      "Collecting visions[type_image_path]==0.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/30/b1e70bc55962239c4c3c9660e892be2d8247a882135a3035c10ff7f02cde/visions-0.6.0-py3-none-any.whl (75kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 9.3MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (1.1.5)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas_profiling) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas_profiling) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas_profiling) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas_profiling) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas_profiling) (2020.12.5)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas_profiling) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas_profiling) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas_profiling) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.11.1->pandas_profiling) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from confuse>=1.0.0->pandas_profiling) (3.13)\n",
      "Requirement already satisfied, skipping upgrade: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pandas_profiling) (5.5.0)\n",
      "Requirement already satisfied, skipping upgrade: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pandas_profiling) (5.0.5)\n",
      "Requirement already satisfied, skipping upgrade: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pandas_profiling) (3.5.1)\n",
      "Requirement already satisfied, skipping upgrade: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pandas_profiling) (5.1.3)\n",
      "Requirement already satisfied, skipping upgrade: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pandas_profiling) (4.10.1)\n",
      "Requirement already satisfied, skipping upgrade: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.1->pandas_profiling) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: networkx>=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.6.0->pandas_profiling) (2.5.1)\n",
      "Requirement already satisfied, skipping upgrade: Pillow; extra == \"type_image_path\" in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.6.0->pandas_profiling) (7.1.2)\n",
      "Collecting imagehash; extra == \"type_image_path\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/18/9dbb772b5ef73a3069c66bb5bf29b9fb4dd57af0d5790c781c3f559bcca6/ImageHash-4.2.0-py2.py3-none-any.whl (295kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 48.0MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas_profiling) (2018.9)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.2.0->pandas_profiling) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas_profiling) (4.4.2)\n",
      "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas_profiling) (2.6.1)\n",
      "Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas_profiling) (0.7.5)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas_profiling) (54.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas_profiling) (4.8.0)\n",
      "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas_profiling) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas_profiling) (1.0.18)\n",
      "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.3.1->ipywidgets>=7.5.1->pandas_profiling) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas_profiling) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.1->pandas_profiling) (4.7.1)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.1->pandas_profiling) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->pandas_profiling) (5.1.1)\n",
      "Requirement already satisfied, skipping upgrade: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->pandas_profiling) (5.3.5)\n",
      "Requirement already satisfied, skipping upgrade: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash; extra == \"type_image_path\"->visions[type_image_path]==0.6.0->pandas_profiling) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas_profiling) (0.7.0)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas_profiling) (0.2.5)\n",
      "Requirement already satisfied, skipping upgrade: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas_profiling) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas_profiling) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas_profiling) (5.6.1)\n",
      "Requirement already satisfied, skipping upgrade: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5.1->pandas_profiling) (22.0.3)\n",
      "Requirement already satisfied, skipping upgrade: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas_profiling) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas_profiling) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas_profiling) (1.4.3)\n",
      "Requirement already satisfied, skipping upgrade: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas_profiling) (0.8.4)\n",
      "Requirement already satisfied, skipping upgrade: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas_profiling) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas_profiling) (0.3)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas_profiling) (20.9)\n",
      "Requirement already satisfied, skipping upgrade: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas_profiling) (0.5.1)\n",
      "Building wheels for collected packages: phik, htmlmin\n",
      "  Building wheel for phik (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for phik: filename=phik-0.11.2-cp37-none-any.whl size=1107413 sha256=9b926be9d6a1b275d0a15272ba6980440cc06f63322c10a211381a5bb868e6a4\n",
      "  Stored in directory: /root/.cache/pip/wheels/c0/a3/b0/f27b1cfe32ea131a3715169132ff6d85653789e80e966c3bf6\n",
      "  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for htmlmin: filename=htmlmin-0.1.12-cp37-none-any.whl size=27085 sha256=8ec48c4ea96904174c45990f4f7b01767521ad59a88e3b985d5006d3ae2e7e51\n",
      "  Stored in directory: /root/.cache/pip/wheels/43/07/ac/7c5a9d708d65247ac1f94066cf1db075540b85716c30255459\n",
      "Successfully built phik htmlmin\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: phik 0.11.2 has requirement scipy>=1.5.2, but you'll have scipy 1.4.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: requests, tangled-up-in-unicode, tqdm, phik, htmlmin, confuse, imagehash, visions, pandas-profiling\n",
      "  Found existing installation: requests 2.23.0\n",
      "    Uninstalling requests-2.23.0:\n",
      "      Successfully uninstalled requests-2.23.0\n",
      "  Found existing installation: tqdm 4.41.1\n",
      "    Uninstalling tqdm-4.41.1:\n",
      "      Successfully uninstalled tqdm-4.41.1\n",
      "  Found existing installation: pandas-profiling 1.4.1\n",
      "    Uninstalling pandas-profiling-1.4.1:\n",
      "      Successfully uninstalled pandas-profiling-1.4.1\n",
      "Successfully installed confuse-1.4.0 htmlmin-0.1.12 imagehash-4.2.0 pandas-profiling-2.11.0 phik-0.11.2 requests-2.25.1 tangled-up-in-unicode-0.0.7 tqdm-4.60.0 visions-0.6.0\n"
     ]
    }
   ],
   "source": [
    "# Colab上的pandas_profiling版本有些老旧，需要更新到最新版（具体更新的操作可能需要执行两次）\n",
    "!pip install --upgrade pandas_profiling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "2058430a95174fd698ed6719a0dc4dec",
      "3b215c73f1014eb9b62fd8bf54a80a6c",
      "1a386842f8c04f018f1a6bc82671b03d",
      "f7095d4380494a59a32950ef7ecfd50b",
      "b9ad6debbbb04e3283f8947b4b36f646",
      "17566945d6ab40829d27a2c02d70928e",
      "242ab962063b43c0aa70095148e5476e",
      "532ecb5e555343449e2afef2f7c29971",
      "c5bd0614b2df48728cb82154f6560497",
      "95320707fc8c4c1c85ca85509b222d0b",
      "7fda10bb8e1748f99aeb6fb6df5836f3",
      "c9767270ac404f6d99a4c1465b69f36e",
      "e064dce707a1430fa4dded5095c71204",
      "959b835107294c0992ef46127c7bea96",
      "fb0d4c2ca5a5431bbbd3515f70df71a0",
      "cf4be2286ea84901873eccdd9a22dda9",
      "bfda2581b5f64bc2b02c1932011cc239",
      "a85ffdd82f984c5694b7060afcb639e0",
      "58e5bab52f22484a91c48700156d7cb8",
      "9579edb7b78f4ab595edf541942e2da7",
      "23604b8b0bdf48a281736e7b45fbc1c0",
      "5e64d698c6c1457b9ea0754c5969567f",
      "fe4b0d91c3454918a9e45b7c69dc7c62",
      "9e4a003ecfa8410d9dc4abb882da22c0",
      "66cac038e50d4f2291bbd63409de6b05",
      "e7582ea428f44afa906452124c43ddb7",
      "b35a6dedfd2d4c36bd59d2159515d988",
      "ddcf9de53741443baed426267ff6ecf1",
      "7ccc142146e847ae92869c5a6a3493a7",
      "5d5ebbb51bd741bcbed8d15a9983bf20",
      "7e96301d6b80451fa455bd143aa6f015",
      "577ac48c48a740c9a2470507bb6c3a60",
      "9ce8864d9c19454692dedb787a4276a5",
      "f8a653e1d4c54ee8bf86bd07c866bb65",
      "ac42e4c91e8c4962871b5c5ffea1e382",
      "685edc46ad494e97babff3247f47858c",
      "471313f08acc429ea93aa0614117d019",
      "cb7885884585433ba2e95b16a76f2bf8",
      "b23942e6aaa04c0a92688689c6e7e8d2",
      "932ee9a6254547e483b877e6f6ee3f37",
      "8ca991303adf42318898ea2054235705",
      "6a281ccafcee46e9bddd670357e1fab9",
      "7220a2a95b8f423f90b2c71631aae85e",
      "a3464d70e65740aabb37d9b4daaf2b41"
     ]
    },
    "executionInfo": {
     "elapsed": 31496,
     "status": "ok",
     "timestamp": 1618548341118,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "QiH3FCdRm0U2",
    "outputId": "7215349b-3cb7-46f9-8196-4b5f3da5a9f3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2058430a95174fd698ed6719a0dc4dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9767270ac404f6d99a4c1465b69f36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4b0d91c3454918a9e45b7c69dc7c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a653e1d4c54ee8bf86bd07c866bb65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 6 用pandas_profiling生成数据报告\n",
    "import pandas_profiling\n",
    "pfr = pandas_profiling.ProfileReport(Train_data)\n",
    "pfr.to_file('/content/drive/MyDrive/CLF_of_ECG_signals/Exploratory_Data_Analysis/example.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QE1K2HDsrzn5"
   },
   "source": [
    "# 特征工程（Task_3）\n",
    "\n",
    "特征预处理、涉及Tsfresh的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHc-TPIAsA3A"
   },
   "source": [
    "## 通用流程\n",
    "\n",
    "1. 数据预处理\n",
    "  * 时间序列数据格式处理\n",
    "  * 加入时间步特征time\n",
    "2. 特征工程\n",
    "  * 时间序列特征构造\n",
    "  * 特征筛选\n",
    "  * 实用tsfresh进行实践序列特征处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsIsDb6I4jRW"
   },
   "source": [
    "## 代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3c0pHCi5EVH"
   },
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4927,
     "status": "ok",
     "timestamp": 1618960679640,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "NUAkWPGQx8gb",
    "outputId": "0e90ee74-8b08-4105-ee6f-e391290b4200"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tsfresh==0.17.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/b7/cbbfb02d50a93dbb710a730f168711eb343829e1cdea9f0d001d91aeefd6/tsfresh-0.17.0-py2.py3-none-any.whl (91kB)\n",
      "\r",
      "\u001b[K     |███▋                            | 10kB 14.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████▏                        | 20kB 9.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 30kB 8.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 40kB 7.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 51kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 61kB 5.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 71kB 5.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 81kB 6.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 92kB 3.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tsfresh==0.17.0) (0.5.1)\n",
      "Requirement already satisfied: tqdm>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh==0.17.0) (4.41.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh==0.17.0) (0.10.2)\n",
      "Requirement already satisfied: dask[dataframe]>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh==0.17.0) (2.12.0)\n",
      "Requirement already satisfied: requests>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from tsfresh==0.17.0) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.15.1 in /usr/local/lib/python3.7/dist-packages (from tsfresh==0.17.0) (1.19.5)\n",
      "Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh==0.17.0) (1.1.5)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh==0.17.0) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19.2 in /usr/local/lib/python3.7/dist-packages (from tsfresh==0.17.0) (0.22.2.post1)\n",
      "Requirement already satisfied: distributed>=2.11.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh==0.17.0) (2021.4.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.1->tsfresh==0.17.0) (1.15.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0; extra == \"dataframe\" in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]>=2.9.0->tsfresh==0.17.0) (2021.4.0)\n",
      "Requirement already satisfied: partd>=0.3.10; extra == \"dataframe\" in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]>=2.9.0->tsfresh==0.17.0) (1.2.0)\n",
      "Requirement already satisfied: toolz>=0.7.3; extra == \"dataframe\" in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]>=2.9.0->tsfresh==0.17.0) (0.11.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->tsfresh==0.17.0) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->tsfresh==0.17.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->tsfresh==0.17.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->tsfresh==0.17.0) (2020.12.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->tsfresh==0.17.0) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->tsfresh==0.17.0) (2.8.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.2->tsfresh==0.17.0) (1.0.1)\n",
      "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh==0.17.0) (5.4.8)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh==0.17.0) (54.2.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh==0.17.0) (3.13)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh==0.17.0) (2.3.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh==0.17.0) (1.7.0)\n",
      "Requirement already satisfied: tornado>=5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh==0.17.0) (5.1.1)\n",
      "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh==0.17.0) (7.1.2)\n",
      "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh==0.17.0) (2.0.0)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh==0.17.0) (1.0.2)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh==0.17.0) (1.6.0)\n",
      "Requirement already satisfied: locket in /usr/local/lib/python3.7/dist-packages (from partd>=0.3.10; extra == \"dataframe\"->dask[dataframe]>=2.9.0->tsfresh==0.17.0) (0.2.1)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.11.0->tsfresh==0.17.0) (1.0.1)\n",
      "Installing collected packages: tsfresh\n",
      "  Found existing installation: tsfresh 0.18.0\n",
      "    Uninstalling tsfresh-0.18.0:\n",
      "      Successfully uninstalled tsfresh-0.18.0\n",
      "Successfully installed tsfresh-0.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tsfresh==0.17.0 # 高版本的tsfresh对scipy有要求，Colab升级scipy有困难，所以安装指定低版本的tsfresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCgdQC21JuM7"
   },
   "outputs": [],
   "source": [
    "# 导入必要的包\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tsfresh as tsf\n",
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "# 读取数据\n",
    "Train_data = pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/train.csv')\n",
    "Test_data = pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/testA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 26553,
     "status": "ok",
     "timestamp": 1618960944071,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "5l10kU0p4i1X",
    "outputId": "d3cc2db7-f128-4118-f29d-b7d919907ebe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>heartbeat_signals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.971482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.928969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.572933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.178457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.122962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>201</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>202</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>203</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>204</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  time  heartbeat_signals\n",
       "1    1     0           0.971482\n",
       "1    1     1           0.928969\n",
       "1    1     2           0.572933\n",
       "1    1     3           0.178457\n",
       "1    1     4           0.122962\n",
       "..  ..   ...                ...\n",
       "1    1   200           0.000000\n",
       "1    1   201           0.000000\n",
       "1    1   202           0.000000\n",
       "1    1   203           0.000000\n",
       "1    1   204           0.000000\n",
       "\n",
       "[205 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 数据预处理\n",
    "\n",
    "# 对心电特征进行转列处理，同时为每个心电信号加入时间步特征time\n",
    "train_heartbeat_df = Train_data['heartbeat_signals'].str.split(',',expand=True).stack()\n",
    "# print(train_heartbeat_df.shape)\n",
    "train_heartbeat_df.head().append(train_heartbeat_df.tail())\n",
    "train_heartbeat_df = train_heartbeat_df.reset_index()\n",
    "train_heartbeat_df = train_heartbeat_df.set_index('level_0')\n",
    "train_heartbeat_df.index.name = None\n",
    "train_heartbeat_df.rename(columns={'level_1':'time', 0:'heartbeat_signals'}, inplace=True)\n",
    "train_heartbeat_df['heartbeat_signals'] = train_heartbeat_df['heartbeat_signals'].astype(float)\n",
    "\n",
    "train_heartbeat_df.head().append(train_heartbeat_df.tail())\n",
    "\n",
    "# 将处理后的心电特征加到训练数据中，同时将训练数据label列单独存储\n",
    "train_label = Train_data['label']\n",
    "\n",
    "# 重构Train_data,并保存为train_data\n",
    "train_data = Train_data.copy()\n",
    "train_data = train_data.drop('label',axis=1)\n",
    "train_data = train_data.drop('heartbeat_signals',axis=1)\n",
    "train_data = train_data.join(train_heartbeat_df)\n",
    "\n",
    "train_data.head().append(Train_data.tail())\n",
    "\n",
    "# 查看数据的层级关系\n",
    "train_data[train_data['id']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ulQbboNWtbr"
   },
   "source": [
    "### 特征工程\n",
    "\n",
    "时序分析，大多需要建立时间序列上的特征\n",
    "\n",
    "其他的则需要频域或者其他物理特征\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TypRb4IQLqtW",
    "outputId": "02972b9b-5766-40fe-9c68-224b067c598a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Feature Extraction:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "#### 特征抽取（By tsfresh)\n",
    "from tsfresh import extract_features\n",
    "\n",
    "'''\n",
    "因为extract_features提供了几个特征提取的方式default_fc_parameters：\n",
    "①None、②ComprehensiveFCParameters()、③EfficientFCParameters()、④MinimalFCParameters()\n",
    "③④虽然是基于②计算得来，但计算要快很多，因为④指定了特征计算类型，③去除了耗时的特征计算类型\n",
    "采用④完成特征抽取，以获取小规模的特征集合 --> result: time_cost:02:16, scale:[100000 rows x 9 columns]\n",
    "采用③完成特征抽取，以获取规模不那么大的特征集合 --> result: time_cost: over 12 minutes Storage_cost: over 8GB scale: No result.\n",
    "'''\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, EfficientFCParameters, MinimalFCParameters\n",
    "settings = ComprehensiveFCParameters()\n",
    "train_features = extract_features(train_data, column_id='id', \\\n",
    "                  column_sort='time', default_fc_parameters=settings)\n",
    "print(train_features)\n",
    "\n",
    "train_features.to_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Features_Extract/extract_features_ComprehensiveFCParameters.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4913,
     "status": "ok",
     "timestamp": 1618675839455,
     "user": {
      "displayName": "Gabriel Fernando",
      "photoUrl": "",
      "userId": "06273348089492808596"
     },
     "user_tz": -480
    },
    "id": "aXbbqYVte3fi",
    "outputId": "42b90de3-7714-4ebb-c0b6-577ca5ddcfc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       heartbeat_signals__sum_values  ...  heartbeat_signals__minimum\n",
      "0                          38.927945  ...                         0.0\n",
      "1                          19.445634  ...                         0.0\n",
      "2                          21.192974  ...                         0.0\n",
      "3                          42.113066  ...                         0.0\n",
      "4                          69.756786  ...                         0.0\n",
      "...                              ...  ...                         ...\n",
      "99995                      63.323449  ...                         0.0\n",
      "99996                      69.657534  ...                         0.0\n",
      "99997                      40.897057  ...                         0.0\n",
      "99998                      42.333303  ...                         0.0\n",
      "99999                      53.290117  ...                         0.0\n",
      "\n",
      "[100000 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "### 特征选择（By impute & select_features)\n",
    "'''\n",
    "select_features 检查特征矩阵X的所有特征(列)的重要性，并返回一个可能简化的只包含相关特征的特征矩阵。\n",
    "'''\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "impute(train_features)  # 去除Nan值\n",
    "\n",
    "train_features_filtered = select_features(train_features, train_label)  # \n",
    "print(train_features_filtered)\n",
    "\n",
    "train_features_filtered.to_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Features_Extract/train_features_filtered_MinimalFCParameters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQfwJ3wHjZQW"
   },
   "outputs": [],
   "source": [
    "# 学习和测试extract_features用法\n",
    "import numpy as np\n",
    "import scipy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "x = np.array([i*0.01 for i in range(0, 640, 5)])\n",
    "x_ = [i for i in range(8)]*16\n",
    "print(x_)\n",
    "id = [[i]*8 for i in range(16)]\n",
    "id = [n for i in id for n in i]\n",
    "print(id)\n",
    "\n",
    "ts = [math.sin(i) for i in x]\n",
    "# print(ts)\n",
    "\n",
    "# plt.plot(ts)\n",
    "from tsfresh import extract_features\n",
    "df_sin = pd.DataFrame(ts)\n",
    "df_sin['time_xcc'] = x_\n",
    "df_sin['id'] = id\n",
    "df_sin.rename(columns={0:'value'},inplace=True)\n",
    "df_sin = df_sin.reset_index()\n",
    "df_sin = df_sin.set_index('id',drop=False)\n",
    "\n",
    "print(df_sin.head(20))\n",
    "\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters, EfficientFCParameters, MinimalFCParameters\n",
    "settings = ComprehensiveFCParameters()\n",
    "\n",
    "train_features = extract_features(df_sin, column_id='value',chunksize=10, \\\n",
    "                  column_sort='time_xcc',default_fc_parameters=settings)\n",
    "print(train_features)\n",
    "\n",
    "# 去除全NaN列和全0值列（从1576列特征缩减到58列特征）\n",
    "df_ = train_features.copy()\n",
    "df_.dropna(axis=1,how='any',inplace=True) \n",
    "df_ = df_.loc[:, (df_ != 0.0).any(axis=0)]\n",
    "print(df_.tail())\n",
    "print(df_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8HAvrFBsQjA"
   },
   "source": [
    "# 模型调参（Task_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XAo6JvnsYIP"
   },
   "source": [
    "## 通用流程\n",
    "\n",
    "1. 常用模型介绍：\n",
    "  * 逻辑回归模型\n",
    "  * 树模型\n",
    "  * 集成模型\n",
    "2. 模型建立与性能评估：\n",
    "  * 模型建立\n",
    "  * 评估方法\n",
    "  * 评估结果\n",
    "3. 模型调参：\n",
    "  * 贪心调参\n",
    "  * 网格调参\n",
    "  * 贝叶斯调参\n",
    "\n",
    "1.和2.中的理论知识参见[ *零基础入门数据挖掘-心跳信号分类预测_Task4 模型调参*](https://tianchi.aliyun.com/forum/postDetail?spm=5176.12282027.0.0.7c2d379cVEtyfx&postId=195837)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6E0chQ2uV2j"
   },
   "source": [
    "## 代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwmtrZK-ucjR"
   },
   "source": [
    "### 导入依赖包和和设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CaID69RouIEq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1TRRIWVvKMf"
   },
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIPJXPDRvNEu"
   },
   "outputs": [],
   "source": [
    "# reduce_mem_usage 函数通过调整数据类型，帮助我们减少数据在内存中占用的空间\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2 \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2 \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19785,
     "status": "ok",
     "timestamp": 1618682418272,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "veGNwMQ8veW-",
    "outputId": "ebbfcbe1-a232-4cf0-c500-7f68e3dc3339"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id       s_0       s_1       s_2  ...  s_202  s_203  s_204  label\n",
      "0  0.0  0.991230  0.943533  0.764677  ...    0.0    0.0    0.0    0.0\n",
      "1  1.0  0.971482  0.928969  0.572933  ...    0.0    0.0    0.0    0.0\n",
      "2  2.0  1.000000  0.959149  0.701378  ...    0.0    0.0    0.0    2.0\n",
      "3  3.0  0.975795  0.934088  0.659637  ...    0.0    0.0    0.0    0.0\n",
      "4  4.0  0.000000  0.055816  0.261294  ...    0.0    0.0    0.0    2.0\n",
      "\n",
      "[5 rows x 207 columns]\n",
      "Memory usage of dataframe is 157.93 MB\n",
      "Memory usage after optimization is: 39.67 MB\n",
      "Decreased by 74.9%\n",
      "原始特征数据:\n",
      "             id       s_0       s_1       s_2  ...  s_202  s_203  s_204  label\n",
      "0          0.0  0.991211  0.943359  0.764648  ...    0.0    0.0    0.0    0.0\n",
      "1          1.0  0.971680  0.929199  0.572754  ...    0.0    0.0    0.0    0.0\n",
      "2          2.0  1.000000  0.958984  0.701172  ...    0.0    0.0    0.0    2.0\n",
      "3          3.0  0.975586  0.934082  0.659668  ...    0.0    0.0    0.0    0.0\n",
      "4          4.0  0.000000  0.055817  0.261230  ...    0.0    0.0    0.0    2.0\n",
      "...        ...       ...       ...       ...  ...    ...    ...    ...    ...\n",
      "99995  99995.0  1.000000  0.677734  0.222412  ...    0.0    0.0    0.0    0.0\n",
      "99996  99996.0  0.926758  0.906250  0.637207  ...    0.0    0.0    0.0    2.0\n",
      "99997  99997.0  0.925781  0.587402  0.633301  ...    0.0    0.0    0.0    3.0\n",
      "99998  99998.0  1.000000  0.994629  0.829590  ...    0.0    0.0    0.0    2.0\n",
      "99999  99999.0  0.925781  0.916504  0.404297  ...    0.0    0.0    0.0    0.0\n",
      "\n",
      "[100000 rows x 207 columns]\n",
      "Memory usage of dataframe is 7.06 MB\n",
      "Memory usage after optimization is: 2.10 MB\n",
      "Decreased by 70.3%\n",
      "时序特征数据：\n",
      "           id  heartbeat_signals__sum_values  ...  heartbeat_signals__minimum  label\n",
      "0          0                      38.937500  ...                         0.0    0.0\n",
      "1          1                      19.453125  ...                         0.0    0.0\n",
      "2          2                      21.187500  ...                         0.0    2.0\n",
      "3          3                      42.125000  ...                         0.0    0.0\n",
      "4          4                      69.750000  ...                         0.0    2.0\n",
      "...      ...                            ...  ...                         ...    ...\n",
      "99995  99995                      63.312500  ...                         0.0    0.0\n",
      "99996  99996                      69.687500  ...                         0.0    2.0\n",
      "99997  99997                      40.906250  ...                         0.0    3.0\n",
      "99998  99998                      42.343750  ...                         0.0    2.0\n",
      "99999  99999                      53.281250  ...                         0.0    0.0\n",
      "\n",
      "[100000 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "data = pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/train.csv')\n",
    "data_features_filtered = pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Features_Extract/train_features_filtered_MinimalFCParameters.csv')\n",
    "\n",
    "# 数据预处理(得到用于建模的原始特征DataFrame)\n",
    "data_list = []\n",
    "for items in data.values:\n",
    "  data_list.append([items[0]] + [float(i) for i in items[1].split(',')] + [items[2]])\n",
    "\n",
    "data = pd.DataFrame(np.array(data_list))\n",
    "data.columns = ['id'] + ['s_'+str(i) for i in range(len(data_list[0])-2)] + ['label']\n",
    "print(data.head())\n",
    "data = reduce_mem_usage(data)\n",
    "print('原始特征数据:\\n',data)\n",
    "\n",
    "# 数据预处理（得到用于建模的时序特征DataFrame）\n",
    "data_features_filtered['label'] = data['label']\n",
    "data_features_filtered.rename(columns={'Unnamed: 0':'id'}, inplace=True)\n",
    "# print(data_features_filtered.shape, '\\n', data_features_filtered)\n",
    "data_features_filtered = reduce_mem_usage(data_features_filtered)\n",
    "print('时序特征数据：\\n', data_features_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P33QV5ft1gec"
   },
   "source": [
    "### 数据建模\n",
    "\n",
    "注：以下建模的据集并未构造任何特征，直接使用原特征。本次主要任务还是模建模调参。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPfgfj8afbXP"
   },
   "source": [
    "#### 预操作\n",
    "\n",
    "分离数据，交叉验证设置，自定义评价指标等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ab6ogTbZfzyf"
   },
   "outputs": [],
   "source": [
    "### 建模前的预操作\n",
    "# 导入特征工程的结果\n",
    "'''如果使用时序特征数据执行以下代码'''\n",
    "data = data_features_filtered\n",
    "\n",
    "# 分离数据集，方便进行交叉验证\n",
    "from sklearn.model_selection import KFold\n",
    "X_train = data.drop(['id','label'], axis=1)   \n",
    "y_train = data['label']\n",
    "# 5折交叉验证\n",
    "folds = 5\n",
    "seed = 2021\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "\n",
    "# 自定义评价指标（必要的话） \n",
    "def f1_score_vali(preds, data_vali):\n",
    "  labels = data_vali.get_label()\n",
    "  preds = np.argmax(preds.reshape(4, -1), axis=0)\n",
    "  score_vali = f1_score(y_true=labels, y_pred=preds, average='macro')\n",
    "  return 'f1_score', score_vali, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5vVKPq5f3wl"
   },
   "source": [
    "#### 建模、训练、验证\n",
    "\n",
    "（By lightgbm）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 119088,
     "status": "ok",
     "timestamp": 1618683851648,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "rxKWGuRF1pwk",
    "outputId": "4339b392-095e-422c-b7b4-ed70d6bdf50e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 0.453616\tvalid_0's f1_score: 0.692875\n",
      "[100]\tvalid_0's multi_logloss: 0.439067\tvalid_0's f1_score: 0.700909\n",
      "[150]\tvalid_0's multi_logloss: 0.43467\tvalid_0's f1_score: 0.707905\n",
      "[200]\tvalid_0's multi_logloss: 0.43447\tvalid_0's f1_score: 0.70845\n",
      "[250]\tvalid_0's multi_logloss: 0.435708\tvalid_0's f1_score: 0.711004\n",
      "[300]\tvalid_0's multi_logloss: 0.43692\tvalid_0's f1_score: 0.713863\n",
      "[350]\tvalid_0's multi_logloss: 0.440179\tvalid_0's f1_score: 0.716117\n",
      "Early stopping, best iteration is:\n",
      "[171]\tvalid_0's multi_logloss: 0.433977\tvalid_0's f1_score: 0.708907\n",
      "未调参前lightgbm单模型在验证集上的f1：0.7089068518692501\n"
     ]
    }
   ],
   "source": [
    "### 使用lightgbm建模\n",
    "'''对训练集数据进行划分，分成训练集和验证集，并进行相应操作'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "'''数据集划分'''\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "train_matrix = lgb.Dataset(X_train_split, label=y_train_split)\n",
    "valid_matrix = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'boosting': 'gbdt',\n",
    "    'lambda_l2': 0.1,\n",
    "    'max_depth': -1,\n",
    "    'num_leaves': 128,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'feature_fraction':0.8,\n",
    "    'metric': None,\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 4,\n",
    "    'nthread': 10,\n",
    "    'verbose': -1,\n",
    "}\n",
    "\n",
    "### 使用lightgbm训练\n",
    "'''使用训练集数据进行模型训练'''\n",
    "model = lgb.train(params,\n",
    "          train_set=train_matrix,\n",
    "          valid_sets=valid_matrix,\n",
    "          num_boost_round=2000,   # 决策树提升循环次数\n",
    "          verbose_eval=50,\n",
    "          early_stopping_rounds=200,\n",
    "          feval=f1_score_vali)\n",
    "\n",
    "### 对验证集进行预测\n",
    "'''对验证集进行预测'''\n",
    "val_pre_lgb = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "preds = np.argmax(val_pre_lgb, axis=1)\n",
    "score = f1_score(y_true=y_val, y_pred=preds, average='macro')\n",
    "print('未调参前lightgbm单模型在验证集上的f1：{}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhxgXnOvglKM"
   },
   "source": [
    "### 交叉验证、性能评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 610167,
     "status": "ok",
     "timestamp": 1618684894969,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "Nfql_GLWEF33",
    "outputId": "8c372c9e-55c3-4b94-9282-36ab32ea7536"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************* 1 *********************************\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.435704\tvalid_0's f1_score: 0.698814\n",
      "[200]\tvalid_0's multi_logloss: 0.429774\tvalid_0's f1_score: 0.707801\n",
      "[300]\tvalid_0's multi_logloss: 0.433872\tvalid_0's f1_score: 0.712207\n",
      "Early stopping, best iteration is:\n",
      "[197]\tvalid_0's multi_logloss: 0.429767\tvalid_0's f1_score: 0.707954\n",
      "[0.7079535642548609]\n",
      "********************************* 2 *********************************\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.451238\tvalid_0's f1_score: 0.702416\n",
      "[200]\tvalid_0's multi_logloss: 0.447452\tvalid_0's f1_score: 0.714068\n",
      "[300]\tvalid_0's multi_logloss: 0.452516\tvalid_0's f1_score: 0.717368\n",
      "Early stopping, best iteration is:\n",
      "[171]\tvalid_0's multi_logloss: 0.44672\tvalid_0's f1_score: 0.710834\n",
      "[0.7079535642548609, 0.7108343665821681]\n",
      "********************************* 3 *********************************\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.428793\tvalid_0's f1_score: 0.713201\n",
      "[200]\tvalid_0's multi_logloss: 0.421646\tvalid_0's f1_score: 0.720607\n",
      "[300]\tvalid_0's multi_logloss: 0.422911\tvalid_0's f1_score: 0.724583\n",
      "[400]\tvalid_0's multi_logloss: 0.426857\tvalid_0's f1_score: 0.728771\n",
      "Early stopping, best iteration is:\n",
      "[224]\tvalid_0's multi_logloss: 0.421637\tvalid_0's f1_score: 0.720771\n",
      "[0.7079535642548609, 0.7108343665821681, 0.7207711024968233]\n",
      "********************************* 4 *********************************\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.441161\tvalid_0's f1_score: 0.707731\n",
      "[200]\tvalid_0's multi_logloss: 0.435942\tvalid_0's f1_score: 0.711188\n",
      "[300]\tvalid_0's multi_logloss: 0.438845\tvalid_0's f1_score: 0.715265\n",
      "Early stopping, best iteration is:\n",
      "[179]\tvalid_0's multi_logloss: 0.435719\tvalid_0's f1_score: 0.710751\n",
      "[0.7079535642548609, 0.7108343665821681, 0.7207711024968233, 0.7107508072067897]\n",
      "********************************* 5 *********************************\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.43392\tvalid_0's f1_score: 0.705836\n",
      "[200]\tvalid_0's multi_logloss: 0.428758\tvalid_0's f1_score: 0.71475\n",
      "[300]\tvalid_0's multi_logloss: 0.430772\tvalid_0's f1_score: 0.716584\n",
      "Early stopping, best iteration is:\n",
      "[175]\tvalid_0's multi_logloss: 0.428581\tvalid_0's f1_score: 0.714072\n",
      "[0.7079535642548609, 0.7108343665821681, 0.7207711024968233, 0.7107508072067897, 0.7140722970996672]\n",
      "lgb_scotrainre_list:[0.7079535642548609, 0.7108343665821681, 0.7207711024968233, 0.7107508072067897, 0.7140722970996672]\n",
      "lgb_score_mean:0.7128764275280619\n",
      "lgb_score_std:0.004397245827975971\n"
     ]
    }
   ],
   "source": [
    "### 使用5折交叉验证进行模型性能评估\n",
    "'''使用lightgbm 5折交叉验证进行建模预测'''\n",
    "cv_scores = []\n",
    "for i, (train_index, valid_index) in enumerate(kf.split(X_train, y_train)):\n",
    "  print('********************************* {} *********************************'.format(str(i+1)))\n",
    "  X_train_split, y_train_split, X_val, y_val = \\\n",
    "  X_train.iloc[train_index], y_train[train_index], X_train.iloc[valid_index], y_train[valid_index]\n",
    "\n",
    "  train_matrix = lgb.Dataset(X_train_split, label=y_train_split)\n",
    "  valid_matrix = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "  params = {\n",
    "      \"learning_rate\": 0.1,\n",
    "      \"boosting\": 'gbdt',  \n",
    "      \"lambda_l2\": 0.1,\n",
    "      \"max_depth\": -1,\n",
    "      \"num_leaves\": 128,\n",
    "      \"bagging_fraction\": 0.8,\n",
    "      \"feature_fraction\": 0.8,\n",
    "      \"metric\": None,\n",
    "      \"objective\": \"multiclass\",\n",
    "      \"num_class\": 4,\n",
    "      \"nthread\": 10,\n",
    "      \"verbose\": -1,\n",
    "  }\n",
    "\n",
    "  model = lgb.train(params,\n",
    "            train_set = train_matrix,\n",
    "            valid_sets = valid_matrix,\n",
    "            num_boost_round = 2000,\n",
    "            verbose_eval = 100,\n",
    "            early_stopping_rounds = 200,\n",
    "            feval = f1_score_vali)\n",
    "  \n",
    "  val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "  val_pred = np.argmax(val_pred, axis=1)\n",
    "  cv_scores.append(f1_score(y_true=y_val, y_pred=val_pred, average='macro'))\n",
    "  print(cv_scores)\n",
    "\n",
    "print('lgb_scotrainre_list:{}'.format(cv_scores))\n",
    "print('lgb_score_mean:{}'.format(np.mean(cv_scores)))\n",
    "print('lgb_score_std:{}'.format(np.std(cv_scores)))\n",
    "\n",
    "# time_cost: 14分54秒"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfzJrXaUg_Ro"
   },
   "source": [
    "### 模型调参\n",
    "\n",
    "常见调参参数及调参（for树模型）顺序为：\n",
    "\n",
    "①：max_depth、num_leaves\n",
    "\n",
    "②：min_data_in_leaf、min_child_weight\n",
    "\n",
    "③：bagging_fraction、 feature_fraction、bagging_freq\n",
    "\n",
    "④：reg_lambda、reg_alpha\n",
    "\n",
    "⑤：min_split_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPYYREwviHz5"
   },
   "source": [
    "#### 贪心调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CzGMbru_iFTP"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# 调整参数 objective\n",
    "'''\n",
    "Default: 'regression' for LGBMRegressor, \n",
    "binary' or 'multiclass' for LGBMClassifier, \n",
    "'lambdarank' for LGBMRanker.\n",
    "''' \n",
    "best_obj = dict()\n",
    "objective = ['multiclass']\n",
    "for obj in objective:\n",
    "  model = LGBMClassifier(objective=obj)\n",
    "  '''预测并计算roc的相关指标''' \n",
    "  score = cross_val_score(model, X_train, y_train, cv=5).mean()\n",
    "  best_obj[obj] = score\n",
    "\n",
    "\n",
    "# 调整参数 num_leaves \n",
    "best_leaves = dict()\n",
    "num_leaves = [8,16,32]  #一般默认num_leaves = 2^max_path，所以图快的话（依据前人经验），可以不再对max_depth调参\n",
    "for leaves in num_leaves:\n",
    "  model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0], num_leaves=leaves)\n",
    "  # 预测并计算roc的相关指标\n",
    "  score = cross_val_score(model, X_train, y_train, cv=5).mean()\n",
    "  best_leaves[leaves] = score\n",
    "# >>> 耗时: 8分32秒 运行结果：{8: 0.95691, 16: 0.97081, 32: 0.9797600000000001}\n",
    "\n",
    "# 调整参数 max_path  暂不调参\n",
    "'''\n",
    "best_path = dict()\n",
    "for depth in max_depth:\n",
    "  model = LGBMRegressor(objective = min(best_obj,items(),key=lambda x:x[1])[0],\\\n",
    "              num_leaves = min(best_leaves.items(), key=lambda x:x[1])[0],\\\n",
    "              max_depth = degpth             \n",
    "              )\n",
    "  # 预测并计算roc的相关指标\n",
    "  score = cross_val_score(model, X_train, y_train, cv=5, scoring='f1').mean()\n",
    "  best_depth[depth] = score\n",
    "'''\n",
    "\n",
    "'''\n",
    "按照调参参数和顺序依次调整优化，并通过可视化观察每个最优参数下模型的得分情况\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wIkLuIq7vCI"
   },
   "source": [
    "#### 网格搜索\n",
    "\n",
    "sklearn 提供GridSearchCV用于进行网格搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DWrZkZN70pw"
   },
   "outputs": [],
   "source": [
    "# 通过网格搜索确定最优参数\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def get_best_cv_params(learning_rate = 0.1,\n",
    "            n_estimators = 581,\n",
    "            num_leaves = 31,\n",
    "            max_depth = -1,\n",
    "            bagging_fraction = 1.0,\n",
    "            feature_fraction = 1.0,\n",
    "            bagging_freq = 0,\n",
    "            min_data_in_leaf = 20,\n",
    "            min_child_weight = 0.001,\n",
    "            min_split_gain = 0,\n",
    "            reg_lambda = 0,\n",
    "            reg_alpha = 0,\n",
    "            param_grid = None\n",
    "            ):\n",
    "  # 设置5折交叉验证\n",
    "  cv_fold = KFold(n_splits=5, shuffle=True, random_state=2021)\n",
    "\n",
    "  model_lgb = lgb.LGBMClassifier(learning_rate= learning_rate,\n",
    "                  n_estimators=n_estimators,\n",
    "                  num_leaves=num_leaves,\n",
    "                  max_depth=max_depth,\n",
    "                  bagging_fraction=bagging_fraction,\n",
    "                  feature_fraction=feature_fraction,\n",
    "                  bagging_freq=bagging_freq,\n",
    "                  min_data_in_leaf=min_data_in_leaf,\n",
    "                  min_child_weight=min_child_weight,\n",
    "                  min_split_gain=min_split_gain,\n",
    "                  reg_lambda=reg_lambda,\n",
    "                  reg_alpha=reg_alpha,\n",
    "                  n_jobs = 2\n",
    "                  )\n",
    "  \n",
    "  f1 = make_scorer(f1_score, average='micro')\n",
    "  grid_search = GridSearchCV(estimator = model_lgb,\n",
    "                cv = cv_fold,\n",
    "                param_grid = param_grid,\n",
    "                scoring = f1\n",
    "                )\n",
    "  \n",
    "  grid_search.fit(X_train, y_train)\n",
    "\n",
    "  print('模型当前最优参数为：{}'.format(grid_search.best_params_))\n",
    "  print('模型当前最优得分为：{}'.format(grid_search.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18OpFTynUZKV"
   },
   "outputs": [],
   "source": [
    "''' \n",
    "  基于函数get_best_cv_params()依次调整参数\n",
    "  调参顺序：\n",
    "  第一阶段：num_leaves和max_depth\n",
    "  第二阶段：bagging_fraction、feature_fraction和bagging_freq\n",
    "  第三阶段：reg_lambda、reg_alpha\n",
    "  第四阶段：min_split_gain\n",
    "  第五阶段：num_boost_round\n",
    "'''\n",
    "\n",
    "#——————第一阶段——————\n",
    "# 1.1）预设n_estimators=581,调整num_leaves和max_depth\n",
    "lgb_params = {'num_leaves':range(10,80,5), 'max_depth':range(3,10,2)}  # 粗调\n",
    "get_best_cv_params(learning_rate = 0.1,\n",
    "          n_estimators = 581,\n",
    "          num_leaves = None,   # 被调整的参数\n",
    "          max_depth = None,    # 被调整的参数\n",
    "          bagging_fraction = 1.0,\n",
    "          feature_fraction = 1.0,\n",
    "          bagging_freq = 0,\n",
    "          min_data_in_leaf = 20,\n",
    "          min_child_weight = 0.001,\n",
    "          min_split_gain = 0,\n",
    "          reg_lambda = 0,\n",
    "          reg_alpha = 0,\n",
    "          param_grid = lgb_params  # 集成待调整参数\n",
    "          )\n",
    "# >>> 耗时比较长:15分46秒，执行结果：执行失败（可能是Colab的原因），基于得到粗调的最优配置参数：num_leaves=30，max_depth=7， 基于前人经验（num_leaves≈2^max_depth),只搜索num_leaves的最优值也可实现粗调的目的\n",
    "# >>> 调整范围：lgb_params = {'num_leaves':[16,32], 'max_depth':[4,5]}，计算耗时\n",
    "\n",
    "# 1.2)确定有限区间内的最优配置参数，进一步细调\n",
    "lgb_params = {'num_leaves':range(25,35,1), 'max_depth':range(5,9,1)}  # 粗调\n",
    "get_best_cv_params(learning_rate = 0.1,\n",
    "          n_estimators = 581,\n",
    "          num_leaves = None,   # 依然作为被调整的参数\n",
    "          max_depth = None,   # 依然作为被调整的参数\n",
    "          bagging_fraction = 1.0,\n",
    "          feature_fraction = 1.0,\n",
    "          bagging_freq = 0,\n",
    "          min_data_in_leaf = 20,\n",
    "          min_child_weight = 0.001,\n",
    "          min_split_gain = 0,\n",
    "          reg_lambda = 0,\n",
    "          reg_alpha = 0,\n",
    "          param_grid = lgb_params  # 集成待调整参数\n",
    "          )\n",
    "# --> 得到细调的最优配置参数：num_leaves=29，max_depth=7\n",
    "# 确定后作为预设值输入函数get_best_cv_params，不再修改。\n",
    "\n",
    "#——————第二阶段——————\n",
    "# 2）预设min_data_in_leaf=45，min_child_weight=0.001，调整bagging_fraction、feature_fraction和bagging_freq\n",
    "lgb_params = {'bagging_fraction': [i/10 for i in range(5,10,1)], \n",
    "        'feature_fraction': [i/10 for i in range(5,10,1)],\n",
    "        'bagging_freq': range(0,81,10)\n",
    "        }\n",
    "get_best_cv_params(learning_rate=0.1,\n",
    "          n_estimators=85, \n",
    "          num_leaves=29, \n",
    "          max_depth=7, \n",
    "          min_data_in_leaf=45, \n",
    "          min_child_weight=0.001,\n",
    "          bagging_fraction=None, # 被调整参数\n",
    "          feature_fraction=None, # 被调整参数\n",
    "          bagging_freq=None, # 被调整参数\n",
    "          min_split_gain=0, \n",
    "          reg_lambda=0, \n",
    "          reg_alpha=0, \n",
    "          param_grid=lgb_params\n",
    "          )\n",
    "\n",
    "#——————第三阶段——————\n",
    "# 3）预设bagging_fraction=0.4、feature_fraction=0.6、bagging_freq=40,\n",
    "#   调整reg_lambda, reg_alpha\n",
    "# 代码参考第二阶段的结果进行调整\n",
    "\n",
    "\n",
    "#——————第四阶段——————\n",
    "# 3）基于已经确定的参数，调整min_split_gain\n",
    "# 代码参考第三阶段的结果进行调整\n",
    "\n",
    "\n",
    "#——————第五阶段——————\n",
    "# 3）预设一个较小的learning_rate=0.005，调整num_boost_round\n",
    "cv_fold = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "final_params = {'boosting_type': 'gbdt',\n",
    "         'learning_rate': 0.01,\n",
    "         'num_leaves': 29,\n",
    "         'max_depth': 7,\n",
    "         'objective': 'multiclass',\n",
    "         'num_class': 4,\n",
    "         'min_data_in_leaf':45,\n",
    "         'min_child_weight':0.001,\n",
    "         'bagging_fraction': 0.9,\n",
    "         'feature_fraction': 0.9,\n",
    "         'bagging_freq': 40,\n",
    "         'min_split_gain': 0,\n",
    "         'reg_lambda':0,\n",
    "         'reg_alpha':0,\n",
    "         'nthread': 6\n",
    "         }\n",
    "cv_result = lgb.cv(train_set = lgb_train,\n",
    "           early_stopping_rounds = 20,\n",
    "           num_boost_round = 5000,\n",
    "           nfold=5,\n",
    "           stratified=True,\n",
    "           shuffle=True,\n",
    "           params=final_params,\n",
    "           feval=f1_score_vali,\n",
    "           seed=0,\n",
    "           )\n",
    "'''\n",
    "实际调整过程中参照如下经验：\n",
    "先设置较大的学习率，用cv函数确定出树的个数（？？），利用上面的代码调整优化，\n",
    "再设置较小的学习率，用cv函数确定出树的个数（？？）。确定最终参数。\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PT9dzbUOKR3"
   },
   "source": [
    "#### 贝叶斯调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xPAWXWRDV4s"
   },
   "source": [
    "依赖包：bayesian-optimization\n",
    "\n",
    "贝叶斯调参的步骤如下：\n",
    "  * 定义优化函数(rf_cv）\n",
    "  * 建立模型\n",
    "  * 定义待优化的参数\n",
    "  * 得到优化结果，并返回要优化的分数指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6937,
     "status": "ok",
     "timestamp": 1618584389704,
     "user": {
      "displayName": "Gabriel Fernando",
      "photoUrl": "",
      "userId": "06273348089492808596"
     },
     "user_tz": -480
    },
    "id": "9SsCcP47ecPy",
    "outputId": "1154c9c6-31db-41a3-ebd8-4e395f95c797"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "Collecting bayesian-optimization\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/bb/7a/fd8059a3881d3ab37ac8f72f56b73937a14e8bb14a9733e68cc8b17dbe3c/bayesian-optimization-1.2.0.tar.gz\n",
      "Requirement already satisfied: numpy>=1.9.0 in d:\\users\\xixi\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.16.4)\n",
      "Requirement already satisfied: scipy>=0.14.0 in d:\\users\\xixi\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.2.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in d:\\users\\xixi\\anaconda3\\lib\\site-packages (from bayesian-optimization) (0.21.2)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\users\\xixi\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.13.2)\n",
      "Building wheels for collected packages: bayesian-optimization\n",
      "  Building wheel for bayesian-optimization (setup.py): started\n",
      "  Building wheel for bayesian-optimization (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\XIXI\\AppData\\Local\\pip\\Cache\\wheels\\16\\80\\a5\\4c2356930fa0a8f25745839daf357842d72265b68e75c8c20f\n",
      "Successfully built bayesian-optimization\n",
      "Installing collected packages: bayesian-optimization\n",
      "Successfully installed bayesian-optimization-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cgZ8RAkELIP"
   },
   "outputs": [],
   "source": [
    "# 定义优化函数\n",
    "from sklearn.model_selection import cross_val_score\n",
    "def rf_cv_lgb(num_leaves, max_depth, bagging_fraction, feature_fraction, \n",
    "        bagging_freq, min_data_in_leaf, min_child_weight, min_split_gain,\n",
    "        reg_lambda, reg_alpha):\n",
    "  model_lgb = lgb.LGBMClassifier(boosting_type='gbdt',\n",
    "         objective='multiclass',\n",
    "         num_class=4,\n",
    "         learning_rate=0.1,\n",
    "         n_estimators=5000,\n",
    "         num_leaves=int(num_leaves),\n",
    "         max_depth=int(max_depth),\n",
    "         bagging_fraction=round(bagging_fraction, 2), \n",
    "         feature_fraction=round(feature_fraction, 2),\n",
    "         bagging_freq=int(bagging_freq), \n",
    "         min_data_in_leaf=int(min_data_in_leaf),\n",
    "         min_child_weight=min_child_weight, \n",
    "         min_split_gain=min_split_gain,\n",
    "         reg_lambda=reg_lambda, \n",
    "         reg_alpha=reg_alpha,\n",
    "         n_jobs= 2 \n",
    "         )\n",
    "  \n",
    "  f1 = make_scorer(f1_score, average='micro')\n",
    "  val = cross_val_score(model_lgb, X_train_split, y_train_split, cv=5).mean()  \n",
    "  #↑原代码有scoring='f1'，执行出错，修改为scoring=f1，执行依然出错，遂去除，此处疑惑待解决\n",
    "\n",
    "  return val\n",
    "\n",
    "# 定义优化参数\n",
    "from bayes_opt import BayesianOptimization\n",
    "bayes_lgb = BayesianOptimization(rf_cv_lgb, \n",
    "                  {\n",
    "                  'num_leaves':(10, 200),\n",
    "                  'max_depth':(3, 20),\n",
    "                  'bagging_fraction':(0.5, 1.0),\n",
    "                  'feature_fraction':(0.5, 1.0),\n",
    "                  'bagging_freq':(0, 100),\n",
    "                  'min_data_in_leaf':(10,100),\n",
    "                  'min_child_weight':(0, 10),\n",
    "                  'min_split_gain':(0.0, 1.0),\n",
    "                  'reg_alpha':(0.0, 10),\n",
    "                  'reg_lambda':(0.0, 10),    \n",
    "                  }                \n",
    "                  )\n",
    "\n",
    "# 开始优化\n",
    "bayes_lgb.maximize(n_iter=1)\n",
    "\n",
    "# >>> 计算耗时较长，取n_iter=3，耗时13分26秒，执行失败，取n_iter=1,耗时1分24秒，执行失败\n",
    "\n",
    "# 显示优化结果\n",
    "print(bayes_lgb.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8ZeeySGOkzx"
   },
   "outputs": [],
   "source": [
    "# 基于BayesianOptimization优化的结果建立新模型并降低学习率寻找最优模型迭代次数\n",
    "base_params_lgb = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'multiclass',\n",
    "            'num_class': 4,\n",
    "            'learning_rate': 0.01,\n",
    "            'num_leaves': 138,\n",
    "            'max_depth': 11,\n",
    "            'min_data_in_leaf': 43,\n",
    "            'min_child_weight':6.5,\n",
    "            'bagging_fraction': 0.64,\n",
    "            'feature_fraction': 0.93,\n",
    "            'bagging_freq': 49,\n",
    "            'reg_lambda': 7,\n",
    "            'reg_alpha': 0.21,\n",
    "            'min_split_gain': 0.288,\n",
    "            'nthread': 10,\n",
    "            'verbose': -1,\n",
    "          }\n",
    "\n",
    "cv_result_lgb = lgb.cv(train_set=train_matrix,\n",
    "             early_stopping_rounds = 1000,\n",
    "             num_boost_round = 20000,\n",
    "             nfold=5,\n",
    "             stratified=True,\n",
    "             shuffle=True,\n",
    "             params=base_params_lgb,\n",
    "             fval=f1_score_vali,\n",
    "             seed=0\n",
    "             )\n",
    "\n",
    "print('迭代次数{}'.format(len(cv_result_lgb['f1_score-mean'])))\n",
    "print('最终模型的f1为{}'.format(max(cv_result_lgb['f1_scor-mean'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQie0q0Jpl5v"
   },
   "source": [
    "#### hyperopt自动调参\n",
    "\n",
    "利用集成了贪心、网格和贝叶斯调参方法的Hypropt工具实现自动调参，使用方法和原生cv函数差不多，还可以自动寻找最优调参方法\n",
    "\n",
    "参考资料：[Hyperopt入门](https://blog.csdn.net/qq_34139222/article/details/60322995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kas5Xzn0viUC"
   },
   "outputs": [],
   "source": [
    "# 定义一个需要优化的目标函数，这里直接复用Bayes-opt中定义的被优化目标函数\n",
    "from sklearn.model_selection import cross_val_score\n",
    "def hyperopt_cv_lgb(space_params):\n",
    "  num_leaves = space_params[\"num_leaves\"]\n",
    "  max_depth = space_params[\"max_depth\"]\n",
    "  bagging_fraction = space_params[\"bagging_fraction\"]\n",
    "  feature_fraction = space_params[\"feature_fraction\"]\n",
    "  bagging_freq = space_params[\"bagging_freq\"]\n",
    "  min_data_in_leaf = space_params[\"min_data_in_leaf\"]\n",
    "  min_child_weight = space_params[\"min_child_weight\"]\n",
    "  min_split_gain = space_params[\"min_split_gain\"]\n",
    "  reg_lambda = space_params[\"reg_lambda\"]\n",
    "  reg_alpha = space_params[\"reg_alpha\"]\n",
    "\n",
    "  model_lgb = lgb.LGBMClassifier(\n",
    "                  boosting_type='gbdt',\n",
    "                  objective='multiclass',\n",
    "                  num_class=4,\n",
    "                  learning_rate=0.1,\n",
    "                  n_estimators=5000,\n",
    "                  num_leaves=num_leaves,\n",
    "                  max_depth=max_depth,\n",
    "                  bagging_fraction=bagging_fraction, \n",
    "                  feature_fraction=feature_fraction,\n",
    "                  bagging_freq=bagging_freq, \n",
    "                  min_data_in_leaf=min_data_in_leaf,\n",
    "                  min_child_weight=min_child_weight, \n",
    "                  min_split_gain=min_split_gain,\n",
    "                  reg_lambda=reg_lambda, \n",
    "                  reg_alpha=reg_alpha,\n",
    "                  n_jobs= 2  #根据计算机配置定  \n",
    "                  )\n",
    "                    \n",
    "  # f1 = make_scorer(f1_score, average='micro')\n",
    "  val = cross_val_score(model_lgb, X_train_split, y_train_split, cv=5).mean()  \n",
    "\n",
    "  return -val  # hyperopt默认将“目标函数”的输出值降到最小，因此这里设为负值\n",
    "\n",
    "# 定义搜索空间，依然参考Bayes-opt的搜索空间，只不过表达形式需要调整\n",
    "from hyperopt import hp\n",
    "space_params = {\n",
    "    'num_leaves': hp.choice('num_leaves',range(135, 145)),\n",
    "    'max_depth': hp.choice('max_depth',range(7, 8)),\n",
    "    'bagging_fraction': hp.choice('bagging_fraction',[0.01*i for i in range(50, 100)]),\n",
    "    'feature_fraction': hp.choice('feature_fraction',[0.01*i for i in range(50, 100)]),\n",
    "    'bagging_freq': hp.choice('bagging_freq',range(0, 100)),\n",
    "    'min_data_in_leaf': hp.choice('min_data_in_leaf',range(10, 100)),\n",
    "    'min_child_weight': hp.choice('min_child_weight',range(0, 10)),\n",
    "    'min_split_gain': hp.choice('min_split_gain',[i*0.1 for i in range(0, 10)]),\n",
    "    'reg_alpha': hp.choice('reg_alpha',[i*0.1 for i in range(0, 10)]),\n",
    "    'reg_lambda': hp.choice('reg_lambda',[i*0.1 for i in range(0, 10)])\n",
    "    }  \n",
    "\n",
    "# 在搜索空间内进行搜索（algo用于定义搜索方法），去最小化目标函数（也可以最大化，见官方文档）\n",
    "from hyperopt import fmin,tpe,hp,space_eval,rand,Trials,partial,STATUS_OK\n",
    "# algo = partial(tpe.suggest,n_startup_jobs=1)\n",
    "best = fmin(hyperopt_cv_lgb, space_params, algo=rand.suggest, max_evals=10) # 累计计算多少个模型，先算10个\n",
    "\n",
    "print(best)\n",
    "# -> 执行结果：\n",
    "print(hyperopt_cv_lgb(best))\n",
    "# -> 执行结果：\n",
    "\n",
    "# 取max_evals=1, 搜索空间为：{'num_leaves':range(135, 145)，'max_depth':range(7, 10)}，耗时为40分38秒，执行手动中止。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNCW7YT1fsqc"
   },
   "source": [
    "# 模型融合（Task_5）\n",
    "\n",
    "基于融合策略提升方案指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntEq_QHbfyFR"
   },
   "source": [
    "## 通用方法\n",
    "\n",
    "1. 简单加权融合：\n",
    "  * 回归（分类概率）：算数平均融合，集合平均融合\n",
    "  * 分类：投票\n",
    "  * 综合：排序融合，log融合\n",
    "\n",
    "2. Stacking/blending：\n",
    "  * 构建多层模型，利用预测结果再拟合预测模型\n",
    "\n",
    "3. boosting/bagging（在xgboost, Adaboost, GBDT已经用到）：\n",
    "  * 多树的提升方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cl2HQy3ihlYM"
   },
   "source": [
    "## 代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNOVZ1rThqBc"
   },
   "source": [
    "### 回归/分类-融合——练习\n",
    "\n",
    "1） 加权平均融合（回归）\n",
    "\n",
    "2） Stacking融合（回归）\n",
    "\n",
    "3） 投票机制融合（分类）\n",
    "\n",
    "4） Stacking融合（分类）\n",
    "\n",
    "5） Blending融合（分类）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpco9LQaoyaz"
   },
   "source": [
    "#### 1）加权平均融合（回归/分类概率）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 981,
     "status": "ok",
     "timestamp": 1618656534990,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "TPLV9NH1l_3x",
    "outputId": "dd2fdda4-e32e-438c-e9e6-b652bb5ed8e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred1 MAE; 0.1750000000000001\n",
      "Pred1 MAE; 0.07499999999999993\n",
      "Pred1 MAE; 0.10000000000000009\n",
      "Weighted_pre MAE: 0.05750000000000027\n"
     ]
    }
   ],
   "source": [
    "### 1）加权平均融合（回归/分类概率） --> 能对预测结果进行数值计算，说明结果是连续性的，适用于回归问题的预测数值和分类问题的类别预测概率\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "# 生成一些样本，test_pre1表示第一个模型的预测值\n",
    "test_pre1 = [1.2, 3.2, 2.1, 6.2]\n",
    "test_pre2 = [0.9, 3.1, 2.0, 5.9]\n",
    "test_pre3 = [1.1, 2.9, 2.2, 6.0]\n",
    "\n",
    "# y_test_true 代表模型的真实值\n",
    "y_test_true = [1, 3, 2, 6]\n",
    "\n",
    "# 定义结果的加权函数\n",
    "def Weighted_method(test_pre1, test_pre2, test_pre3, w=[1/3, 1/3, 1/3]):\n",
    "  Weighted_result = w[0]*pd.Series(test_pre1) + w[1]*pd.Series(test_pre2) + w[2]*pd.Series(test_pre3)\n",
    "  return Weighted_result\n",
    "\n",
    "# 计算各个模型的MAE（也可是MSE等吧）\n",
    "print('Pred1 MAE;', metrics.mean_absolute_error(y_test_true, test_pre1))\n",
    "print('Pred1 MAE;', metrics.mean_absolute_error(y_test_true, test_pre2))\n",
    "print('Pred1 MAE;', metrics.mean_absolute_error(y_test_true, test_pre3))\n",
    "\n",
    "# 根据加权计算MAE\n",
    "w = [0.3, 0.4, 0.3]\n",
    "Weighted_pre = Weighted_method(test_pre1,test_pre2,test_pre3,w)\n",
    "print('Weighted_pre MAE:', metrics.mean_absolute_error(y_test_true, Weighted_pre))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 858,
     "status": "ok",
     "timestamp": 1618656629919,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "CgvxWFErT0_z",
    "outputId": "20ff4f29-a7ec-4fc9-ad00-70807c003f33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_pre MAE: 0.06666666666666693\n",
      "Mean_pre MAE: 0.07500000000000007\n"
     ]
    }
   ],
   "source": [
    "# 定义结果的加权平均函数\n",
    "def Mean_method(test_pre1, test_pre2, test_pre3):\n",
    "  Mean_result = pd.concat([pd.Series(test_pre1), pd.Series(test_pre2), pd.Series(test_pre3)], axis=1).mean(axis=1)\n",
    "  return Mean_result\n",
    "\n",
    "Mean_pre = Mean_method(test_pre1, test_pre2, test_pre3)\n",
    "print('Mean_pre MAE:', metrics.mean_absolute_error(y_test_true, Mean_pre))\n",
    "\n",
    "def Mean_method(test_pre1, test_pre2, test_pre3):\n",
    "  Mean_result = pd.concat([pd.Series(test_pre1), pd.Series(test_pre2), pd.Series(test_pre3)], axis=1).median(axis=1)\n",
    "  return Mean_result\n",
    "\n",
    "Mean_pre = Mean_method(test_pre1, test_pre2, test_pre3)\n",
    "print('Mean_pre MAE:', metrics.mean_absolute_error(y_test_true, Mean_pre))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thmO_S4Ro3P_"
   },
   "source": [
    "#### 2）Stacking融合（回归/分类概率）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 893,
     "status": "ok",
     "timestamp": 1618656643521,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "cDz_2w__af6S",
    "outputId": "3355d816-95db-4055-9ddf-5e5d231c2d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking_pre MAE: 0.042134831460675204\n"
     ]
    }
   ],
   "source": [
    "### 2）Stacking融合（回归/分类概率）  --> 能对预测结果进行数值计算，说明结果是连续性的，适用于回归问题的预测数值和分类问题的类别预测概率\n",
    "from sklearn import linear_model\n",
    "\n",
    "def Stacking_method(train_reg1, train_reg2, train_reg3, y_train_true, \n",
    "          test_pre1, test_pre2, test_pre3, \n",
    "          model_L2 = linear_model.LinearRegression()):\n",
    "  model_L2.fit(pd.concat([pd.Series(train_reg1),pd.Series(train_reg2),pd.Series(train_reg3)], axis=1).values, y_train_true)\n",
    "  Stacking_result = model_L2.predict(pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=1).values)\n",
    "  return Stacking_result\n",
    "\n",
    "# 定义一批数据进行Stacking融合（only for regression problem）\n",
    "train_reg1 = [3.2, 8.2, 9.1, 5.2]\n",
    "train_reg2 = [2.9, 8.1, 9.0, 4.9]\n",
    "train_reg3 = [3.1, 7.9, 9.2, 5.0]\n",
    "# y_test_true 代表第模型的真实值\n",
    "y_train_true = [3, 8, 9, 5] \n",
    "\n",
    "test_pre1 = [1.2, 3.2, 2.1, 6.2]\n",
    "test_pre2 = [0.9, 3.1, 2.0, 5.9]\n",
    "test_pre3 = [1.1, 2.9, 2.2, 6.0]\n",
    "\n",
    "# y_test_true 代表第模型的真实值\n",
    "y_test_true = [1, 3, 2, 6] \n",
    "\n",
    "# 开始融合\n",
    "model_L2 = linear_model.LinearRegression()\n",
    "Stacking_pre = Stacking_method(train_reg1, train_reg2, train_reg3, y_train_true,\n",
    "                test_pre1, test_pre2, test_pre3, model_L2)\n",
    "print('Stacking_pre MAE:', metrics.mean_absolute_error(y_test_true, Stacking_pre) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICd5SbO2o66j"
   },
   "source": [
    "#### 3）投票机制融合（分类）\n",
    "\n",
    "少数服从多数，包括软投票和硬投票"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdtwb0qNjHTD"
   },
   "outputs": [],
   "source": [
    "# 加载依赖包\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7322,
     "status": "ok",
     "timestamp": 1618656675205,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "r2LrOzsYpAib",
    "outputId": "ab210aca-22b4-4ffb-8f40-01e8fd74f6ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95 (+/- 0.03) [LGB]\n",
      "Accuracy: 0.33 (+/- 0.00) [Random Forest]\n",
      "Accuracy: 0.92 (+/- 0.03) [SVM]\n",
      "Accuracy: 0.92 (+/- 0.03) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "### 3）投票机制融合（分类）\n",
    "'''\n",
    "硬投票：对多个模型直接进行投票，不区分模型结果的相对重要度，最终投票最多的类即为被预测的类\n",
    "'''\n",
    "# 以鸢尾花数据集为例实践\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "\n",
    "clf1 = lgb.LGBMClassifier(learning_rate=0.1,\n",
    "              n_estimators=150,\n",
    "              max_depth=3,\n",
    "              min_child_weight=2,\n",
    "              subsample=0.7,\n",
    "              colsample_bytree=0.6,\n",
    "              objective='binary:logistic')\n",
    "\n",
    "clf2 = RandomForestClassifier(n_estimators=200, \n",
    "                max_depth=10, \n",
    "                min_samples_split=10, \n",
    "                min_samples_leaf=63, \n",
    "                oob_score=True )\n",
    "\n",
    "clf3 = SVC(C=0.1)\n",
    "\n",
    "# 硬投票\n",
    "eclf = VotingClassifier(estimators=[('lgb',clf1), ('rf',clf2), ('svc',clf3)], voting='hard')\n",
    "for clf, label in zip([clf1,clf2,clf3,eclf],['LGB','Random Forest', 'SVM', 'Ensemble']):\n",
    "  scores = cross_val_score(clf, x, y, cv=5, scoring='accuracy')\n",
    "  print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oEo19z-pEzF"
   },
   "source": [
    "#### 4）Stacking融合（分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4bVNSjlLT2h"
   },
   "outputs": [],
   "source": [
    "# 加载依赖包\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1275,
     "status": "ok",
     "timestamp": 1618656750679,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "xPjKFmtSLvD4",
    "outputId": "cd9e14c7-edf2-4062-c312-371db39aad2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val auc score: 1.000000\n",
      "val auc score: 0.500000\n",
      "val auc score: 0.500000\n",
      "val auc score: 0.500000\n",
      "val auc score: 0.500000\n",
      "Val auc Score of Stacking: 1.000000\n"
     ]
    }
   ],
   "source": [
    "### 4）Stacking融合（分类）\n",
    "''' 5-Fold Stacking '''\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "# 以鸢尾花数据为例\n",
    "data_0 = iris.data\n",
    "data = data_0[:100,:]   # 前100个数据只包含了2种类别,因此该单元格的代码解决的是二分类问题\n",
    "\n",
    "target_0 = iris.target   # 前100个数据只包含了2种类别\n",
    "target = target_0[:100]\n",
    "\n",
    "# 融合单模\n",
    "clfs = [LogisticRegression(solver='lbfgs'), \n",
    "     RandomForestClassifier(n_estimators=5, n_jobs=1, criterion='gini'),\n",
    "     ExtraTreesClassifier(n_estimators=5, n_jobs=1, criterion='gini'),\n",
    "     ExtraTreesClassifier(n_estimators=5, n_jobs=1, criterion='entropy'),\n",
    "     GradientBoostingClassifier(learning_rate=0.005, subsample=0.5, max_depth=6, n_estimators=5)\n",
    "    ]\n",
    "\n",
    "# 切分部分数据作为测试集（测试集不参与模型调参，只参与模型选择）\n",
    "X, X_predict, y, y_predict = train_test_split(data, target, test_size=0.3, random_state=2022)\n",
    "\n",
    "dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n",
    "dataset_blend_test = np.zeros((X_predict.shape[0], len(clfs)))\n",
    "\n",
    "# 5折Stacking\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits)\n",
    "skf = skf.split(X, y)\n",
    "\n",
    "for j, clf in enumerate(clfs): \n",
    "  # 依次训练单模 \n",
    "  dataset_blend_test_j = np.zeros((X_predict.shape[0], 5)) \n",
    "  for i, (train, test) in enumerate(skf): \n",
    "    # 5-Fold交叉训练，使用第i个部分作为预测，剩余部分来训练模型，获得其预测的输出作为第i部分的新特征 \n",
    "    X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]  \n",
    "    clf.fit(X_train, y_train) \n",
    "    y_submission = clf.predict_proba(X_test)[:,1] \n",
    "    dataset_blend_train[test, j] = y_submission  \n",
    "    dataset_blend_test_j[:,i] = clf.predict_proba(X_predict)[:,1] \n",
    "  # 对于测试集（y_predict)，与5折计算结果（分类概率）的均值进行比较得到ROC相关的评估指标 \n",
    "  dataset_blend_test[:,j] = dataset_blend_test_j.mean(1)   \n",
    "  print('val auc score: %f' % roc_auc_score(y_predict, dataset_blend_test[:,j]))  \n",
    "  # break \n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs') \n",
    "clf.fit(dataset_blend_train, y) \n",
    "y_submission = clf.predict_proba(dataset_blend_test)[:,1] \n",
    "\n",
    "print('Val auc Score of Stacking: %f' % (roc_auc_score(y_predict,y_submission)))                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCOMgxZS9keM"
   },
   "source": [
    "#### 5）Blending融合（分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 777,
     "status": "ok",
     "timestamp": 1618656760875,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "8RHMZnBZ9tGU",
    "outputId": "3b9e1c33-f4f0-45a6-f838-f4bbbaef746b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val auc Score:1.000000\n",
      "Val auc Score:1.000000\n",
      "Val auc Score:1.000000\n",
      "Val auc Score:1.000000\n",
      "Val auc Score:1.000000\n",
      "Val auc Score:1.000000\n",
      "Val auc Score of Blending: 1.000000\n"
     ]
    }
   ],
   "source": [
    "### Blending融合\n",
    "# 创建训练的数据集\n",
    "data_0 = iris.data\n",
    "data = data_0[:100,:]\n",
    "target_0 = iris.target\n",
    "target = target_0[:100]\n",
    "\n",
    "# 引入单模\n",
    "clfs = [ LogisticRegression(solver='lbfgs'), \n",
    "      RandomForestClassifier(n_estimators=5, n_jobs=1, criterion='gini'),\n",
    "      RandomForestClassifier(n_estimators=5, n_jobs=1, criterion='entropy'),\n",
    "      ExtraTreesClassifier(n_estimators=5, n_jobs=1, criterion='gini'),\n",
    "      ExtraTreesClassifier(n_estimators=5, n_jobs=1, criterion='entropy'),\n",
    "      GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)\n",
    "    ]\n",
    "# 划分一部分数据作为测试集\n",
    "X, X_predict, y, y_predict = train_test_split(data, target, test_size=0.3, random_state=2021)\n",
    "\n",
    "# 切分训练数据集为d1,d2两部分\n",
    "X_d1, X_d2, y_d1, y_d2 = train_test_split(X, y, test_size=0.5, random_state=2022)\n",
    "dataset_d1 = np.zeros((X_d1.shape[0], len(clfs)))\n",
    "dataset_d2 = np.zeros((X_predict.shape[0], len(clfs)))\n",
    "\n",
    "# 依次训练单模\n",
    "for j, clf in enumerate(clfs):\n",
    "  clf.fit(X_d1,y_d1)\n",
    "  y_submission = clf.predict_proba(X_d2)[:,1]\n",
    "  dataset_d1[:,j] = y_submission\n",
    "  # 对于测试集，直接用这k个模型的预测值作为新的特征\n",
    "  dataset_d2[:,j] = clf.predict_proba(X_predict)[:,1]\n",
    "  print('Val auc Score:%f' % roc_auc_score(y_predict, dataset_d2[:,j]))\n",
    "\n",
    "# 融合使用的模型\n",
    "clf = GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=30)\n",
    "clf.fit(dataset_d1, y_d2)\n",
    "y_submission = clf.predict_proba(dataset_d2)[:,1]\n",
    "print('Val auc Score of Blending: %f' % (roc_auc_score(y_predict, y_submission)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "av-J8z3zQ_Dr"
   },
   "source": [
    "### 分类-模型融合——心电信号分类实战\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_B_Wi5Cw-ckc"
   },
   "source": [
    "#### 准备工作\n",
    "\n",
    "流程：\n",
    "\n",
    "1. 导入数据，预处理\n",
    "2. 划分train和test集\n",
    "3. 构建单模：RF，LGB，NN\n",
    "4. 读取并演示如何利用融合模型生成可提交预测数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKs36kC8RNEa"
   },
   "outputs": [],
   "source": [
    "# 加载依赖包\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier,MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ye6hRMKbUIU1"
   },
   "outputs": [],
   "source": [
    "# 引入降内存的函数\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2 \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2 \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27697,
     "status": "ok",
     "timestamp": 1618659647897,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "KtgwNp8gWysh",
    "outputId": "b98f1899-bb2a-4312-8749-0492ccf3dc6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 157.93 MB\n",
      "Memory usage after optimization is: 39.67 MB\n",
      "Decreased by 74.9%\n",
      "Memory usage of dataframe is 31.43 MB\n",
      "Memory usage after optimization is: 7.90 MB\n",
      "Decreased by 74.9%\n",
      "predict rf...\n",
      "predict lgb...\n",
      "predict NN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "train = pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/train.csv')\n",
    "test = pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/testA.csv')\n",
    "# 简单预处理\n",
    "train_list = []\n",
    "for items in train.values:\n",
    "  train_list.append([items[0]] + [float(i) for i in items[1].split(',')] + [items[2]])\n",
    "\n",
    "test_list = []\n",
    "for items in test.values:\n",
    "  test_list.append([items[0]] + [float(i) for i in items[1].split(',')])\n",
    "\n",
    "train = pd.DataFrame(np.array(train_list))\n",
    "test = pd.DataFrame(np.array(test_list))\n",
    "\n",
    "# rename_columns\n",
    "features = ['s_' + str(i) for i in range(len(train_list[0])-2)]\n",
    "train.columns = ['id'] + features + ['label']\n",
    "test.columns = ['id'] + features\n",
    "\n",
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)\n",
    "\n",
    "# 划分特征集和标签集\n",
    "features = train.drop(['id', 'label'], axis=1)\n",
    "labels = train['label']\n",
    "\n",
    "# 测试集\n",
    "X_test = test.drop(['id'], axis=1)\n",
    "\n",
    "# 第一次运行可以先用一个subdata\n",
    "# X_train = X_train.iloc[:500, :20]\n",
    "# y_train = y_train.iloc[:500]\n",
    "# X_test = X_test.iloc[:500, :20]\n",
    "\n",
    "# 划分训练集train和验证集val\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gVo8Xbhon9U"
   },
   "source": [
    "#### 构建单模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEiuCzxct8U8"
   },
   "outputs": [],
   "source": [
    "# 单模函数\n",
    "def build_model_rf(X_train, y_train):\n",
    "  model = RandomForestClassifier(n_estimators=100)\n",
    "  model.fit(X_train, y_train)\n",
    "  return model\n",
    "\n",
    "def build_model_lgb(X_train, y_train):\n",
    "  model = lgb.LGBMRegressor(num_leaves=63, learning_rate=0.1, n_estimators=100)\n",
    "  model.fit(X_train, y_train)\n",
    "  return model\n",
    "\n",
    "def build_model_nn(X_train, y_train):\n",
    "  model = MLPRegressor(alpha=1e-05, hidden_layer_sizes=(100,4), random_state=1, solver='lbfgs')\n",
    "  model.fit(X_train, y_train)\n",
    "  return model\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# 赛制评分标准\n",
    "def feval_abs_sum(preds, labels):  \n",
    "  # preds = np.argmax(preds.reshape(4,-1),axis=0)\n",
    "  onehot_encoder = OneHotEncoder(sparse=False)\n",
    "  labels_ = onehot_encoder.fit_transform(labels.values.reshape(len(labels), 1))\n",
    "  preds_ = onehot_encoder.fit_transform(preds.reshape(len(preds), 1))\n",
    "  print(labels_.shape, '\\n', preds_.shape)\n",
    "  score_for_Competition = sum(sum(abs(labels_ - preds_)))\n",
    "  return score_for_Competition\n",
    "\n",
    "# 单模/弱分类器训练（不含调参）\n",
    "# print('predict rf...')\n",
    "# model_rf = build_model_rf(X_train, y_train)\n",
    "# val_rf = model_rf.predict(X_val)   # 准确来说，该处为验证环节\n",
    "# subA_rf = model_rf.predict(X_test)  # 针对testA.csv文件，是测试环节\n",
    "# score_for_Competition = feval_abs_sum(val_rf, y_val)  # 注意区分y_true和y_val\n",
    "# print('score_for_Competition of RF:',score_for_Competition)\n",
    "'''\n",
    "predict rf...\n",
    "(30000, 4) \n",
    " (30000, 4)\n",
    "score_for_Competition of RF: 1162.0\n",
    "'''\n",
    "\n",
    "# print('predict lgb...')\n",
    "# model_lgb = build_model_lgb(X_train, y_train)\n",
    "# val_lgb = model_lgb.predict(X_val)\n",
    "# subA_lgb = model_lgb.predict(X_test)\n",
    "# score_for_Competition = feval_abs_sum(val_lgb, y_val)  # 注意区分y_true和y_val\n",
    "# print('score_for_Competition of lightGBM:',score_for_Competition)\n",
    "'''报错:\n",
    "operands could not be broadcast together with shapes (30000,4) (30000,28678) \n",
    "'''\n",
    "\n",
    "print('predict NN...')\n",
    "model_nn = build_model_nn(X_train,y_train)\n",
    "val_nn = model_nn.predict(X_val)\n",
    "subA_nn = model_nn.predict(X_test)\n",
    "score_for_Competition = feval_abs_sum(val_nn, y_val)  # 注意区分y_true和y_val\n",
    "print('score_for_Competition of NN:',score_for_Competition)\n",
    "'''\n",
    "operands could not be broadcast together with shapes (30000,4) (30000,29999) \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BtvYDxB-kle"
   },
   "source": [
    "#### 加权融合\n",
    "\n",
    "加权矩阵可自定义，如果没有给，则默认均值加权融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 984,
     "status": "ok",
     "timestamp": 1618657818350,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "npQj13FG-pGn",
    "outputId": "dc9ee1c3-051b-4936-aaf5-04e9076ce5be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of Weighted of val： 0.32152826961009273\n"
     ]
    }
   ],
   "source": [
    "# 均值融合 --> w = [1/3, 1/3, 1/3] 或者自定义：w=[0.2,0.3,0.5]\n",
    "def Weighted_method(test_pre1, test_pre2, test_pre3, w=[1/3,1/3,1/3]):\n",
    "  Weighted_result = pd.Series(test_pre1)*w[0] + pd.Series(test_pre2)*w[1] + pd.Series(test_pre3)*w[2]\n",
    "  return Weighted_result\n",
    "\n",
    "# 自定义w进行融合\n",
    "w=[0.2,0.3,0.5]\n",
    "val_pre = Weighted_method(val_rf, val_lgb, val_nn, w)\n",
    "MAE_Weighted = mean_absolute_error(y_val, val_pre)\n",
    "print('MAE of Weighted of val：', MAE_Weighted)\n",
    "\n",
    "# 测试与提交\n",
    "subA = Weighted_method(subA_rf, subA_lgb, subA_nn, w)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['SaleID'] = X_test.index\n",
    "sub['price'] = subA\n",
    "sub.to_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/sub_weighted.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIQEu5uzGTiB"
   },
   "source": [
    "#### STacking融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 892,
     "status": "ok",
     "timestamp": 1618661196392,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "pEC_NcfEGXZL",
    "outputId": "c162abac-39e0-43b3-95f7-1b259bb71794"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of stacking: 0.0\n",
      "MAE of stacking: 0.2067\n",
      "Predict stacking...\n"
     ]
    }
   ],
   "source": [
    "## 第一层\n",
    "train_rf_pred = model_rf.predict(X_train)\n",
    "train_lgb_pred = model_lgb.predict(X_train)\n",
    "train_nn_pred = model_nn.predict(X_train)\n",
    "\n",
    "stacking_X_train = pd.DataFrame()\n",
    "stacking_X_train['Method_1'] = train_rf_pred\n",
    "stacking_X_train['Method_2'] = train_lgb_pred\n",
    "stacking_X_train['Method_3'] = train_nn_pred\n",
    "\n",
    "stacking_X_val = pd.DataFrame()\n",
    "stacking_X_val['Method_1'] = val_rf\n",
    "stacking_X_val['Method_2'] = val_lgb\n",
    "stacking_X_val['Method_3'] = val_nn\n",
    "\n",
    "stacking_X_test = pd.DataFrame()\n",
    "stacking_X_test['Method_1'] = subA_rf \n",
    "stacking_X_test['Method_2'] = subA_lgb\n",
    "stacking_X_test['Method_3'] = subA_nn\n",
    "\n",
    "## 第二层用random forest\n",
    "model_lr_stacking = build_model_rf(stacking_X_train, y_train)\n",
    "\n",
    "# 训练集\n",
    "train_pre_Stacking = model_lr_stacking.predict(stacking_X_train)\n",
    "print('MAE of stacking:', mean_absolute_error(y_train, train_pre_Stacking))\n",
    "\n",
    "# 验证集\n",
    "val_pre_Stacking = model_lr_stacking.predict(stacking_X_val)\n",
    "print('MAE of stacking:',mean_absolute_error(y_val,val_pre_Stacking))\n",
    "\n",
    "# 测试集（for testA.csv)\n",
    "print('Predict stacking...')\n",
    "subA_Stacking = model_lr_stacking.predict(stacking_X_test)\n",
    "\n",
    "# 提交\n",
    "sub = pd.DataFrame()\n",
    "sub['SaleID'] = X_test.index\n",
    "sub['price'] = subA_Stacking\n",
    "sub.to_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/sub_Stacking.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *“短平快实践 ”至“AutoML”为实操代码*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfXATxJlRwWN"
   },
   "source": [
    "# 短平快实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJkDQg_M-9N5"
   },
   "source": [
    "## 准备工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44694,
     "status": "ok",
     "timestamp": 1620873250158,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "bC5NRGlm_UER",
    "outputId": "932f2140-5ec8-4cbb-b95b-babdb57061f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 157.93 MB\n",
      "Memory usage after optimization is: 39.67 MB\n",
      "Decreased by 74.9%\n"
     ]
    }
   ],
   "source": [
    "# 导入包\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    " \n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    " \n",
    " \n",
    "# 数据预处理\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2 \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    " \n",
    "    end_mem = df.memory_usage().sum() / 1024**2 \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    " \n",
    " \n",
    "# 读取数据\n",
    "data = pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/train.csv')\n",
    "data_features_filtered = pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Features_Extract/train_features_filtered_MinimalFCParameters.csv')\n",
    "X_test = pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/testA.csv').values\n",
    " \n",
    "# 原始特征Dataframe\n",
    "data_list = []\n",
    "for item in data.values:\n",
    "  data_list.append([item[0]] + [float(i) for i in item[1].split(',')] + [item[2]])\n",
    "data = pd.DataFrame(np.array(data_list))\n",
    "data.columns = ['id'] + ['s_' + str(i) for i in range(len(data_list[0])-2)] + ['label']\n",
    " \n",
    "data = reduce_mem_usage(data)\n",
    " \n",
    "data.to_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/features_labels.csv')\n",
    " \n",
    "# 分离数据\n",
    "from sklearn.model_selection import KFold\n",
    "features = data.drop(['id','label'], axis=1)\n",
    "labels = data['label']\n",
    " \n",
    "# 划分X_train, X_val, y_train, y_val\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4Ok6Qc4LIX_"
   },
   "outputs": [],
   "source": [
    "# 划分X_train, X_val, y_train, y_val\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-HUInGQiy40"
   },
   "source": [
    "## 构建单模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEbhk_WSoDK5"
   },
   "source": [
    "### lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77TmT27Ml21c"
   },
   "outputs": [],
   "source": [
    "# 自定义评价指标\n",
    "def f1_score_vali(preds, data_vali):\n",
    "  labels = data_vali.get_label()\n",
    "  preds = np.argmax(preds.reshape(4,-1),axis=0)\n",
    "  # print('labels type&shape:',type(labels), len(labels), '\\n','preds type&shape:',type(preds), preds.shape)\n",
    "  score_vali = f1_score(y_true=labels, y_pred=preds, average='macro')\n",
    "  return 'f1_score', score_vali, True\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "def feval_abs_sum(preds, data_vali):\n",
    "  labels = data_vali.get_label()\n",
    "  preds = np.argmax(preds.reshape(4,-1),axis=0)\n",
    "  labels_ = onehot_encoder.fit_transform(labels.values.reshape(len(labels), 1))\n",
    "  preds_ = onehot_encoder.fit_transform(preds.reshape(len(preds), 1))\n",
    "  score_for_Competition = sum(sum(abs(labels_ - preds_)))\n",
    "  # score_vali = f1_score(y_true=labels, y_pred=preds, average='macro')\n",
    "  return 'score_for_Competition', score_for_Competition, True\n",
    "\n",
    "\n",
    "def feval_abs_sum_sparse(preds, labels):  # 导入概率分布的pred和稀疏标签的labels，用OneHot计算score_for_Competition\n",
    "  # preds = np.argmax(preds,axis=1)\n",
    "  preds = np.argmax(preds.reshape(4,-1),axis=0)\n",
    "  labels = labels.get_label()\n",
    "  # print(preds)\n",
    "  # print(labels)\n",
    "  score_for_Competition = 0\n",
    "  for i in range(preds.shape[0]):\n",
    "    if preds[i] != np.array(labels)[i]:\n",
    "      score_for_Competition += 2\n",
    "  return 'score_for_Competition', score_for_Competition, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 121247,
     "status": "ok",
     "timestamp": 1619558770135,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "R8g_t-LFR7tD",
    "outputId": "116b6d35-1f3e-46f3-b840-e111f5418a63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 400 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 0.106386\tvalid_0's f1_score: 0.925387\n",
      "[100]\tvalid_0's multi_logloss: 0.0735948\tvalid_0's f1_score: 0.941689\n",
      "[150]\tvalid_0's multi_logloss: 0.0630055\tvalid_0's f1_score: 0.947696\n",
      "[200]\tvalid_0's multi_logloss: 0.0560957\tvalid_0's f1_score: 0.952306\n",
      "[250]\tvalid_0's multi_logloss: 0.0516787\tvalid_0's f1_score: 0.955923\n",
      "[300]\tvalid_0's multi_logloss: 0.0499089\tvalid_0's f1_score: 0.957633\n",
      "[350]\tvalid_0's multi_logloss: 0.0489718\tvalid_0's f1_score: 0.958154\n",
      "[400]\tvalid_0's multi_logloss: 0.0486045\tvalid_0's f1_score: 0.959966\n",
      "[450]\tvalid_0's multi_logloss: 0.0480774\tvalid_0's f1_score: 0.960603\n",
      "[500]\tvalid_0's multi_logloss: 0.0472645\tvalid_0's f1_score: 0.961346\n",
      "[550]\tvalid_0's multi_logloss: 0.0471555\tvalid_0's f1_score: 0.961156\n",
      "[600]\tvalid_0's multi_logloss: 0.0471555\tvalid_0's f1_score: 0.961156\n",
      "[650]\tvalid_0's multi_logloss: 0.0471555\tvalid_0's f1_score: 0.961156\n",
      "[700]\tvalid_0's multi_logloss: 0.0471555\tvalid_0's f1_score: 0.961156\n",
      "[750]\tvalid_0's multi_logloss: 0.0471555\tvalid_0's f1_score: 0.961156\n",
      "[800]\tvalid_0's multi_logloss: 0.0471555\tvalid_0's f1_score: 0.961156\n",
      "[850]\tvalid_0's multi_logloss: 0.0471555\tvalid_0's f1_score: 0.961156\n",
      "Early stopping, best iteration is:\n",
      "[496]\tvalid_0's multi_logloss: 0.0473315\tvalid_0's f1_score: 0.961662\n"
     ]
    }
   ],
   "source": [
    "# 用lightGBM建立Dataset\n",
    "train_matrix = lgb.Dataset(X_train, label=y_train)\n",
    "valid_matrix = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "# 设置模型参数\n",
    "params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'boosting': 'gbdt',\n",
    "    'lambda_l2': 0.1,\n",
    "    'max_depth': 7,\n",
    "    'num_leaves': 128,\n",
    "    'bagging_fraction': 0.4,\n",
    "    'feature_fraction': 0.6,\n",
    "    'bagging_freq': 40,\n",
    "    'min_data_in_lea': 45,\n",
    "    'min_child_weight': 0.001,\n",
    "    'metric': None,\n",
    "    'objective': 'multiclass',\n",
    "    'min_split_gain': 0.1,\n",
    "    'num_class': 4,\n",
    "    'nthread': 10,\n",
    "    'verbose': -1,\n",
    "}\n",
    "\n",
    "\n",
    "# 使用lightGBM进行训练\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_set = train_matrix,\n",
    "    valid_sets = valid_matrix,\n",
    "    num_boost_round = 2000,\n",
    "    verbose_eval = 50,\n",
    "    early_stopping_rounds = 400,\n",
    "    feval = f1_score_vali\n",
    ")\n",
    "\n",
    "# 保存模型\n",
    "# model.booster_.savemodel(\"/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/Model/LightGBM_ECG.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 981,
     "status": "ok",
     "timestamp": 1619558831601,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "uEQ6rrL0p3XP",
    "outputId": "2e9a43a2-86e3-4f4d-ee19-55f24e95702f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7f11200bf450>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_model(\"/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/Model/LightGBM_ECG.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1118,
     "status": "ok",
     "timestamp": 1619548207607,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "Uv1Tq7cUlShA",
    "outputId": "9d27dfb2-fc5d-4b90-f2ea-764ca354166c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未调参前lightGBM单模在验证集上的f1:0.19541168157118838\n",
      "score_for_Competition (17922, True)\n"
     ]
    }
   ],
   "source": [
    "# 加载已有模型\n",
    "model = lgb.Booster(model_file=\"/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/Model/LightGBM_ECG.txt\")\n",
    "\n",
    "# 对验证集进行验证\n",
    "val_pred_lgb = model.predict(X_val, num_iteration=model.best_iteration) \n",
    "preds = np.argmax(val_pred_lgb, axis=1)\n",
    "score = f1_score(y_true=y_val, y_pred=preds, average='macro')\n",
    "print(\"未调参前lightGBM单模在验证集上的f1:{}\".format(score))\n",
    "\n",
    "# 按照比赛规定的计分方法计算评分\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# onehot_encoder = OneHotEncoder(sparse=False)\n",
    "# y_val_ = onehot_encoder.fit_transform(y_val.values.reshape(len(y_val), 1))\n",
    "# preds_ = onehot_encoder.fit_transform(preds.reshape(len(preds), 1))\n",
    "# def abs_sum(y_pred, y_true):\n",
    "#   loss = sum(sum(abs(y_pred-y_true)))\n",
    "#   return loss\n",
    "# print(abs_sum(preds_, y_val_))\n",
    "\n",
    "\n",
    "def feval_abs_sum_sparse(preds, labels):  # 导入概率分布的pred和稀疏标签的labels，用OneHot计算score_for_Competition\n",
    "  # preds = np.argmax(preds,axis=1)\n",
    "  preds = np.argmax(preds,axis=1)\n",
    "  # print(preds)\n",
    "  # print(labels)\n",
    "  score_for_Competition = 0\n",
    "  for i in range(preds.shape[0]):\n",
    "    if preds[i] != np.array(labels)[i]:\n",
    "      score_for_Competition += 2\n",
    "  return score_for_Competition\n",
    "score_for_Competition = feval_abs_sum_sparse(val_pred_lgb, y_val)\n",
    "print('score_for_Competition', score_for_Competition )\n",
    "\n",
    "\n",
    "# 以该lightGBM结构计算500步后得到score_for_Competition为531.2， 记录于2021/04/23/02:30，模型名称: LightGBM_ECG.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 531403,
     "status": "ok",
     "timestamp": 1619280473434,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "fuEe-6F9dvKV",
    "outputId": "a172f5c8-2db6-411e-a74f-d23059122246"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************* 1 *********************************\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.0680069\tvalid_0's f1_score: 0.943052\n",
      "[200]\tvalid_0's multi_logloss: 0.051981\tvalid_0's f1_score: 0.952192\n",
      "[300]\tvalid_0's multi_logloss: 0.0465615\tvalid_0's f1_score: 0.957848\n",
      "[400]\tvalid_0's multi_logloss: 0.0438683\tvalid_0's f1_score: 0.961399\n",
      "[500]\tvalid_0's multi_logloss: 0.0427846\tvalid_0's f1_score: 0.963495\n",
      "[600]\tvalid_0's multi_logloss: 0.0428119\tvalid_0's f1_score: 0.963709\n",
      "Early stopping, best iteration is:\n",
      "[493]\tvalid_0's multi_logloss: 0.0427778\tvalid_0's f1_score: 0.963903\n",
      "[0.9639028937105489]\n",
      "********************************* 2 *********************************\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.0702369\tvalid_0's f1_score: 0.94154\n",
      "[200]\tvalid_0's multi_logloss: 0.0543702\tvalid_0's f1_score: 0.95352\n",
      "[300]\tvalid_0's multi_logloss: 0.0478633\tvalid_0's f1_score: 0.957045\n",
      "[400]\tvalid_0's multi_logloss: 0.0456973\tvalid_0's f1_score: 0.960362\n",
      "[500]\tvalid_0's multi_logloss: 0.0443044\tvalid_0's f1_score: 0.962232\n",
      "[600]\tvalid_0's multi_logloss: 0.0443265\tvalid_0's f1_score: 0.962122\n",
      "Early stopping, best iteration is:\n",
      "[491]\tvalid_0's multi_logloss: 0.0441585\tvalid_0's f1_score: 0.961866\n",
      "[0.9639028937105489, 0.9618661070253979]\n",
      "********************************* 3 *********************************\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.0699167\tvalid_0's f1_score: 0.946372\n",
      "[200]\tvalid_0's multi_logloss: 0.0560052\tvalid_0's f1_score: 0.954593\n",
      "[300]\tvalid_0's multi_logloss: 0.0495382\tvalid_0's f1_score: 0.960488\n",
      "[400]\tvalid_0's multi_logloss: 0.0476321\tvalid_0's f1_score: 0.961676\n",
      "[500]\tvalid_0's multi_logloss: 0.0468683\tvalid_0's f1_score: 0.963242\n",
      "[600]\tvalid_0's multi_logloss: 0.046883\tvalid_0's f1_score: 0.963849\n",
      "Early stopping, best iteration is:\n",
      "[444]\tvalid_0's multi_logloss: 0.0470997\tvalid_0's f1_score: 0.964088\n",
      "[0.9639028937105489, 0.9618661070253979, 0.9640878355611643]\n",
      "********************************* 4 *********************************\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.0676124\tvalid_0's f1_score: 0.946275\n",
      "[200]\tvalid_0's multi_logloss: 0.0493184\tvalid_0's f1_score: 0.958167\n",
      "[300]\tvalid_0's multi_logloss: 0.0422166\tvalid_0's f1_score: 0.96465\n",
      "[400]\tvalid_0's multi_logloss: 0.0407112\tvalid_0's f1_score: 0.965074\n",
      "[500]\tvalid_0's multi_logloss: 0.039108\tvalid_0's f1_score: 0.967165\n",
      "[600]\tvalid_0's multi_logloss: 0.039108\tvalid_0's f1_score: 0.967165\n",
      "Early stopping, best iteration is:\n",
      "[464]\tvalid_0's multi_logloss: 0.0391182\tvalid_0's f1_score: 0.967442\n",
      "[0.9639028937105489, 0.9618661070253979, 0.9640878355611643, 0.9674416557062316]\n",
      "********************************* 5 *********************************\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\tvalid_0's multi_logloss: 0.0693076\tvalid_0's f1_score: 0.940349\n",
      "[200]\tvalid_0's multi_logloss: 0.0514287\tvalid_0's f1_score: 0.956156\n",
      "[300]\tvalid_0's multi_logloss: 0.0441897\tvalid_0's f1_score: 0.960519\n",
      "[400]\tvalid_0's multi_logloss: 0.0426504\tvalid_0's f1_score: 0.963557\n",
      "[500]\tvalid_0's multi_logloss: 0.0419835\tvalid_0's f1_score: 0.964207\n",
      "[600]\tvalid_0's multi_logloss: 0.0418446\tvalid_0's f1_score: 0.965018\n",
      "[700]\tvalid_0's multi_logloss: 0.0418446\tvalid_0's f1_score: 0.965018\n",
      "Early stopping, best iteration is:\n",
      "[524]\tvalid_0's multi_logloss: 0.041905\tvalid_0's f1_score: 0.965619\n",
      "[0.9639028937105489, 0.9618661070253979, 0.9640878355611643, 0.9674416557062316, 0.9653493466758218]\n",
      "lgb_scotrainre_list:[0.9639028937105489, 0.9618661070253979, 0.9640878355611643, 0.9674416557062316, 0.9653493466758218]\n",
      "lgb_score_mean:0.9645295677358329\n",
      "lgb_score_std:0.0018348931555774335\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# 使用5折交叉验证进行模型性能评估\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "cv_scores = []\n",
    "for i, (train_index, valid_index) in enumerate(kf.split(features, labels)):\n",
    "  print('********************************* {} *********************************'.format(str(i+1)))\n",
    "  X_train, y_train, X_val, y_val = \\\n",
    "  features.iloc[train_index], labels[train_index], features.iloc[valid_index], labels[valid_index]\n",
    "\n",
    "  train_matrix = lgb.Dataset(X_train, label=y_train)\n",
    "  valid_matrix = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "  params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'boosting': 'gbdt',\n",
    "    'lambda_l2': 0.1,\n",
    "    'max_depth': 7,\n",
    "    'num_leaves': 128,\n",
    "    'bagging_fraction': 0.4,\n",
    "    'feature_fraction': 0.6,\n",
    "    'bagging_freq': 40,\n",
    "    'min_data_in_lea': 45,\n",
    "    'min_child_weight': 0.001,\n",
    "    'metric': None,\n",
    "    'objective': 'multiclass',\n",
    "    'min_split_gain': 0.1,\n",
    "    'num_class': 4,\n",
    "    'nthread': 10,\n",
    "    'verbose': -1,\n",
    "  }\n",
    "\n",
    "  model = lgb.train(params,\n",
    "            train_set = train_matrix,\n",
    "            valid_sets = valid_matrix,\n",
    "            num_boost_round = 2000,\n",
    "            verbose_eval = 100,\n",
    "            early_stopping_rounds = 200,\n",
    "            feval = f1_score_vali)\n",
    "  # model = lgb.Booster(model_file=\"/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/Model/LightGBM_ECG.txt\")\n",
    "  val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "  val_pred = np.argmax(val_pred, axis=1)\n",
    "  cv_scores.append(f1_score(y_true=y_val, y_pred=val_pred, average='macro'))\n",
    "  print(cv_scores)\n",
    "\n",
    "print('lgb_scotrainre_list:{}'.format(cv_scores))\n",
    "print('lgb_score_mean:{}'.format(np.mean(cv_scores)))\n",
    "print('lgb_score_std:{}'.format(np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-bvnf_ImUW8"
   },
   "outputs": [],
   "source": [
    "####  lightGBM网格调参\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OMoDSNioLrf"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1769,
     "status": "ok",
     "timestamp": 1619789448681,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "OHlw6GTIXcKz",
    "outputId": "866fa9aa-8a89-453a-91cd-81ba0c5aeec2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (75000, 205, 1)\n",
      "<class 'numpy.ndarray'> (25000, 205, 1)\n",
      "<class 'numpy.ndarray'> (75000, 1)\n",
      "<class 'numpy.ndarray'> (25000, 1)\n"
     ]
    }
   ],
   "source": [
    "# 划分X_train, X_val, y_train, y_val\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels)\n",
    "\n",
    "# 导入模型前预处理\n",
    "X_train = X_train.values.reshape(X_train.shape[0],X_train.shape[1],1)\n",
    "X_val = X_val.values.reshape(X_val.shape[0],X_val.shape[1],1)\n",
    "y_train = y_train.values.reshape(y_train.shape[0],1)\n",
    "y_val = y_val.values.reshape(y_val.shape[0],1)\n",
    "\n",
    "print(type(X_train),X_train.shape)\n",
    "print(type(X_val),X_val.shape)\n",
    "print(type(y_train),y_train.shape)\n",
    "print(type(y_val),y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1040187,
     "status": "ok",
     "timestamp": 1619790540795,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "rU0tzEep5-lu",
    "outputId": "bf529de1-46ad-41ca-8a35-67fef628654d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "713/713 [==============================] - 27s 15ms/step - loss: 0.9863 - accuracy: 0.6631 - val_loss: 0.9080 - val_accuracy: 0.7360\n",
      "Epoch 2/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.6132 - accuracy: 0.7991 - val_loss: 0.7451 - val_accuracy: 0.7691\n",
      "Epoch 3/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.5047 - accuracy: 0.8400 - val_loss: 0.4540 - val_accuracy: 0.8520\n",
      "Epoch 4/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.4517 - accuracy: 0.8624 - val_loss: 0.4142 - val_accuracy: 0.8789\n",
      "Epoch 5/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.4239 - accuracy: 0.8716 - val_loss: 0.7186 - val_accuracy: 0.7851\n",
      "Epoch 6/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.4114 - accuracy: 0.8751 - val_loss: 0.4597 - val_accuracy: 0.8496\n",
      "Epoch 7/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.3745 - accuracy: 0.8853 - val_loss: 0.3121 - val_accuracy: 0.9136\n",
      "Epoch 8/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.3473 - accuracy: 0.8951 - val_loss: 0.3598 - val_accuracy: 0.8909\n",
      "Epoch 9/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.3335 - accuracy: 0.9003 - val_loss: 0.4646 - val_accuracy: 0.8363\n",
      "Epoch 10/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.3129 - accuracy: 0.9055 - val_loss: 0.3171 - val_accuracy: 0.9061\n",
      "Epoch 11/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.2916 - accuracy: 0.9136 - val_loss: 0.2846 - val_accuracy: 0.9077\n",
      "Epoch 12/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.2715 - accuracy: 0.9217 - val_loss: 0.3975 - val_accuracy: 0.8704\n",
      "Epoch 13/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.2653 - accuracy: 0.9228 - val_loss: 0.2436 - val_accuracy: 0.9307\n",
      "Epoch 14/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.2484 - accuracy: 0.9287 - val_loss: 0.3006 - val_accuracy: 0.9093\n",
      "Epoch 15/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.2314 - accuracy: 0.9359 - val_loss: 0.3706 - val_accuracy: 0.8792\n",
      "Epoch 16/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.2219 - accuracy: 0.9381 - val_loss: 0.2151 - val_accuracy: 0.9459\n",
      "Epoch 17/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.2137 - accuracy: 0.9412 - val_loss: 0.2200 - val_accuracy: 0.9352\n",
      "Epoch 18/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.2002 - accuracy: 0.9433 - val_loss: 0.1762 - val_accuracy: 0.9509\n",
      "Epoch 19/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.1974 - accuracy: 0.9473 - val_loss: 0.1930 - val_accuracy: 0.9456\n",
      "Epoch 20/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.1938 - accuracy: 0.9457 - val_loss: 0.1488 - val_accuracy: 0.9589\n",
      "Epoch 21/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.1767 - accuracy: 0.9515 - val_loss: 0.1463 - val_accuracy: 0.9619\n",
      "Epoch 22/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.1691 - accuracy: 0.9533 - val_loss: 0.1466 - val_accuracy: 0.9595\n",
      "Epoch 23/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.1640 - accuracy: 0.9554 - val_loss: 0.2902 - val_accuracy: 0.8973\n",
      "Epoch 24/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.1647 - accuracy: 0.9545 - val_loss: 0.1671 - val_accuracy: 0.9573\n",
      "Epoch 25/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.1566 - accuracy: 0.9571 - val_loss: 0.1423 - val_accuracy: 0.9621\n",
      "Epoch 26/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.1511 - accuracy: 0.9590 - val_loss: 0.1182 - val_accuracy: 0.9688\n",
      "Epoch 27/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.1462 - accuracy: 0.9602 - val_loss: 0.1385 - val_accuracy: 0.9624\n",
      "Epoch 28/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.1406 - accuracy: 0.9610 - val_loss: 0.1371 - val_accuracy: 0.9616\n",
      "Epoch 29/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.1396 - accuracy: 0.9623 - val_loss: 0.1418 - val_accuracy: 0.9627\n",
      "Epoch 30/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.1354 - accuracy: 0.9635 - val_loss: 0.1153 - val_accuracy: 0.9707\n",
      "Epoch 31/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.1303 - accuracy: 0.9639 - val_loss: 0.1189 - val_accuracy: 0.9675\n",
      "Epoch 32/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.1318 - accuracy: 0.9642 - val_loss: 0.1030 - val_accuracy: 0.9720\n",
      "Epoch 33/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.1240 - accuracy: 0.9659 - val_loss: 0.1839 - val_accuracy: 0.9475\n",
      "Epoch 34/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.1183 - accuracy: 0.9672 - val_loss: 0.1112 - val_accuracy: 0.9667\n",
      "Epoch 35/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.1126 - accuracy: 0.9704 - val_loss: 0.1616 - val_accuracy: 0.9579\n",
      "Epoch 36/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.1109 - accuracy: 0.9695 - val_loss: 0.1088 - val_accuracy: 0.9733\n",
      "Epoch 37/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.1109 - accuracy: 0.9697 - val_loss: 0.0952 - val_accuracy: 0.9736\n",
      "Epoch 38/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.1073 - accuracy: 0.9700 - val_loss: 0.0949 - val_accuracy: 0.9763\n",
      "Epoch 39/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.1023 - accuracy: 0.9721 - val_loss: 0.0877 - val_accuracy: 0.9771\n",
      "Epoch 40/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0989 - accuracy: 0.9722 - val_loss: 0.1369 - val_accuracy: 0.9624\n",
      "Epoch 41/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.1013 - accuracy: 0.9724 - val_loss: 0.0865 - val_accuracy: 0.9771\n",
      "Epoch 42/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0949 - accuracy: 0.9738 - val_loss: 0.0808 - val_accuracy: 0.9789\n",
      "Epoch 43/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0913 - accuracy: 0.9750 - val_loss: 0.1013 - val_accuracy: 0.9709\n",
      "Epoch 44/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0909 - accuracy: 0.9742 - val_loss: 0.0862 - val_accuracy: 0.9768\n",
      "Epoch 45/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0853 - accuracy: 0.9768 - val_loss: 0.0883 - val_accuracy: 0.9765\n",
      "Epoch 46/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0866 - accuracy: 0.9770 - val_loss: 0.0847 - val_accuracy: 0.9792\n",
      "Epoch 47/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0885 - accuracy: 0.9754 - val_loss: 0.1047 - val_accuracy: 0.9709\n",
      "Epoch 48/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0849 - accuracy: 0.9758 - val_loss: 0.1258 - val_accuracy: 0.9661\n",
      "Epoch 49/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0828 - accuracy: 0.9774 - val_loss: 0.0721 - val_accuracy: 0.9816\n",
      "Epoch 50/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0787 - accuracy: 0.9776 - val_loss: 0.0768 - val_accuracy: 0.9811\n",
      "Epoch 51/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0808 - accuracy: 0.9770 - val_loss: 0.0744 - val_accuracy: 0.9781\n",
      "Epoch 52/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0779 - accuracy: 0.9782 - val_loss: 0.0777 - val_accuracy: 0.9787\n",
      "Epoch 53/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0751 - accuracy: 0.9794 - val_loss: 0.0820 - val_accuracy: 0.9779\n",
      "Epoch 54/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0724 - accuracy: 0.9796 - val_loss: 0.0831 - val_accuracy: 0.9755\n",
      "Epoch 55/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0715 - accuracy: 0.9803 - val_loss: 0.0783 - val_accuracy: 0.9781\n",
      "Epoch 56/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0728 - accuracy: 0.9802 - val_loss: 0.0750 - val_accuracy: 0.9800\n",
      "Epoch 57/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0746 - accuracy: 0.9788 - val_loss: 0.0869 - val_accuracy: 0.9768\n",
      "Epoch 58/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0681 - accuracy: 0.9804 - val_loss: 0.0789 - val_accuracy: 0.9792\n",
      "Epoch 59/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0689 - accuracy: 0.9819 - val_loss: 0.0764 - val_accuracy: 0.9813\n",
      "Epoch 60/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0703 - accuracy: 0.9802 - val_loss: 0.0649 - val_accuracy: 0.9819\n",
      "Epoch 61/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0664 - accuracy: 0.9811 - val_loss: 0.0694 - val_accuracy: 0.9803\n",
      "Epoch 62/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0640 - accuracy: 0.9823 - val_loss: 0.0661 - val_accuracy: 0.9824\n",
      "Epoch 63/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0626 - accuracy: 0.9831 - val_loss: 0.0688 - val_accuracy: 0.9827\n",
      "Epoch 64/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0624 - accuracy: 0.9830 - val_loss: 0.0685 - val_accuracy: 0.9787\n",
      "Epoch 65/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0627 - accuracy: 0.9837 - val_loss: 0.0655 - val_accuracy: 0.9819\n",
      "Epoch 66/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0608 - accuracy: 0.9825 - val_loss: 0.0688 - val_accuracy: 0.9808\n",
      "Epoch 67/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0606 - accuracy: 0.9829 - val_loss: 0.0646 - val_accuracy: 0.9832\n",
      "Epoch 68/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0555 - accuracy: 0.9852 - val_loss: 0.0726 - val_accuracy: 0.9813\n",
      "Epoch 69/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0598 - accuracy: 0.9830 - val_loss: 0.0871 - val_accuracy: 0.9773\n",
      "Epoch 70/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0576 - accuracy: 0.9839 - val_loss: 0.0746 - val_accuracy: 0.9800\n",
      "Epoch 71/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0568 - accuracy: 0.9843 - val_loss: 0.0798 - val_accuracy: 0.9760\n",
      "Epoch 72/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0569 - accuracy: 0.9837 - val_loss: 0.0630 - val_accuracy: 0.9827\n",
      "Epoch 73/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0543 - accuracy: 0.9847 - val_loss: 0.0683 - val_accuracy: 0.9808\n",
      "Epoch 74/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0526 - accuracy: 0.9857 - val_loss: 0.0686 - val_accuracy: 0.9797\n",
      "Epoch 75/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0532 - accuracy: 0.9851 - val_loss: 0.0750 - val_accuracy: 0.9803\n",
      "Epoch 76/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0507 - accuracy: 0.9865 - val_loss: 0.0514 - val_accuracy: 0.9843\n",
      "Epoch 77/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0489 - accuracy: 0.9872 - val_loss: 0.0602 - val_accuracy: 0.9843\n",
      "Epoch 78/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0518 - accuracy: 0.9864 - val_loss: 0.0702 - val_accuracy: 0.9805\n",
      "Epoch 79/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0534 - accuracy: 0.9860 - val_loss: 0.0566 - val_accuracy: 0.9856\n",
      "Epoch 80/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0485 - accuracy: 0.9870 - val_loss: 0.0678 - val_accuracy: 0.9827\n",
      "Epoch 81/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0499 - accuracy: 0.9859 - val_loss: 0.0591 - val_accuracy: 0.9840\n",
      "Epoch 82/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0456 - accuracy: 0.9869 - val_loss: 0.0609 - val_accuracy: 0.9824\n",
      "Epoch 83/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0466 - accuracy: 0.9869 - val_loss: 0.0659 - val_accuracy: 0.9845\n",
      "Epoch 84/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0487 - accuracy: 0.9865 - val_loss: 0.0636 - val_accuracy: 0.9845\n",
      "Epoch 85/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0447 - accuracy: 0.9880 - val_loss: 0.0652 - val_accuracy: 0.9803\n",
      "Epoch 86/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0446 - accuracy: 0.9880 - val_loss: 0.0612 - val_accuracy: 0.9837\n",
      "Epoch 87/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0451 - accuracy: 0.9876 - val_loss: 0.0621 - val_accuracy: 0.9845\n",
      "Epoch 88/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0467 - accuracy: 0.9873 - val_loss: 0.0713 - val_accuracy: 0.9816\n",
      "Epoch 89/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0436 - accuracy: 0.9881 - val_loss: 0.0615 - val_accuracy: 0.9848\n",
      "Epoch 90/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0440 - accuracy: 0.9878 - val_loss: 0.0553 - val_accuracy: 0.9851\n",
      "Epoch 91/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0412 - accuracy: 0.9883 - val_loss: 0.0762 - val_accuracy: 0.9795\n",
      "Epoch 92/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0441 - accuracy: 0.9877 - val_loss: 0.0621 - val_accuracy: 0.9845\n",
      "Epoch 93/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0452 - accuracy: 0.9874 - val_loss: 0.0742 - val_accuracy: 0.9797\n",
      "Epoch 94/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0411 - accuracy: 0.9883 - val_loss: 0.0763 - val_accuracy: 0.9827\n",
      "Epoch 95/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0390 - accuracy: 0.9895 - val_loss: 0.0558 - val_accuracy: 0.9851\n",
      "Epoch 96/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0407 - accuracy: 0.9888 - val_loss: 0.0592 - val_accuracy: 0.9861\n",
      "Epoch 97/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0419 - accuracy: 0.9883 - val_loss: 0.0611 - val_accuracy: 0.9864\n",
      "Epoch 98/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0393 - accuracy: 0.9893 - val_loss: 0.0596 - val_accuracy: 0.9848\n",
      "Epoch 99/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0394 - accuracy: 0.9890 - val_loss: 0.0536 - val_accuracy: 0.9872\n",
      "Epoch 100/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0389 - accuracy: 0.9892 - val_loss: 0.0703 - val_accuracy: 0.9859\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import keras\n",
    "\n",
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), kernel_regularizer=keras.regularizers.l2(0.01)))  #  return_sequences=True\n",
    "model.add(BatchNormalization())\n",
    "# model.add(CuDNNLSTM(64, return_sequences=True))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(CuDNNLSTM(32))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=1e-3, beta_1=0.9, beta_2=0.99, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=100,  validation_split =0.05)\n",
    "# model.save('/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/Model/LSTM_ECG_0423_2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yk2l_yVSApRS",
    "outputId": "22cdc5a5-fe5c-4a5c-f1e8-47886e133b90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "713/713 [==============================] - 11s 15ms/step - loss: 0.0269 - accuracy: 0.9934 - val_loss: 0.0648 - val_accuracy: 0.9877\n",
      "Epoch 2/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0162 - accuracy: 0.9952 - val_loss: 0.0438 - val_accuracy: 0.9893\n",
      "Epoch 3/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0174 - accuracy: 0.9947 - val_loss: 0.0418 - val_accuracy: 0.9901\n",
      "Epoch 4/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0155 - accuracy: 0.9953 - val_loss: 0.0683 - val_accuracy: 0.9864\n",
      "Epoch 5/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0150 - accuracy: 0.9952 - val_loss: 0.0445 - val_accuracy: 0.9904\n",
      "Epoch 6/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0154 - accuracy: 0.9958 - val_loss: 0.0476 - val_accuracy: 0.9896\n",
      "Epoch 7/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0149 - accuracy: 0.9961 - val_loss: 0.0515 - val_accuracy: 0.9872\n",
      "Epoch 10/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0144 - accuracy: 0.9963 - val_loss: 0.0478 - val_accuracy: 0.9893\n",
      "Epoch 11/100\n",
      "713/713 [==============================] - 10s 14ms/step - loss: 0.0148 - accuracy: 0.9960 - val_loss: 0.0478 - val_accuracy: 0.9885\n",
      "Epoch 12/100\n",
      "149/713 [=====>........................] - ETA: 7s - loss: 0.0107 - accuracy: 0.9973"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=1e-3, beta_1=0.9, beta_2=0.99, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=100,  validation_split =0.05)\n",
    "# model.save('/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/Model/LSTM_ECG_0430.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9147,
     "status": "ok",
     "timestamp": 1619796832829,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "jpu9eDSszidZ",
    "outputId": "f3eedf92-9a31-4d67-c4fc-b4af786431e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_for_Competition: 540\n"
     ]
    }
   ],
   "source": [
    "# 加载已有模型进行验证\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import keras\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model_path = '/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/Model/LSTM_ECG_0430.h5'\n",
    "\n",
    "model_xcc = load_model(model_path)\n",
    "\n",
    "def feval_abs_sum(preds, labels):  # 导入的都是稀疏标签\n",
    "  score_for_Competition = 0\n",
    "  for i in range(preds.shape[0]):\n",
    "    if preds[i] != np.array(labels)[i]:\n",
    "      score_for_Competition += 2\n",
    "  return score_for_Competition\n",
    "\n",
    "# optimizer = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.99, epsilon=1e-08, decay=0.0)\n",
    "# model_xcc.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "# history = model_xcc.fit(X_train, y_train, epochs=50, batch_size=100,  validation_split =0.05)\n",
    "\n",
    "y_pred = model_xcc.predict_classes(X_val)\n",
    "score_for_Competition = feval_abs_sum(y_pred, y_val)\n",
    "print('score_for_Competition:',score_for_Competition)\n",
    "\n",
    "# # 输出预测结果\n",
    "# X_test = pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/testA.csv')\n",
    "\n",
    "# # X_test处理\n",
    "# X_test_list = []\n",
    "# for item in X_test.values:\n",
    "#   X_test_list.append([item[0]] + [float(i) for i in item[1].split(',')])\n",
    "# X_test = pd.DataFrame(np.array(X_test_list))\n",
    "# X_test.columns = ['id'] + ['s_' + str(i) for i in range(len(data_list[0])-2)]\n",
    "# X_test = reduce_mem_usage(X_test)\n",
    "\n",
    "# X_test = X_test.values.reshape((X_test.shape[0],X_test.shape[1],1))\n",
    "# # y_preds_subm = model_xcc.predict_proba(X_test)\n",
    "# y_preds_subm = model_xcc.predict_classes(X_test)\n",
    "# y_preds_subm = onehot_encoder.fit_transform(y_preds_subm.reshape(len(y_preds_subm), 1))\n",
    "# result=pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/sample_submit.csv')\n",
    "# result['label_0']=y_preds_subm[:,0]\n",
    "# result['label_1']=y_preds_subm[:,1]\n",
    "# result['label_2']=y_preds_subm[:,2]\n",
    "# result['label_3']=y_preds_subm[:,3]\n",
    "# result.to_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/submit.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60008,
     "status": "ok",
     "timestamp": 1619636144976,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "DgB5Y88qsFzf",
    "outputId": "bfc4acef-24cd-446d-9135-fc3aaf4a0e25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "713/713 [==============================] - 13s 16ms/step - loss: 1.9431e-04 - accuracy: 1.0000 - val_loss: 0.0910 - val_accuracy: 0.9944\n",
      "Epoch 2/5\n",
      "713/713 [==============================] - 11s 16ms/step - loss: 8.3819e-05 - accuracy: 1.0000 - val_loss: 0.0910 - val_accuracy: 0.9944\n",
      "Epoch 3/5\n",
      "713/713 [==============================] - 12s 16ms/step - loss: 1.9583e-04 - accuracy: 1.0000 - val_loss: 0.0910 - val_accuracy: 0.9944\n",
      "Epoch 4/5\n",
      "713/713 [==============================] - 11s 16ms/step - loss: 5.2772e-05 - accuracy: 1.0000 - val_loss: 0.0910 - val_accuracy: 0.9944\n",
      "Epoch 5/5\n",
      "713/713 [==============================] - 11s 16ms/step - loss: 3.2438e-05 - accuracy: 1.0000 - val_loss: 0.0910 - val_accuracy: 0.9944\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.000001, beta_1=0.9, beta_2=0.99, epsilon=1e-08, decay=0.95)\n",
    "model_xcc.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model_xcc.fit(X_train, y_train, epochs=5, batch_size=100,  validation_split =0.05)\n",
    "model_xcc.save('/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/Model/LSTM_ECG_0429-128.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1559,
     "status": "ok",
     "timestamp": 1619797010042,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "JSxqK5uyrsqM",
    "outputId": "66750bf4-3a07-4642-ba12-7c8b783a0c9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9892\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in range(y_val.shape[0]):\n",
    "  if y_pred[i] == y_val[i][0]:\n",
    "    sum += 1\n",
    "\n",
    "print(float(sum/y_val.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5858,
     "status": "ok",
     "timestamp": 1619796996796,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "twFiJPnVqhFy",
    "outputId": "10c00e78-5f54-4a8d-b499-89f0a1247a6e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_for_Competition: 540\n"
     ]
    }
   ],
   "source": [
    "def feval_abs_sum(preds, labels):  # 导入的都是稀疏标签\n",
    "  score_for_Competition = 0\n",
    "  for i in range(preds.shape[0]):\n",
    "    if preds[i] != np.array(labels)[i]:\n",
    "      score_for_Competition += 2\n",
    "  return score_for_Competition\n",
    "\n",
    "\n",
    "y_pred = model_xcc.predict_classes(X_val)\n",
    "score_for_Competition = feval_abs_sum(y_pred, y_val)\n",
    "print('score_for_Competition:',score_for_Competition)\n",
    "\n",
    "\n",
    "# y_pred = model_xcc.predict_proba(X_val)\n",
    "# # 调整预测结果\n",
    "# data = y_pred\n",
    "# for i in range(y_pred.shape[0]):\n",
    "#   zeros_i_row = np.array([0,0,0,0])\n",
    "#   for j in range(y_pred.shape[1]):\n",
    "#     if data[i][j] > 0.9:\n",
    "#       zeros_i_row[j] = 1\n",
    "#       data[i,:] = zeros_i_row\n",
    "# print(data)\n",
    "# np.savetxt('/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/data.csv', data, delimiter = ',')\n",
    "# score_for_Competition = feval_abs_sum_1(data, y_val)\n",
    "# print('score_for_Competition:',score_for_Competition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zb5PGh8fuvTJ"
   },
   "source": [
    "### CNN_1D\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1323,
     "status": "ok",
     "timestamp": 1620532313367,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "fWmNxMOK8HWz",
    "outputId": "bbafb69d-4cf6-446b-c188-4070f62b0f04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (75000, 205, 1)\n",
      "<class 'numpy.ndarray'> (25000, 205, 1)\n",
      "<class 'numpy.ndarray'> (75000, 1)\n",
      "<class 'numpy.ndarray'> (25000, 1)\n"
     ]
    }
   ],
   "source": [
    "# 划分X_train, X_val, y_train, y_val\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels)\n",
    " \n",
    "# 导入模型前预处理\n",
    "X_train = X_train.values.reshape(X_train.shape[0],X_train.shape[1],1)\n",
    "X_val = X_val.values.reshape(X_val.shape[0],X_val.shape[1],1)\n",
    "y_train = y_train.values.reshape(y_train.shape[0],1)\n",
    "y_val = y_val.values.reshape(y_val.shape[0],1)\n",
    " \n",
    "print(type(X_train),X_train.shape)\n",
    "print(type(X_val),X_val.shape)\n",
    "print(type(y_train),y_train.shape)\n",
    "print(type(y_val),y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-vxmqf0-uDl"
   },
   "outputs": [],
   "source": [
    "#%tensorflow_version 1.x   # 使用1.x版本的TF\n",
    " \n",
    "# 用CNN做序列特征提取\n",
    "import keras\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt \n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.objectives import *\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    " \n",
    " \n",
    "\"\"\"GPU设置为按需增长\"\"\"\n",
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    " \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #有多个GPU时可以指定只使用第几号GPU\n",
    "config = tf.ConfigProto()\n",
    "config.allow_soft_placement=True #允许动态放置张量和操作符\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4 #最多使用40%GPU内存\n",
    "config.gpu_options.allow_growth=True   #初始化时不全部占满GPU显存, 按需分配 \n",
    "sess = tf.Session(config = config)\n",
    "set_session(sess)\n",
    " \n",
    "from keras import backend as K\n",
    " \n",
    "TIME_PERIODS = 205\n",
    "num_sensors = 1\n",
    "def build_model(input_shape=(TIME_PERIODS,num_sensors),num_classes=4):\n",
    "    model = Sequential()\n",
    "    # model.add(Reshape((TIME_PERIODS, num_sensors), input_shape=input_shape))\n",
    "    model.add(Conv1D(64, 6, strides=1, activation='relu',input_shape=input_shape)) # （batch_size，205，1）》》（batch_size, 200, 16)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(64, 4, strides=1, activation='relu',padding=\"same\"))     # （batch_size，200,16）》》（batch_size, 200, 16)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(2))                            # （batch_size，200,16）》》（batch_size, 100, 16)\n",
    "    model.add(Conv1D(128, 8,strides=1, activation='relu',padding=\"same\"))      # （batch_size，100,64）》》（batch_size, 100, 64)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(128, 8,strides=1, activation='relu',padding=\"same\"))      # （batch_size，100,64）》》（batch_size, 100, 64)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(2))                            # （batch_size，100,64）》》（batch_size, 50, 64)\n",
    "    model.add(Conv1D(256, 8,strides=1, activation='relu',padding=\"same\"))      # （batch_size，100,64）》》（batch_size, 100, 64)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(256, 8,strides=1, activation='relu',padding=\"same\"))      # （batch_size，100,64）》》（batch_size, 100, 64)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(2)) \n",
    "    model.add(GlobalAveragePooling1D())                        # （batch_size, 50, 64) 》》 （batch_size, 64)\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation='softmax'))               # （batch_size, 64) 》》 （batch_size, num_classes)\n",
    "    return(model)\n",
    " \n",
    "K.clear_session()\n",
    "model_CNN = build_model(input_shape=(TIME_PERIODS,num_sensors),num_classes=4)\n",
    "optimizer = keras.optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.99, epsilon=1e-08, decay=0.0)\n",
    "model_CNN.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model_CNN.fit(X_train, y_train, epochs=50, batch_size=36,  validation_split =0.1) \n",
    "# model_CNN.save('/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/Model/CNN_ECG_0422_1.h5')\n",
    " \n",
    "# 30步就可以达到0.98的f1值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 682
    },
    "executionInfo": {
     "elapsed": 136374,
     "status": "error",
     "timestamp": 1619712167167,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "VeNAH0Hywi5f",
    "outputId": "55e3be4e-dc50-4d52-d1bb-580b80ed192c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1875/1875 [==============================] - 19s 9ms/step - loss: 0.0312 - accuracy: 0.9904 - val_loss: 0.0422 - val_accuracy: 0.9885\n",
      "Epoch 2/50\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0308 - accuracy: 0.9911 - val_loss: 0.0422 - val_accuracy: 0.9881\n",
      "Epoch 3/50\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0295 - accuracy: 0.9913 - val_loss: 0.0423 - val_accuracy: 0.9888\n",
      "Epoch 4/50\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0311 - accuracy: 0.9908 - val_loss: 0.0422 - val_accuracy: 0.9883\n",
      "Epoch 5/50\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0311 - accuracy: 0.9912 - val_loss: 0.0423 - val_accuracy: 0.9887\n",
      "Epoch 6/50\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0286 - accuracy: 0.9922 - val_loss: 0.0422 - val_accuracy: 0.9887\n",
      "Epoch 7/50\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0307 - accuracy: 0.9912 - val_loss: 0.0422 - val_accuracy: 0.9887\n",
      "Epoch 8/50\n",
      "1399/1875 [=====================>........] - ETA: 4s - loss: 0.0290 - accuracy: 0.9920"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d92059d37264>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-08\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel_CNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_CNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m36\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0.10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# 200 + -------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# model_CNN.save('/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/Model/CNN_ECG_0429_BN.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import keras\n",
    "# path_model = '/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/Model/CNN_ECG_0429_BN.h5'\n",
    "# model_CNN = load_model(path_model)\n",
    "optimizer = keras.optimizers.Adam(lr=1e-9, beta_1=0.9, beta_2=0.99, epsilon=1e-08, decay=0.0)\n",
    "model_CNN.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model_CNN.fit(X_train, y_train, epochs=50, batch_size=36,  validation_split =0.10)\n",
    "# 200 + -------\n",
    "# model_CNN.save('/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/Model/CNN_ECG_0429_BN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3091,
     "status": "ok",
     "timestamp": 1619636433998,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "e-HZxZnhMDkC",
    "outputId": "3557c188-7135-4b68-8329-ca937c7dc251"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_for_Competition: 486\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.models import load_model\n",
    "def feval_abs_sum(preds, labels):  # 导入的都是稀疏标签\n",
    "  score_for_Competition = 0\n",
    "  for i in range(preds.shape[0]):\n",
    "    if preds[i] != np.array(labels)[i]:\n",
    "      score_for_Competition += 2\n",
    "  return score_for_Competition\n",
    "\n",
    "# model_CNN = load_model('/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/Model/CNN_ECG_0429.h5')\n",
    "y_pred = model_CNN.predict_classes(X_val)  # 亦可用predict_proba函数得到各类别的预测概率（PS：不需要转换为OneHot code)\n",
    "# print(y_pred)\n",
    "# print(y_val)\n",
    "score_for_Competition = feval_abs_sum(y_pred, y_val)\n",
    "print('score_for_Competition:',score_for_Competition)\n",
    "\n",
    "\n",
    "# 以该CNN_1D结构计算30步后得到score_for_Competition为904，明显次于lightGBM的531.2， 记录于2021/04/22/01:03，模型名称: CNN_ECG_0422.h5\n",
    "# 以该CNN_1D结构计算30步后得到score_for_Competition为634，次于lightGBM的531.2， 记录于2021/04/22/03:47，模型名称: CNN_ECG_0422_1.h5\n",
    "# 以该CNN_1D结构计算200步后得到score_for_Competition为576，次于lightGBM的531.2， 记录于2021/04/27/23:47，模型名称: CNN_ECG_0427.h5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiHWBcw4-zeJ"
   },
   "source": [
    "### Conv-LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "executionInfo": {
     "elapsed": 1752,
     "status": "error",
     "timestamp": 1620532480023,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "DWiVchVX8IlB",
    "outputId": "32373cc8-acb7-4a74-9298-2c50322931b9"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f57fa7dfd208>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 导入模型前预处理\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "# 导入模型前预处理\n",
    "X_train = X_train.values.reshape(X_train.shape[0],X_train.shape[1],1)\n",
    "X_val = X_val.values.reshape(X_val.shape[0],X_val.shape[1],1)\n",
    "y_train = y_train.values.reshape(y_train.shape[0],1)\n",
    "y_val = y_val.values.reshape(y_val.shape[0],1)\n",
    " \n",
    "print(type(X_train),X_train.shape)\n",
    "print(type(X_val),X_val.shape)\n",
    "print(type(y_train),y_train.shape)\n",
    "print(type(y_val),y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 371891,
     "status": "ok",
     "timestamp": 1620533018836,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "AZF-ZyM1-5Xf",
    "outputId": "ad269c23-a537-4033-b770-aa6c4c8b8ef5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "713/713 - 25s - loss: 0.2056 - accuracy: 0.9372 - val_loss: 0.3221 - val_accuracy: 0.9096\n",
      "Epoch 2/100\n",
      "713/713 - 23s - loss: 0.0976 - accuracy: 0.9706 - val_loss: 0.0866 - val_accuracy: 0.9723\n",
      "Epoch 3/100\n",
      "713/713 - 23s - loss: 0.0763 - accuracy: 0.9766 - val_loss: 0.1051 - val_accuracy: 0.9619\n",
      "Epoch 4/100\n",
      "713/713 - 23s - loss: 0.0614 - accuracy: 0.9806 - val_loss: 0.0910 - val_accuracy: 0.9731\n",
      "Epoch 5/100\n",
      "713/713 - 23s - loss: 0.0501 - accuracy: 0.9841 - val_loss: 0.0645 - val_accuracy: 0.9792\n",
      "Epoch 6/100\n",
      "713/713 - 23s - loss: 0.0449 - accuracy: 0.9859 - val_loss: 0.0696 - val_accuracy: 0.9789\n",
      "Epoch 7/100\n",
      "713/713 - 23s - loss: 0.0392 - accuracy: 0.9874 - val_loss: 0.0746 - val_accuracy: 0.9765\n",
      "Epoch 8/100\n",
      "713/713 - 23s - loss: 0.0348 - accuracy: 0.9885 - val_loss: 0.0641 - val_accuracy: 0.9827\n",
      "Epoch 9/100\n",
      "713/713 - 23s - loss: 0.0323 - accuracy: 0.9892 - val_loss: 0.0878 - val_accuracy: 0.9723\n",
      "Epoch 10/100\n",
      "713/713 - 23s - loss: 0.0272 - accuracy: 0.9911 - val_loss: 0.1645 - val_accuracy: 0.9536\n",
      "Epoch 11/100\n",
      "713/713 - 23s - loss: 0.0242 - accuracy: 0.9920 - val_loss: 0.0771 - val_accuracy: 0.9795\n",
      "Epoch 12/100\n",
      "713/713 - 23s - loss: 0.0230 - accuracy: 0.9924 - val_loss: 0.1305 - val_accuracy: 0.9597\n",
      "Epoch 13/100\n",
      "713/713 - 23s - loss: 0.0219 - accuracy: 0.9926 - val_loss: 0.0610 - val_accuracy: 0.9832\n",
      "Epoch 14/100\n",
      "713/713 - 23s - loss: 0.0197 - accuracy: 0.9932 - val_loss: 0.0705 - val_accuracy: 0.9824\n",
      "Epoch 15/100\n",
      "713/713 - 23s - loss: 0.0177 - accuracy: 0.9937 - val_loss: 0.0733 - val_accuracy: 0.9808\n",
      "Epoch 16/100\n",
      "713/713 - 23s - loss: 0.0153 - accuracy: 0.9947 - val_loss: 0.0797 - val_accuracy: 0.9819\n",
      "Epoch 17/100\n",
      "713/713 - 23s - loss: 0.0167 - accuracy: 0.9947 - val_loss: 0.0682 - val_accuracy: 0.9864\n",
      "Epoch 18/100\n",
      "713/713 - 23s - loss: 0.0144 - accuracy: 0.9951 - val_loss: 0.0723 - val_accuracy: 0.9821\n",
      "Epoch 19/100\n",
      "713/713 - 23s - loss: 0.0158 - accuracy: 0.9946 - val_loss: 0.0583 - val_accuracy: 0.9867\n",
      "Epoch 20/100\n",
      "713/713 - 23s - loss: 0.0137 - accuracy: 0.9955 - val_loss: 0.0532 - val_accuracy: 0.9877\n",
      "Epoch 21/100\n",
      "713/713 - 23s - loss: 0.0099 - accuracy: 0.9966 - val_loss: 0.0834 - val_accuracy: 0.9803\n",
      "Epoch 22/100\n",
      "713/713 - 23s - loss: 0.0131 - accuracy: 0.9953 - val_loss: 0.0781 - val_accuracy: 0.9819\n",
      "Epoch 23/100\n",
      "713/713 - 23s - loss: 0.0105 - accuracy: 0.9962 - val_loss: 0.0620 - val_accuracy: 0.9864\n",
      "Epoch 24/100\n",
      "713/713 - 23s - loss: 0.0115 - accuracy: 0.9962 - val_loss: 0.0623 - val_accuracy: 0.9859\n",
      "Epoch 25/100\n",
      "713/713 - 23s - loss: 0.0101 - accuracy: 0.9968 - val_loss: 0.0982 - val_accuracy: 0.9821\n",
      "Epoch 26/100\n",
      "713/713 - 23s - loss: 0.0086 - accuracy: 0.9970 - val_loss: 0.0691 - val_accuracy: 0.9877\n",
      "Epoch 27/100\n",
      "713/713 - 23s - loss: 0.0074 - accuracy: 0.9976 - val_loss: 0.0735 - val_accuracy: 0.9835\n",
      "Epoch 28/100\n",
      "713/713 - 23s - loss: 0.0106 - accuracy: 0.9964 - val_loss: 0.0603 - val_accuracy: 0.9877\n",
      "Epoch 29/100\n",
      "713/713 - 23s - loss: 0.0086 - accuracy: 0.9973 - val_loss: 0.0712 - val_accuracy: 0.9883\n",
      "Epoch 30/100\n",
      "713/713 - 23s - loss: 0.0073 - accuracy: 0.9974 - val_loss: 0.0653 - val_accuracy: 0.9891\n",
      "Epoch 31/100\n",
      "713/713 - 23s - loss: 0.0088 - accuracy: 0.9972 - val_loss: 0.0822 - val_accuracy: 0.9861\n",
      "Epoch 32/100\n",
      "713/713 - 23s - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.0832 - val_accuracy: 0.9851\n",
      "Epoch 33/100\n",
      "713/713 - 23s - loss: 0.0076 - accuracy: 0.9976 - val_loss: 0.0682 - val_accuracy: 0.9893\n",
      "Epoch 34/100\n",
      "713/713 - 23s - loss: 0.0065 - accuracy: 0.9979 - val_loss: 0.0627 - val_accuracy: 0.9880\n",
      "Epoch 35/100\n",
      "713/713 - 23s - loss: 0.0069 - accuracy: 0.9978 - val_loss: 0.0800 - val_accuracy: 0.9883\n",
      "Epoch 36/100\n",
      "713/713 - 23s - loss: 0.0063 - accuracy: 0.9978 - val_loss: 0.0883 - val_accuracy: 0.9869\n",
      "Epoch 37/100\n",
      "713/713 - 23s - loss: 0.0072 - accuracy: 0.9977 - val_loss: 0.0816 - val_accuracy: 0.9896\n",
      "Epoch 38/100\n",
      "713/713 - 23s - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.0832 - val_accuracy: 0.9875\n",
      "Epoch 39/100\n",
      "713/713 - 23s - loss: 0.0084 - accuracy: 0.9971 - val_loss: 0.0877 - val_accuracy: 0.9867\n",
      "Epoch 40/100\n",
      "713/713 - 23s - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.0694 - val_accuracy: 0.9875\n",
      "Epoch 41/100\n",
      "713/713 - 23s - loss: 0.0049 - accuracy: 0.9983 - val_loss: 0.0843 - val_accuracy: 0.9877\n",
      "Epoch 42/100\n",
      "713/713 - 23s - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.0738 - val_accuracy: 0.9891\n",
      "Epoch 43/100\n",
      "713/713 - 23s - loss: 0.0060 - accuracy: 0.9980 - val_loss: 0.0682 - val_accuracy: 0.9885\n",
      "Epoch 44/100\n",
      "713/713 - 23s - loss: 0.0049 - accuracy: 0.9987 - val_loss: 0.0859 - val_accuracy: 0.9853\n",
      "Epoch 45/100\n",
      "713/713 - 23s - loss: 0.0078 - accuracy: 0.9976 - val_loss: 0.0737 - val_accuracy: 0.9869\n",
      "Epoch 46/100\n",
      "713/713 - 23s - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.0838 - val_accuracy: 0.9859\n",
      "Epoch 47/100\n",
      "713/713 - 23s - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.0838 - val_accuracy: 0.9875\n",
      "Epoch 48/100\n",
      "713/713 - 23s - loss: 0.0055 - accuracy: 0.9983 - val_loss: 0.0803 - val_accuracy: 0.9888\n",
      "Epoch 49/100\n",
      "713/713 - 23s - loss: 0.0052 - accuracy: 0.9982 - val_loss: 0.0893 - val_accuracy: 0.9864\n",
      "Epoch 50/100\n",
      "713/713 - 23s - loss: 0.0053 - accuracy: 0.9987 - val_loss: 0.0871 - val_accuracy: 0.9880\n",
      "Epoch 51/100\n",
      "713/713 - 23s - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.0940 - val_accuracy: 0.9869\n",
      "Epoch 52/100\n",
      "713/713 - 23s - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.0869 - val_accuracy: 0.9888\n",
      "Epoch 53/100\n",
      "713/713 - 23s - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.0993 - val_accuracy: 0.9875\n",
      "Epoch 54/100\n",
      "713/713 - 23s - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.1028 - val_accuracy: 0.9877\n",
      "Epoch 55/100\n",
      "713/713 - 23s - loss: 0.0053 - accuracy: 0.9986 - val_loss: 0.0725 - val_accuracy: 0.9904\n",
      "Epoch 56/100\n",
      "713/713 - 23s - loss: 0.0055 - accuracy: 0.9985 - val_loss: 0.1018 - val_accuracy: 0.9891\n",
      "Epoch 57/100\n",
      "713/713 - 23s - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.0739 - val_accuracy: 0.9896\n",
      "Epoch 58/100\n",
      "713/713 - 23s - loss: 0.0051 - accuracy: 0.9987 - val_loss: 0.0901 - val_accuracy: 0.9885\n",
      "Epoch 59/100\n",
      "713/713 - 23s - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.0821 - val_accuracy: 0.9872\n",
      "Epoch 60/100\n",
      "713/713 - 23s - loss: 0.0041 - accuracy: 0.9986 - val_loss: 0.1114 - val_accuracy: 0.9869\n",
      "Epoch 61/100\n",
      "713/713 - 23s - loss: 0.0061 - accuracy: 0.9984 - val_loss: 0.0987 - val_accuracy: 0.9864\n",
      "Epoch 62/100\n",
      "713/713 - 23s - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.0960 - val_accuracy: 0.9856\n",
      "Epoch 63/100\n",
      "713/713 - 23s - loss: 0.0053 - accuracy: 0.9986 - val_loss: 0.1030 - val_accuracy: 0.9883\n",
      "Epoch 64/100\n",
      "713/713 - 22s - loss: 0.0038 - accuracy: 0.9989 - val_loss: 0.1035 - val_accuracy: 0.9875\n",
      "Epoch 65/100\n",
      "713/713 - 23s - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0990 - val_accuracy: 0.9872\n",
      "Epoch 66/100\n",
      "713/713 - 23s - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.0856 - val_accuracy: 0.9888\n",
      "Epoch 67/100\n",
      "713/713 - 23s - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0856 - val_accuracy: 0.9880\n",
      "Epoch 68/100\n",
      "713/713 - 23s - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.0802 - val_accuracy: 0.9883\n",
      "Epoch 69/100\n",
      "713/713 - 23s - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.1133 - val_accuracy: 0.9856\n",
      "Epoch 70/100\n",
      "713/713 - 23s - loss: 0.0064 - accuracy: 0.9980 - val_loss: 0.0769 - val_accuracy: 0.9880\n",
      "Epoch 71/100\n",
      "713/713 - 23s - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.1170 - val_accuracy: 0.9891\n",
      "Epoch 72/100\n",
      "713/713 - 23s - loss: 0.0047 - accuracy: 0.9987 - val_loss: 0.0989 - val_accuracy: 0.9869\n",
      "Epoch 73/100\n",
      "713/713 - 23s - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.0869 - val_accuracy: 0.9880\n",
      "Epoch 74/100\n",
      "713/713 - 23s - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.0974 - val_accuracy: 0.9891\n",
      "Epoch 75/100\n",
      "713/713 - 23s - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.1108 - val_accuracy: 0.9891\n",
      "Epoch 76/100\n",
      "713/713 - 23s - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.1286 - val_accuracy: 0.9880\n",
      "Epoch 77/100\n",
      "713/713 - 23s - loss: 0.0044 - accuracy: 0.9988 - val_loss: 0.1235 - val_accuracy: 0.9864\n",
      "Epoch 78/100\n",
      "713/713 - 23s - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.1001 - val_accuracy: 0.9885\n",
      "Epoch 79/100\n",
      "713/713 - 23s - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.1096 - val_accuracy: 0.9880\n",
      "Epoch 80/100\n",
      "713/713 - 23s - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0989 - val_accuracy: 0.9896\n",
      "Epoch 81/100\n",
      "713/713 - 23s - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.1207 - val_accuracy: 0.9867\n",
      "Epoch 82/100\n",
      "713/713 - 23s - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.0936 - val_accuracy: 0.9891\n",
      "Epoch 83/100\n",
      "713/713 - 23s - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.1361 - val_accuracy: 0.9875\n",
      "Epoch 84/100\n",
      "713/713 - 23s - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.0946 - val_accuracy: 0.9901\n",
      "Epoch 85/100\n",
      "713/713 - 23s - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.1103 - val_accuracy: 0.9875\n",
      "Epoch 86/100\n",
      "713/713 - 23s - loss: 0.0045 - accuracy: 0.9989 - val_loss: 0.1093 - val_accuracy: 0.9883\n",
      "Epoch 87/100\n",
      "713/713 - 23s - loss: 0.0043 - accuracy: 0.9989 - val_loss: 0.1285 - val_accuracy: 0.9877\n",
      "Epoch 88/100\n",
      "713/713 - 23s - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.1032 - val_accuracy: 0.9883\n",
      "Epoch 89/100\n",
      "713/713 - 23s - loss: 0.0034 - accuracy: 0.9991 - val_loss: 0.1167 - val_accuracy: 0.9885\n",
      "Epoch 90/100\n",
      "713/713 - 23s - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0859 - val_accuracy: 0.9883\n",
      "Epoch 91/100\n",
      "713/713 - 23s - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.1111 - val_accuracy: 0.9877\n",
      "Epoch 92/100\n",
      "713/713 - 23s - loss: 0.0051 - accuracy: 0.9988 - val_loss: 0.1375 - val_accuracy: 0.9872\n",
      "Epoch 93/100\n",
      "713/713 - 23s - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.1013 - val_accuracy: 0.9893\n",
      "Epoch 94/100\n",
      "713/713 - 23s - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.0894 - val_accuracy: 0.9899\n",
      "Epoch 95/100\n",
      "713/713 - 23s - loss: 0.0043 - accuracy: 0.9989 - val_loss: 0.0902 - val_accuracy: 0.9877\n",
      "Epoch 96/100\n",
      "713/713 - 23s - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.0895 - val_accuracy: 0.9883\n",
      "Epoch 97/100\n",
      "713/713 - 23s - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.1249 - val_accuracy: 0.9877\n",
      "Epoch 98/100\n",
      "713/713 - 23s - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.1478 - val_accuracy: 0.9856\n",
      "Epoch 99/100\n",
      "713/713 - 23s - loss: 0.0044 - accuracy: 0.9989 - val_loss: 0.1097 - val_accuracy: 0.9880\n",
      "Epoch 100/100\n",
      "713/713 - 23s - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.1173 - val_accuracy: 0.9864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_for_Competition: 542\n"
     ]
    }
   ],
   "source": [
    "# 用Conv-LSTM建模预测\n",
    "import keras\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt \n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.objectives import *\n",
    "from tensorflow.keras.layers import ConvLSTM2D\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    " \n",
    "\"\"\"GPU设置为按需增长\"\"\"\n",
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    " \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #有多个GPU时可以指定只使用第几号GPU\n",
    "config = tf.ConfigProto()\n",
    "config.allow_soft_placement=True #允许动态放置张量和操作符\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4 #最多使用40%GPU内存\n",
    "config.gpu_options.allow_growth=True   #初始化时不全部占满GPU显存, 按需分配 \n",
    "sess = tf.Session(config = config)\n",
    "set_session(sess)\n",
    " \n",
    " \n",
    " \n",
    "def feval_abs_sum(preds, labels):  # 导入的都是稀疏标签\n",
    "  score_for_Competition = 0\n",
    "  for i in range(preds.shape[0]):\n",
    "    if preds[i] != np.array(labels)[i]:\n",
    "      score_for_Competition += 2\n",
    "  return score_for_Competition\n",
    " \n",
    "# reshape input from 3dims to 5dims\n",
    "n_timesteps, n_features = X_train.shape[1], X_train.shape[2]\n",
    "n_outputs = 4  # 因为y_train的元素是数值型（稀疏标签），不是OneHotCode型\n",
    "n_steps, n_length = 5, 41  # 设置子序列个数和长度\n",
    "X_train = X_train.reshape((X_train.shape[0], n_steps, 1, n_length, n_features))\n",
    "X_val = X_val.reshape((X_val.shape[0], n_steps, 1, n_length, n_features))\n",
    " \n",
    " \n",
    "def build_model(n_steps, n_length, n_features, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=128, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(ConvLSTM2D(filters=32, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    return model\n",
    " \n",
    "model = build_model(n_steps, n_length, n_features, n_outputs)\n",
    " \n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=100, verbose=1)\n",
    "# model.save('/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/Model/Conv-LSTM_ECG_0422.h5')\n",
    "y_pred = model.predict_classes(X_val)  # 亦可用predict_proba函数得到各类别的预测概率（PS：不需要转换为OneHot code)\n",
    " \n",
    "score_for_Competition = feval_abs_sum(y_pred, y_val)\n",
    "print('score_for_Competition:',score_for_Competition)\n",
    " \n",
    "# 以该Conv-LSTM结构计算50步后得到score_for_Competition为600，略次于lightGBM的531.2， 记录于2021/04/22/21:37，模型名称: Conv-LSTM_ECG_0422.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "executionInfo": {
     "elapsed": 1172,
     "status": "error",
     "timestamp": 1620533065685,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "XWyEiiVISTSI",
    "outputId": "af4ac8eb-46ed-4360-d006-d37776f83c54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1980/1980 - 59s - loss: 0.0029 - accuracy: 0.9990 - val_loss: 3.2717e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "1980/1980 - 57s - loss: 0.0012 - accuracy: 0.9996 - val_loss: 6.8307e-04 - val_accuracy: 0.9997\n",
      "Epoch 3/5\n",
      "1980/1980 - 56s - loss: 8.2553e-04 - accuracy: 0.9997 - val_loss: 3.1602e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "1980/1980 - 55s - loss: 4.3734e-04 - accuracy: 0.9999 - val_loss: 0.0012 - val_accuracy: 0.9997\n",
      "Epoch 5/5\n",
      "1980/1980 - 56s - loss: 4.9860e-04 - accuracy: 0.9998 - val_loss: 3.1975e-04 - val_accuracy: 0.9997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_for_Competition: 498\n"
     ]
    }
   ],
   "source": [
    "\"\"\"GPU设置为按需增长\"\"\"\n",
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    " \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #有多个GPU时可以指定只使用第几号GPU\n",
    "config = tf.ConfigProto()\n",
    "config.allow_soft_placement=True #允许动态放置张量和操作符\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4 #最多使用40%GPU内存\n",
    "config.gpu_options.allow_growth=True   #初始化时不全部占满GPU显存, 按需分配 \n",
    "sess = tf.Session(config = config)\n",
    "set_session(sess)\n",
    " \n",
    "from keras.models import load_model\n",
    "import keras\n",
    "# path_model = '/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/Model/Conv-LSTM_ECG_0429.h5'\n",
    "# model_CNN = load_model(path_model)\n",
    "optimizer = keras.optimizers.Adam(lr=1e-4, beta_1=0.9, beta_2=0.99, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=36, verbose=2, validation_split =0.05)\n",
    "# 30 + 20 + 50 + 50 + 50\n",
    "# model.save('/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/Model/Conv-LSTM_ECG_0429.h5')\n",
    "y_pred = model.predict_classes(X_val)  # 亦可用predict_proba函数得到各类别的预测概率（PS：不需要转换为OneHot code)\n",
    " \n",
    "score_for_Competition = feval_abs_sum(y_pred, y_val)\n",
    "print('score_for_Competition:',score_for_Competition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5507,
     "status": "ok",
     "timestamp": 1619642176210,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "h5q9bwD45gBc",
    "outputId": "478eca10-7b56-4db7-d821-13950cf18325"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_for_Competition: 590\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt \n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.objectives import *\n",
    "from tensorflow.keras.layers import ConvLSTM2D\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.models import load_model\n",
    "\n",
    "# reshape input from 3dims to 5dims\n",
    "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n",
    "n_steps, n_length = 5, 41  # 设置子序列个数和长度\n",
    "X_train = X_train.reshape((X_train.shape[0], n_steps, 1, n_length, n_features))\n",
    "X_val = X_val.reshape((X_val.shape[0], n_steps, 1, n_length, n_features))\n",
    "\n",
    "# model_path = '/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/Model/Conv-LSTM_ECG_0428.h5'\n",
    "\n",
    "# model_xcc = load_model(model_path)\n",
    "\n",
    "def feval_abs_sum(preds, labels):  # 导入的都是稀疏标签\n",
    "  score_for_Competition = 0\n",
    "  for i in range(preds.shape[0]):\n",
    "    if preds[i] != np.array(labels)[i]:\n",
    "      score_for_Competition += 2\n",
    "  return score_for_Competition\n",
    "\n",
    "y_pred = model.predict_classes(X_val)\n",
    "score_for_Competition = feval_abs_sum(y_pred, y_val)\n",
    "print('score_for_Competition:',score_for_Competition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvYpMUi0iPdZ"
   },
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrCPPA3fb7ga"
   },
   "outputs": [],
   "source": [
    "# 划分X_train, X_val, y_train, y_val\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8JHNTqKijHb"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "def build_model_rf(X_train, y_train):\n",
    "  model = RandomForestClassifier(n_estimators=100)\n",
    "  model.fit(X_train, y_train)\n",
    "  return model\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# 赛制评分标准\n",
    "def feval_abs_sum(preds, labels):  \n",
    "  # preds = np.argmax(preds.reshape(4,-1),axis=0)\n",
    "  onehot_encoder = OneHotEncoder(sparse=False)\n",
    "  labels_ = onehot_encoder.fit_transform(labels.values.reshape(len(labels), 1))\n",
    "  preds_ = onehot_encoder.fit_transform(preds.reshape(len(preds), 1))\n",
    "  print(labels_.shape, '\\n', preds_.shape)\n",
    "  score_for_Competition = sum(sum(abs(labels_ - preds_)))\n",
    "  return score_for_Competition\n",
    "\n",
    "# 单模/弱分类器训练（不含调参）\n",
    "print('predict rf...')\n",
    "model_rf = build_model_rf(X_train, y_train)\n",
    "val_rf = model_rf.predict(X_val)   # 准确来说，该处为验证环节\n",
    "# subA_rf = model_rf.predict(X_test)  # 针对testA.csv文件，是测试环节\n",
    "score_for_Competition = feval_abs_sum(val_rf, y_val)  # 注意区分y_true和y_val\n",
    "print('score_for_Competition of RF:',score_for_Competition)\n",
    "\n",
    "# 保存模型\n",
    "from sklearn.externals import joblib\n",
    "# joblib.dump(model_rf,\"/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/Model/RF_ECG_0424.pkl\")\n",
    "\n",
    "#2.加载模型\n",
    "clf2 = joblib.load('classification.pkl')\n",
    "\n",
    "'''\n",
    "predict rf...\n",
    "(30000, 4) \n",
    "(30000, 4)\n",
    "score_for_Competition of RF: 1162.0\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3352,
     "status": "ok",
     "timestamp": 1619277136098,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "izvtRdzg4srx",
    "outputId": "9ae838ac-221e-4e7a-d704-21fd5d967eff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 4) \n",
      " (25000, 4)\n",
      "220.0\n"
     ]
    }
   ],
   "source": [
    "def feval_abs_sum(preds, labels):  \n",
    "  # preds = np.argmax(preds.reshape(4,-1),axis=0)\n",
    "  onehot_encoder = OneHotEncoder(sparse=False)\n",
    "  labels_ = onehot_encoder.fit_transform(labels.values.reshape(len(labels), 1))\n",
    "  preds_ = onehot_encoder.fit_transform(preds.reshape(len(preds), 1))\n",
    "  print(labels_.shape, '\\n', preds_.shape)\n",
    "  score_for_Competition = sum(sum(abs(labels_ - preds_)))\n",
    "  return score_for_Competition\n",
    "\n",
    "\n",
    "# 加载模型\n",
    "from sklearn.externals import joblib\n",
    "model_rf = joblib.load(\"/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/Model/RF_ECG_0424.pkl\")\n",
    "\n",
    "val_rf = model_rf.predict(X_val)   # 准确来说，该处为验证环节\n",
    "# subA_rf = model_rf.predict(X_test)  # 针对testA.csv文件，是测试环节\n",
    "score_for_Competition = feval_abs_sum(val_rf, y_val)  # 注意区分y_true和y_val\n",
    "print(score_for_Competition)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76Hdstip0Kto"
   },
   "source": [
    "## 模型融合\n",
    "\n",
    "首先，基于理论知识和经验，lightGBM和RF都是树模型，做模型融合似乎意义不大，且在各大机器学习赛事中lightGBM以超高的频次刷榜。考虑到Conv1D和LSTM均取得了显著效果，因此考虑lightGBM与这两种模型进行融合：（模型融合之前需要先将各个单模调至最优）\n",
    "\n",
    "① lightGBM + Conv1D\n",
    "\n",
    "② lightGBM + LSTM\n",
    "\n",
    "③ lightGBM + Conv1D + LSTM\n",
    "\n",
    "*-------准备工作-------*\n",
    "\n",
    "① 评估模型得分与预测结果的差异，目的是挑选出得分相近结果分化的模型组合，方便施加“结果融合”。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcU_vxVyW-P8"
   },
   "source": [
    "### Voting结果融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hAG86SfzW_G"
   },
   "outputs": [],
   "source": [
    "# 划分X_train, X_val, y_train, y_val\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "executionInfo": {
     "elapsed": 15073,
     "status": "error",
     "timestamp": 1619540549421,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "ZXSL3xpM0nkU",
    "outputId": "6025f93b-aa48-4276-d5e7-14912038820d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-bc47c00ed0d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mmodel_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mResult_Different_of_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResult_preds\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mDifferent_of_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-71-bc47c00ed0d0>\u001b[0m in \u001b[0;36mDifferent_of_preds\u001b[0;34m(model_1, model_2, model_3, model_4, model_5)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mpred_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# 两两对照比较差异\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mdifferent_1_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpred_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0mdifferent_1_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpred_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mdifferent_1_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpred_4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from keras.models import load_model\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def Different_of_preds(model_1, model_2, model_3, model_4, model_5):\n",
    "  pred_1 = model_1.predict(X_val)\n",
    "  pred_2 = model_2.predict_proba(X_val.values.reshape(X_val.shape[0], X_val.shape[1],1))\n",
    "  pred_3 = model_3.predict_proba(X_val.values.reshape(X_val.shape[0], X_val.shape[1],1))\n",
    "  pred_4 = model_4.predict_proba(X_val.values.reshape((X_val.shape[0], 5, 1, 41, 1)))\n",
    "  pred_5 = model_5.predict_proba(X_val)\n",
    "  # 两两对照比较差异\n",
    "  different_1_2 = sum(sum(abs(pred_1 - pred_2)))\n",
    "  different_1_3 = sum(sum(abs(pred_1 - pred_3)))\n",
    "  different_1_4 = sum(sum(abs(pred_1 - pred_4)))\n",
    "  different_1_5 = sum(sum(abs(pred_1 - pred_5)))\n",
    "  different_2_3 = sum(sum(abs(pred_2 - pred_3)))\n",
    "  different_2_4 = sum(sum(abs(pred_2 - pred_4)))\n",
    "  different_2_5 = sum(sum(abs(pred_2 - pred_5)))\n",
    "  different_3_4 = sum(sum(abs(pred_3 - pred_4)))\n",
    "  different_3_5 = sum(sum(abs(pred_3 - pred_5)))\n",
    "  different_4_5 = sum(sum(abs(pred_4 - pred_5)))\n",
    "  \n",
    "  Result_Different_of_preds = [different_1_2, different_1_3, different_1_4, different_1_5, \\\n",
    "                  different_2_3, different_2_4, different_2_5, different_3_4, \\\n",
    "                  different_3_5, different_4_5]\n",
    "  Result_preds = [pred_1, pred_2, pred_3, pred_4, pred_5]\n",
    "  return Result_Different_of_preds, Result_preds\n",
    "\n",
    "root_path_model = '/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/Model/'\n",
    "path_model = [root_path_model + 'LightGBM_ECG.txt',   # 531.2\n",
    "        root_path_model + 'LSTM_ECG_0424-128.h5',  # 446\n",
    "        root_path_model + 'CNN_ECG_0427_1.h5',   # 634\n",
    "        root_path_model + 'Conv-LSTM_ECG_0422.h5',  # 600\n",
    "        root_path_model + 'RF_ECG_0424.pkl'     # 994\n",
    "        ]\n",
    "model_1 = lgb.Booster(model_file=path_model[0])\n",
    "model_2 = load_model(path_model[1])\n",
    "model_3 = load_model(path_model[2])\n",
    "model_4 = load_model(path_model[3])\n",
    "model_5 = joblib.load(path_model[4])\n",
    "\n",
    "Result_Different_of_preds, Result_preds =  Different_of_preds(model_1, model_2, model_3, model_4, model_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2014,
     "status": "ok",
     "timestamp": 1619284164625,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "JlIFyLfs5aUT",
    "outputId": "246ac7e3-36f9-429e-98e5-d7c62f14d2aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[484.37289277862794, 617.7208580975662, 567.846786585735, 1327.0504731352892, 523.1990756988525, 420.26441383361816, 1480.5633889219894, 615.8993053436279, 1503.3095344112648, 1541.931601877929]\n"
     ]
    }
   ],
   "source": [
    "print(Result_Different_of_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 885,
     "status": "ok",
     "timestamp": 1619495966042,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "eLvvGbttZnUU",
    "outputId": "eabc1c9f-249b-497d-bcd0-76113ba703ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[9.99821361e-01, 4.11225380e-05, 1.34671616e-04, 2.84512583e-06],\n",
       "        [9.99645571e-01, 1.28558224e-04, 2.17902417e-04, 7.96793342e-06],\n",
       "        [9.99606834e-01, 2.91000079e-04, 1.75787175e-05, 8.45876689e-05],\n",
       "        ...,\n",
       "        [9.99907327e-01, 3.21730720e-05, 5.11588748e-05, 9.34077182e-06],\n",
       "        [9.99761423e-01, 2.12355551e-04, 1.58495158e-05, 1.03720650e-05],\n",
       "        [9.99957827e-01, 8.98652523e-06, 2.78226537e-05, 5.36374421e-06]]),\n",
       " array([[9.99918103e-01, 5.84226800e-05, 2.31177146e-05, 3.81509238e-07],\n",
       "        [9.99843001e-01, 1.28381012e-04, 2.80892637e-05, 5.54846736e-07],\n",
       "        [9.99998093e-01, 1.09249140e-06, 6.81527126e-07, 1.02731164e-07],\n",
       "        ...,\n",
       "        [9.99987841e-01, 1.07461237e-05, 1.42397175e-06, 1.05139764e-08],\n",
       "        [9.99739587e-01, 2.59416323e-04, 8.40757707e-07, 8.17772730e-08],\n",
       "        [9.99988675e-01, 8.29538021e-06, 2.37323707e-06, 5.76691662e-07]],\n",
       "       dtype=float32),\n",
       " array([[9.9999964e-01, 3.3001587e-07, 4.2189119e-14, 4.3040348e-29],\n",
       "        [9.9999976e-01, 1.8177232e-07, 4.0362446e-19, 1.4540193e-31],\n",
       "        [9.9999976e-01, 2.0702981e-07, 2.7606345e-23, 1.0114862e-23],\n",
       "        ...,\n",
       "        [9.9934179e-01, 6.5814552e-04, 7.3079438e-11, 1.1492325e-20],\n",
       "        [1.0000000e+00, 4.9264587e-10, 3.8581268e-22, 1.2543110e-28],\n",
       "        [9.9994981e-01, 4.9960287e-05, 2.1286399e-07, 1.5941677e-19]],\n",
       "       dtype=float32),\n",
       " array([[9.9997652e-01, 2.5103657e-06, 2.0925012e-05, 5.4484477e-23],\n",
       "        [9.9999988e-01, 1.3565247e-07, 1.6134620e-11, 8.7275907e-21],\n",
       "        [9.9971563e-01, 2.8440327e-04, 1.8894905e-10, 1.5229068e-09],\n",
       "        ...,\n",
       "        [1.0000000e+00, 1.4374668e-08, 6.5693033e-12, 1.3430193e-18],\n",
       "        [1.0000000e+00, 6.5104216e-10, 1.9407737e-21, 7.4681762e-22],\n",
       "        [9.9999857e-01, 1.4125510e-06, 1.1954149e-09, 6.6393771e-17]],\n",
       "       dtype=float32),\n",
       " array([[1.   , 0.   , 0.   , 0.   ],\n",
       "        [1.   , 0.   , 0.   , 0.   ],\n",
       "        [0.975, 0.015, 0.005, 0.005],\n",
       "        ...,\n",
       "        [1.   , 0.   , 0.   , 0.   ],\n",
       "        [1.   , 0.   , 0.   , 0.   ],\n",
       "        [1.   , 0.   , 0.   , 0.   ]])]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Result_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "executionInfo": {
     "elapsed": 1347,
     "status": "error",
     "timestamp": 1619531444371,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "TIGya6zVz8_8",
    "outputId": "19bd2fc4-7b16-44e5-ed86-04d9acedde56"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-966a98b764f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResult_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResult_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0monehot_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabels_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monehot_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Result_preds' is not defined"
     ]
    }
   ],
   "source": [
    "print(np.array(Result_preds).shape)\n",
    "print(np.array(Result_preds[0].shape))\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "labels_ = onehot_encoder.fit_transform(y_val.values.reshape(len(y_val), 1))\n",
    "# score_for_Competition = sum(sum(abs(labels_ - Result_preds[0])))\n",
    "# print(score_for_Competition)\n",
    "for i in range(5):\n",
    "  score_for_Competition = sum(sum(abs(labels_ - Result_preds[i])))\n",
    "  print(score_for_Competition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1636,
     "status": "ok",
     "timestamp": 1619284187890,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "Uj8afAS4JozN",
    "outputId": "aae0118f-c7a0-4ac0-e475-d7a6f6164de2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 4)\n",
      "0.9999999888523176\n",
      "score_for_Competition: 357.42971866257614\n"
     ]
    }
   ],
   "source": [
    "def feval_abs_sum(preds, labels):\n",
    "  # preds = np.argmax(preds.reshape(4,-1),axis=0)\n",
    "  preds_ = preds\n",
    "  onehot_encoder = OneHotEncoder(sparse=False)\n",
    "  labels_ = onehot_encoder.fit_transform(labels.values.reshape(len(labels), 1))\n",
    "  # preds_ = onehot_encoder.fit_transform(preds.reshape(len(preds), 1))\n",
    "  # print(labels_.shape, '\\n', preds_.shape)\n",
    "  score_for_Competition = sum(sum(abs(labels_ - preds_)))\n",
    "  return score_for_Competition\n",
    "\n",
    "Result_xcc = np.array(Result_preds[:-1]).mean(axis=0)\n",
    "print(Result_xcc.shape)  # (25000, 4)\n",
    "print(sum(Result_xcc[0]))\n",
    "\n",
    "score_for_Competition = feval_abs_sum(Result_xcc, y_val)  # 注意区分y_true和y_val\n",
    "print('score_for_Competition:',score_for_Competition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fykVdTlKXSLM"
   },
   "source": [
    "### Stacking融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "executionInfo": {
     "elapsed": 8115,
     "status": "error",
     "timestamp": 1620804200265,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "2GozLfmq8Ltx",
    "outputId": "41fae6d9-fcf0-4be5-cca5-52e3821e496a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id       s_0       s_1  ...     s_202     s_203     s_204\n",
      "0  100000.0  0.991571  1.000000  ...  0.000000  0.000000  0.000000\n",
      "1  100001.0  0.607553  0.541708  ...  0.350575  0.350565  0.363874\n",
      "2  100002.0  0.975273  0.671097  ...  0.000000  0.000000  0.000000\n",
      "3  100003.0  0.995635  0.917025  ...  0.000000  0.000000  0.000000\n",
      "4  100004.0  1.000000  0.887949  ...  0.000000  0.000000  0.000000\n",
      "\n",
      "[5 rows x 206 columns]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-97bfdb4b6327>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# 划分X_train, X_val, y_train, y_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 输出预测结果\n",
    "X_test = pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/testA.csv')\n",
    "\n",
    "# X_test处理\n",
    "X_test_list = []\n",
    "for item in X_test.values:\n",
    "  X_test_list.append([item[0]] + [float(i) for i in item[1].split(',')])\n",
    "X_test = pd.DataFrame(np.array(X_test_list))\n",
    "X_test.columns = ['id'] + ['s_' + str(i) for i in range(len(X_test_list[0])-1)]\n",
    "# X_test = X_test.drop(['id'], axis=1)\n",
    "# X_test = reduce_mem_usage(X_test)\n",
    "print(X_test.head())\n",
    "\n",
    "X_test.to_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/features_X_test.csv')\n",
    "\n",
    "# 划分X_train, X_val, y_train, y_val\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11271,
     "status": "ok",
     "timestamp": 1619700549305,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "GmZstLEi8sa0",
    "outputId": "18aa5c7c-49a3-41b3-be17-5956e08a4bf2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n",
      "74\n",
      "28\n",
      "168\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from keras.models import load_model\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "root_path_model = '/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/Model/'\n",
    "path_model = [root_path_model + 'LightGBM_ECG.txt',   # 531.2\n",
    "        root_path_model + 'LSTM_ECG_0429-128.h5',  # 446\n",
    "        root_path_model + 'CNN_ECG_0429_1.h5',   # 634\n",
    "        root_path_model + 'Conv-LSTM_ECG_0428.h5',  # 600\n",
    "        root_path_model + 'RF_ECG_0424.pkl'     # 994\n",
    "        ]\n",
    "model_1 = lgb.Booster(model_file=path_model[0])\n",
    "model_2 = load_model(path_model[1])\n",
    "model_3 = load_model(path_model[2])\n",
    "model_4 = load_model(path_model[3])\n",
    "\n",
    "pred_1 = model_1.predict(X_val)  # 结果是类别的概率分布，与其他模型输出类型一致，即稀疏标签类型\n",
    "pred_2 = model_2.predict_classes(X_val.values.reshape(X_val.shape[0], X_val.shape[1],1))\n",
    "pred_3 = model_3.predict_classes(X_val.values.reshape(X_val.shape[0], X_val.shape[1],1))\n",
    "pred_4 = model_4.predict_classes(X_val.values.reshape((X_val.shape[0], 5, 1, 41, 1)))\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "pred_1 = np.argmax(pred_1,axis=1)\n",
    "# pred_1 = onehot_encoder.fit_transform(pred_1.reshape(pred_1.shape[0], 1))\n",
    "\n",
    "Result_preds = [pred_1, pred_2, pred_3, pred_4]\n",
    "\n",
    "def feval_abs_sum(preds, labels):  # 导入的都是稀疏标签\n",
    "  score_for_Competition = 0\n",
    "  for i in range(preds.shape[0]):\n",
    "    if preds[i] != labels[i]:\n",
    "      score_for_Competition += 2\n",
    "  return score_for_Competition\n",
    "\n",
    "for i in range(4):\n",
    "  score_for_Competition = feval_abs_sum(Result_preds[i], np.array(y_val))\n",
    "  print(score_for_Competition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34718,
     "status": "ok",
     "timestamp": 1619700429014,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "mGXzJdlO0uOZ",
    "outputId": "93583799-6c2d-4a89-ae09-baea9619c032"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.005, max_depth=-1,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=200, n_jobs=-1, num_leaves=128, objective=None,\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "#### 从Result_Different_of_preds选用得分相近的模型进行堆叠融合，毕竟学霸对出来的答案更靠谱\n",
    "## 第一层\n",
    "# train_model_1_pred = model_1.predict(X_train)\n",
    "train_model_2_pred = model_2.predict_classes(X_train.values.reshape(X_train.shape[0], X_train.shape[1],1))\n",
    "train_model_3_pred = model_3.predict_classes(X_train.values.reshape(X_train.shape[0], X_train.shape[1],1))\n",
    "train_model_4_pred = model_4.predict_classes(X_train.values.reshape(X_train.shape[0], 5, 1, 41, 1))\n",
    "\n",
    "stacking_X_train = pd.DataFrame()\n",
    "# stacking_X_train['mode_1'] = np.argmax(train_model_1_pred,axis=1)\n",
    "stacking_X_train['mode_2'] = train_model_2_pred\n",
    "stacking_X_train['mode_3'] = train_model_3_pred\n",
    "stacking_X_train['mode_4'] = train_model_4_pred\n",
    "\n",
    "# val_model_1_pred = model_1.predict(X_val)\n",
    "val_model_2_pred = model_2.predict_classes(X_val.values.reshape(X_val.shape[0], X_val.shape[1],1))\n",
    "val_model_3_pred = model_3.predict_classes(X_val.values.reshape(X_val.shape[0], X_val.shape[1],1))\n",
    "val_model_4_pred = model_4.predict_classes(X_val.values.reshape(X_val.shape[0], 5, 1, 41, 1))\n",
    "\n",
    "stacking_X_val = pd.DataFrame()\n",
    "# stacking_X_val['model_1'] = np.argmax(val_model_1_pred,axis=1)\n",
    "stacking_X_val['model_2'] = val_model_2_pred\n",
    "stacking_X_val['model_3'] = val_model_3_pred\n",
    "stacking_X_val['model_4'] = val_model_4_pred\n",
    "\n",
    "# test_model_1_pred = model_1.predict(X_test)\n",
    "test_model_2_pred = model_2.predict_classes(X_test.reshape(X_test.shape[0], X_test.shape[1],1))\n",
    "test_model_3_pred = model_3.predict_classes(X_test.reshape(X_test.shape[0], X_test.shape[1],1))\n",
    "test_model_4_pred = model_4.predict_classes(X_test.reshape(X_test.shape[0], 5, 1, 41, 1))\n",
    "\n",
    "stacking_X_test = pd.DataFrame()\n",
    "# stacking_X_test['model_1'] = np.argmax(test_model_1_pred,axis=1)\n",
    "stacking_X_test['model_2'] = test_model_2_pred\n",
    "stacking_X_test['model_3'] = test_model_3_pred\n",
    "stacking_X_test['model_4'] = test_model_4_pred\n",
    "\n",
    "## 第二层用lightGBM预测\n",
    "model_lgb_stacking = lgb.LGBMClassifier(num_leaves=128, learning_rate=0.005, n_estimators=200)\n",
    "model_lgb_stacking.fit(stacking_X_train, y_train)\n",
    "\n",
    "\n",
    "# 第二层用人工神经网络（因为融合模型的本质应该是一个非线性映射，对单模进行优势互补，用人工神经网络来做比较合适\n",
    "# from sklearn.neural_network import MLPClassifier,MLPRegressor\n",
    "# model_nn_stacking = MLPRegressor(alpha=1e-05, hidden_layer_sizes=(200,4), random_state=1, solver='lbfgs')\n",
    "# model_nn_stacking.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# 对单模LSTM、CNN_1D、Conv-LSTM三个单模进行stacking融合可能会比对4个单模进行融合要好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 183100,
     "status": "ok",
     "timestamp": 1619568623787,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "XWxkAwmjzQdB",
    "outputId": "0cab43fd-9e7f-4da6-cfbf-571818ffc27f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=(200, 4), learning_rate='constant',\n",
       "             learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "             power_t=0.5, random_state=1, shuffle=True, solver='lbfgs',\n",
       "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier,MLPRegressor\n",
    "model_nn_stacking = MLPRegressor(alpha=1e-05, hidden_layer_sizes=(200,4), random_state=1, solver='lbfgs')\n",
    "model_nn_stacking.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4713,
     "status": "ok",
     "timestamp": 1619700470016,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "NTE4HXcIuRgY",
    "outputId": "04184ce0-18d8-4efe-b4dc-3781ae121219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_for_Competition in Train_Dataset: 60\n",
      "score_for_Competition in Train_Dataset: 36\n",
      "预测准确率： 0.99928\n",
      "[0. 2. 3. ... 2. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def feval_abs_sum(preds, labels):  # 导入的都是稀疏标签\n",
    "  score_for_Competition = 0\n",
    "  for i in range(preds.shape[0]):\n",
    "    if preds[i] != np.array(labels)[i]:\n",
    "      score_for_Competition += 2\n",
    "  return score_for_Competition\n",
    "\n",
    "# 训练集上预测\n",
    "train_pred_Stacking = model_lgb_stacking.predict(stacking_X_train)\n",
    "# train_pred_Stacking = np.argmax(train_pred_Stacking,axis=1)\n",
    "score_for_Competition = feval_abs_sum(train_pred_Stacking, y_train)  # 注意区分y_true和y_val\n",
    "print('score_for_Competition in Train_Dataset:',score_for_Competition)\n",
    "\n",
    "# 验证集上预测\n",
    "val_pred_Stacking = model_lgb_stacking.predict(stacking_X_val)\n",
    "# val_pred_Stacking = np.argmax(val_pred_Stacking,axis=1)\n",
    "score_for_Competition = feval_abs_sum(val_pred_Stacking, y_val)  # 注意区分y_true和y_val\n",
    "print('score_for_Competition in Train_Dataset:',score_for_Competition)\n",
    "sum = 0\n",
    "for i in range(y_val.shape[0]):\n",
    "  if val_pred_Stacking[i] == np.array(y_val)[i]:\n",
    "    sum += 1\n",
    "\n",
    "print('预测准确率：', float(sum/y_val.shape[0]))\n",
    "\n",
    "\n",
    "# 测试集上预测\n",
    "test_pred_Stacking = model_lgb_stacking.predict(stacking_X_test)\n",
    "# test_pred_Stacking = np.argmax(test_pred_Stacking,axis=1)\n",
    "print(test_pred_Stacking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1879,
     "status": "ok",
     "timestamp": 1619500768777,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "_wBl6KzIo345",
    "outputId": "f4958aba-3612-460f-cc09-efa1d89ed2f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n"
     ]
    }
   ],
   "source": [
    "# score_for_Competition = sum(sum(abs(labels_ - preds_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2159,
     "status": "ok",
     "timestamp": 1619559059141,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "nGuLFM58jULs",
    "outputId": "6b29078c-6c14-42fa-8876-eb796799a2cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "           id  label_0  label_1  label_2  label_3\n",
      "0      100000      1.0      0.0      0.0      0.0\n",
      "1      100001      0.0      0.0      1.0      0.0\n",
      "2      100002      0.0      0.0      0.0      1.0\n",
      "3      100003      1.0      0.0      0.0      0.0\n",
      "4      100004      1.0      0.0      0.0      0.0\n",
      "...       ...      ...      ...      ...      ...\n",
      "19995  119995      1.0      0.0      0.0      0.0\n",
      "19996  119996      1.0      0.0      0.0      0.0\n",
      "19997  119997      0.0      0.0      1.0      0.0\n",
      "19998  119998      1.0      0.0      0.0      0.0\n",
      "19999  119999      1.0      0.0      0.0      0.0\n",
      "\n",
      "[20000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "y_preds_subm = test_pred_Stacking\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_preds_subm = onehot_encoder.fit_transform(y_preds_subm.reshape(test_pred_Stacking.shape[0],1))\n",
    "print(y_preds_subm)\n",
    "\n",
    "result=pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/sample_submit.csv')\n",
    "result['label_0']=y_preds_subm[:,0]\n",
    "result['label_1']=y_preds_subm[:,1]\n",
    "result['label_2']=y_preds_subm[:,2]\n",
    "result['label_3']=y_preds_subm[:,3]\n",
    "print(result)\n",
    "result.to_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/submit.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 988,
     "status": "ok",
     "timestamp": 1619559390735,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "KeWvmA3IKyXi",
    "outputId": "29a9e5d4-f29a-4a79-ff39-37fb8dcb8259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99928\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in range(y_val.shape[0]):\n",
    "  if val_pred_Stacking[i] == np.array(y_val)[i]:\n",
    "    sum += 1\n",
    "\n",
    "print(float(sum/y_val.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYqA-YzFsMbU"
   },
   "source": [
    "# 易明建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55194,
     "status": "ok",
     "timestamp": 1620003378996,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "sS6uPd_-tNf0",
    "outputId": "3d5cb7ee-cfe6-477a-d20a-42cff204f0dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 157.93 MB\n",
      "Memory usage after optimization is: 39.67 MB\n",
      "Decreased by 74.9%\n",
      "Memory usage of dataframe is 31.28 MB\n",
      "Memory usage after optimization is: 7.82 MB\n",
      "Decreased by 75.0%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 导入包\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# 数据预处理\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2 \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2 \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/train.csv')\n",
    "data_features_filtered = pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Features_Extract/train_features_filtered_MinimalFCParameters.csv')\n",
    "X_test = pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/testA.csv').values\n",
    "\n",
    "# 原始特征Dataframe\n",
    "data_list = []\n",
    "for item in data.values:\n",
    "  data_list.append([item[0]] + [float(i) for i in item[1].split(',')] + [item[2]])\n",
    "data = pd.DataFrame(np.array(data_list))\n",
    "data.columns = ['id'] + ['s_' + str(i) for i in range(len(data_list[0])-2)] + ['label']\n",
    "\n",
    "data = reduce_mem_usage(data)\n",
    "\n",
    "data.to_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/Data_features_labels.csv')\n",
    "\n",
    "\n",
    "\n",
    "# 输出预测结果\n",
    "X_test = pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/testA.csv')\n",
    "\n",
    "# X_test处理\n",
    "X_test_list = []\n",
    "for item in X_test.values:\n",
    "  X_test_list.append([item[0]] + [float(i) for i in item[1].split(',')])\n",
    "X_test = pd.DataFrame(np.array(X_test_list))\n",
    "X_test.columns = ['id'] + ['s_' + str(i) for i in range(len(data_list[0])-2)]\n",
    "X_test = X_test.drop(['id'], axis=1)\n",
    "X_test = reduce_mem_usage(X_test)\n",
    "\n",
    "X_test.to_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/X_test_features.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE-NgV2dug20"
   },
   "source": [
    "## 易明建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYSGd9HsuR2a"
   },
   "source": [
    "## 结果形式调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1136,
     "status": "ok",
     "timestamp": 1620005481424,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "yBocXVu6sQVA",
    "outputId": "3273777d-2076-4a5d-9d22-8bbed387f101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 4)\n",
      "(20000,)\n",
      "(20000, 4)\n",
      "           id  label_0  label_1  label_2  label_3\n",
      "0      100000        0        0        0        0\n",
      "1      100001        0        0        0        0\n",
      "2      100002        0        0        0        0\n",
      "3      100003        0        0        0        0\n",
      "4      100004        0        0        0        0\n",
      "...       ...      ...      ...      ...      ...\n",
      "19995  119995        0        0        0        0\n",
      "19996  119996        0        0        0        0\n",
      "19997  119997        0        0        0        0\n",
      "19998  119998        0        0        0        0\n",
      "19999  119999        0        0        0        0\n",
      "\n",
      "[20000 rows x 5 columns]\n",
      "           id  label_0  label_1  label_2  label_3\n",
      "0      100000      1.0      0.0      0.0      0.0\n",
      "1      100001      0.0      0.0      1.0      0.0\n",
      "2      100002      0.0      0.0      0.0      1.0\n",
      "3      100003      1.0      0.0      0.0      0.0\n",
      "4      100004      1.0      0.0      0.0      0.0\n",
      "...       ...      ...      ...      ...      ...\n",
      "19995  119995      1.0      0.0      0.0      0.0\n",
      "19996  119996      1.0      0.0      0.0      0.0\n",
      "19997  119997      0.0      0.0      1.0      0.0\n",
      "19998  119998      1.0      0.0      0.0      0.0\n",
      "19999  119999      1.0      0.0      0.0      0.0\n",
      "\n",
      "[20000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# 使用易明数据建模软件预测的结果提交\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "result_proba_ym = pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/submit_ym.csv')\n",
    "print(result_proba_ym.shape)\n",
    "result_ym = np.argmax(result_proba_ym.values, axis=1)\n",
    "print(result_ym.shape)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "result_ym = onehot_encoder.fit_transform(result_ym.reshape(result_ym.shape[0],1))\n",
    "print(result_ym.shape)\n",
    "\n",
    "# 生成可提交的结果\n",
    "result=pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/sample_submit.csv')\n",
    "print(result)\n",
    "result['label_0']=result_ym[:,0]\n",
    "result['label_1']=result_ym[:,1]\n",
    "result['label_2']=result_ym[:,2]\n",
    "result['label_3']=result_ym[:,3]\n",
    "print(result)\n",
    "result.to_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/submit.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRA5Pe9O9xyW"
   },
   "source": [
    "# AutoML\n",
    "\n",
    "auto-sklearn\n",
    "autogluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXJZc6kx93sn"
   },
   "source": [
    "## Auto-sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yw_pE-nAxBz-"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 76643,
     "status": "ok",
     "timestamp": 1620872613279,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "qRjochmD98PO",
    "outputId": "07e468e3-0b8d-4479-afea-7a1edd430be2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting auto-sklearn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/1b/9249f7d4498cbdb0130352a838e02ca7d7bebbcacde74d1f4f26a5d9202b/auto-sklearn-0.12.6.tar.gz (6.1MB)\n",
      "\u001b[K     |████████████████████████████████| 6.1MB 11.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (56.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.14.1 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (1.4.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (1.0.1)\n",
      "Collecting scikit-learn<0.25.0,>=0.24.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/eb/a48f25c967526b66d5f1fa7a984594f0bf0a5afafa94a8c4dbc317744620/scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
      "\u001b[K     |████████████████████████████████| 22.3MB 31.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: dask in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (2.12.0)\n",
      "Collecting distributed>=2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/f8/ac2c18adde6477bca3881c4d3cfa74c7f4da7ee82f3c83c201aa3b9ca5ee/distributed-2021.4.1-py3-none-any.whl (696kB)\n",
      "\u001b[K     |████████████████████████████████| 706kB 43.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (3.13)\n",
      "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (1.1.5)\n",
      "Collecting liac-arff\n",
      "  Downloading https://files.pythonhosted.org/packages/6e/43/73944aa5ad2b3185c0f0ba0ee6f73277f2eb51782ca6ccf3e6793caf209a/liac-arff-2.5.0.tar.gz\n",
      "Collecting ConfigSpace<0.5,>=0.4.14\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/c3/3c21e8d82a639fd821f538e6d7f830b654a6ce1fe52644be1a67f323f707/ConfigSpace-0.4.18.tar.gz (950kB)\n",
      "\u001b[K     |████████████████████████████████| 952kB 38.4MB/s \n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting pynisher>=0.6.3\n",
      "  Downloading https://files.pythonhosted.org/packages/8d/39/edac9acf3bd245ecf475151014cce3652c25ca3c2352eac725502cfce6ea/pynisher-0.6.4.tar.gz\n",
      "Collecting pyrfr<0.9,>=0.8.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/1a/56b630c949e942d12f4ad5f4fd240c1cf2e1260e5126190b171ca2aa9199/pyrfr-0.8.2-cp37-cp37m-manylinux2014_x86_64.whl (4.0MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0MB 42.1MB/s \n",
      "\u001b[?25hCollecting smac<0.14,>=0.13.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/f2/8ea040eaa2253a3606472b08d9c2a23be1a177c0c19e236a2b3222c0fd78/smac-0.13.1.tar.gz (258kB)\n",
      "\u001b[K     |████████████████████████████████| 266kB 48.1MB/s \n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.2.0->auto-sklearn) (1.7.0)\n",
      "Requirement already satisfied: tornado>=5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from distributed>=2.2.0->auto-sklearn) (5.1.1)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.2.0->auto-sklearn) (0.11.1)\n",
      "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.2.0->auto-sklearn) (7.1.2)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.2.0->auto-sklearn) (2.3.0)\n",
      "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.2.0->auto-sklearn) (5.4.8)\n",
      "Collecting cloudpickle>=1.5.0\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/e3/898487e5dbeb612054cf2e0c188463acb358167fef749c53c8bb8918cea1/cloudpickle-1.6.0-py3-none-any.whl\n",
      "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.2.0->auto-sklearn) (2.0.0)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.2.0->auto-sklearn) (1.0.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->auto-sklearn) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->auto-sklearn) (2.8.1)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from ConfigSpace<0.5,>=0.4.14->auto-sklearn) (0.29.23)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from ConfigSpace<0.5,>=0.4.14->auto-sklearn) (2.4.7)\n",
      "Collecting lazy_import\n",
      "  Downloading https://files.pythonhosted.org/packages/44/2e/5378f9b9cbc893826c2ecb022646c97ece9efbaad351adf89425fff33990/lazy_import-0.2.2.tar.gz\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.2.0->auto-sklearn) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0->auto-sklearn) (1.15.0)\n",
      "Building wheels for collected packages: ConfigSpace\n",
      "  Building wheel for ConfigSpace (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ConfigSpace: filename=ConfigSpace-0.4.18-cp37-cp37m-linux_x86_64.whl size=2879812 sha256=d7b4843dc0646f22b882efd9bf93d12f38cc45d0eead22517605f7a5f03674d5\n",
      "  Stored in directory: /root/.cache/pip/wheels/39/ea/40/d93931850f700427db0a84180829c709d30484c9475040c7bd\n",
      "Successfully built ConfigSpace\n",
      "Building wheels for collected packages: auto-sklearn, liac-arff, pynisher, smac, lazy-import\n",
      "  Building wheel for auto-sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for auto-sklearn: filename=auto_sklearn-0.12.6-cp37-none-any.whl size=6370105 sha256=57df4cc3397307c3d2cda0012b7502ea0d043d666e469ef8fd9d39a00ad8bafa\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/c8/1f/3a6d11c1e156bf431e7cc4c4ff27e71059acc9638caa11ab35\n",
      "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for liac-arff: filename=liac_arff-2.5.0-cp37-none-any.whl size=11732 sha256=78da8fbe3b4b0e969b50105fe6c7a84353d215059c9d892c80bfa01c6ca563dc\n",
      "  Stored in directory: /root/.cache/pip/wheels/77/8d/b4/8bfce5beea9a3496cc15b24961876adb7b6e2912ff09164179\n",
      "  Building wheel for pynisher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pynisher: filename=pynisher-0.6.4-cp37-none-any.whl size=7045 sha256=605c742e2506d7ba7be43141a441e1d4bc309873b5f13f42c7541fc309fa108c\n",
      "  Stored in directory: /root/.cache/pip/wheels/70/07/6b/c0e6d547d91cd50a30207421c3c3a63d71f195255c66401209\n",
      "  Building wheel for smac (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for smac: filename=smac-0.13.1-cp37-none-any.whl size=252183 sha256=4382e149ec078b275195bc2c332e4123e2816715273e006b8ec356ae63b08cbd\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/a6/af/9ec3c1ff517759ad1aad6babcbcf047dd5078c8b08fa4e63cc\n",
      "  Building wheel for lazy-import (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for lazy-import: filename=lazy_import-0.2.2-py2.py3-none-any.whl size=16486 sha256=f039c51d2431a3c306caa2027799887a99b7381ff74882ff268c0e449da06573\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/b0/b5/8c7e6810aee14bc4ed4a542ce56e744126263bf4f4825a9094\n",
      "Successfully built auto-sklearn liac-arff pynisher smac lazy-import\n",
      "\u001b[31mERROR: distributed 2021.4.1 has requirement dask>=2021.03.0, but you'll have dask 2.12.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: threadpoolctl, scikit-learn, cloudpickle, distributed, liac-arff, ConfigSpace, pynisher, pyrfr, lazy-import, smac, auto-sklearn\n",
      "  Found existing installation: scikit-learn 0.22.2.post1\n",
      "    Uninstalling scikit-learn-0.22.2.post1:\n",
      "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
      "  Found existing installation: cloudpickle 1.3.0\n",
      "    Uninstalling cloudpickle-1.3.0:\n",
      "      Successfully uninstalled cloudpickle-1.3.0\n",
      "  Found existing installation: distributed 1.25.3\n",
      "    Uninstalling distributed-1.25.3:\n",
      "      Successfully uninstalled distributed-1.25.3\n",
      "Successfully installed ConfigSpace-0.4.18 auto-sklearn-0.12.6 cloudpickle-1.6.0 distributed-2021.4.1 lazy-import-0.2.2 liac-arff-2.5.0 pynisher-0.6.4 pyrfr-0.8.2 scikit-learn-0.24.2 smac-0.13.1 threadpoolctl-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!sudo pip3 install auto-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64437,
     "status": "ok",
     "timestamp": 1620900339866,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "KOSkVOvU93IQ",
    "outputId": "03808944-0046-4da9-aadc-5cae40c53d20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting auto-sklearn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/1b/9249f7d4498cbdb0130352a838e02ca7d7bebbcacde74d1f4f26a5d9202b/auto-sklearn-0.12.6.tar.gz (6.1MB)\n",
      "\u001b[K     |████████████████████████████████| 6.1MB 8.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (56.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.14.1 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (1.4.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (1.0.1)\n",
      "Requirement already satisfied: scikit-learn<0.25.0,>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (0.24.2)\n",
      "Requirement already satisfied: dask in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (2021.4.1)\n",
      "Collecting distributed>=2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/f8/ac2c18adde6477bca3881c4d3cfa74c7f4da7ee82f3c83c201aa3b9ca5ee/distributed-2021.4.1-py3-none-any.whl (696kB)\n",
      "\u001b[K     |████████████████████████████████| 706kB 55.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (3.13)\n",
      "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn) (1.1.5)\n",
      "Collecting liac-arff\n",
      "  Downloading https://files.pythonhosted.org/packages/6e/43/73944aa5ad2b3185c0f0ba0ee6f73277f2eb51782ca6ccf3e6793caf209a/liac-arff-2.5.0.tar.gz\n",
      "Collecting ConfigSpace<0.5,>=0.4.14\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/c3/3c21e8d82a639fd821f538e6d7f830b654a6ce1fe52644be1a67f323f707/ConfigSpace-0.4.18.tar.gz (950kB)\n",
      "\u001b[K     |████████████████████████████████| 952kB 51.5MB/s \n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting pynisher>=0.6.3\n",
      "  Downloading https://files.pythonhosted.org/packages/8d/39/edac9acf3bd245ecf475151014cce3652c25ca3c2352eac725502cfce6ea/pynisher-0.6.4.tar.gz\n",
      "Collecting pyrfr<0.9,>=0.8.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/1a/56b630c949e942d12f4ad5f4fd240c1cf2e1260e5126190b171ca2aa9199/pyrfr-0.8.2-cp37-cp37m-manylinux2014_x86_64.whl (4.0MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0MB 46.1MB/s \n",
      "\u001b[?25hCollecting smac<0.14,>=0.13.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/f2/8ea040eaa2253a3606472b08d9c2a23be1a177c0c19e236a2b3222c0fd78/smac-0.13.1.tar.gz (258kB)\n",
      "\u001b[K     |████████████████████████████████| 266kB 25.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25.0,>=0.24.0->auto-sklearn) (2.1.0)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from dask->auto-sklearn) (1.3.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from dask->auto-sklearn) (0.11.1)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from dask->auto-sklearn) (2021.4.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.7/dist-packages (from dask->auto-sklearn) (1.2.0)\n",
      "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.2.0->auto-sklearn) (7.1.2)\n",
      "Requirement already satisfied: tornado>=5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from distributed>=2.2.0->auto-sklearn) (5.1.1)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.2.0->auto-sklearn) (1.0.2)\n",
      "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.2.0->auto-sklearn) (2.0.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.2.0->auto-sklearn) (1.7.0)\n",
      "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.2.0->auto-sklearn) (5.4.8)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.2.0->auto-sklearn) (2.3.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->auto-sklearn) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->auto-sklearn) (2.8.1)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from ConfigSpace<0.5,>=0.4.14->auto-sklearn) (2.4.7)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from ConfigSpace<0.5,>=0.4.14->auto-sklearn) (0.29.23)\n",
      "Collecting lazy_import\n",
      "  Downloading https://files.pythonhosted.org/packages/44/2e/5378f9b9cbc893826c2ecb022646c97ece9efbaad351adf89425fff33990/lazy_import-0.2.2.tar.gz\n",
      "Requirement already satisfied: locket in /usr/local/lib/python3.7/dist-packages (from partd>=0.3.10->dask->auto-sklearn) (0.2.1)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.2.0->auto-sklearn) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0->auto-sklearn) (1.15.0)\n",
      "Building wheels for collected packages: ConfigSpace\n",
      "  Building wheel for ConfigSpace (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ConfigSpace: filename=ConfigSpace-0.4.18-cp37-cp37m-linux_x86_64.whl size=2879758 sha256=fb09069a3fce75f291d325cb556995491394c34e0bd10c5460bc09b33878bc63\n",
      "  Stored in directory: /root/.cache/pip/wheels/39/ea/40/d93931850f700427db0a84180829c709d30484c9475040c7bd\n",
      "Successfully built ConfigSpace\n",
      "Building wheels for collected packages: auto-sklearn, liac-arff, pynisher, smac, lazy-import\n",
      "  Building wheel for auto-sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for auto-sklearn: filename=auto_sklearn-0.12.6-cp37-none-any.whl size=6370105 sha256=f8affbfc63c10bfb8946bcf8fd005a0588b888c95460abd820c5763717dfa51a\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/c8/1f/3a6d11c1e156bf431e7cc4c4ff27e71059acc9638caa11ab35\n",
      "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for liac-arff: filename=liac_arff-2.5.0-cp37-none-any.whl size=11732 sha256=89a27efeaaa7a3ebc14a46601371f7403d331f23b45d33b86e4d9f53faf09808\n",
      "  Stored in directory: /root/.cache/pip/wheels/77/8d/b4/8bfce5beea9a3496cc15b24961876adb7b6e2912ff09164179\n",
      "  Building wheel for pynisher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pynisher: filename=pynisher-0.6.4-cp37-none-any.whl size=7045 sha256=93007e78a4dd8e878d64ddf579e7f63c3d47bd7bf384cd253c51064897c05ee2\n",
      "  Stored in directory: /root/.cache/pip/wheels/70/07/6b/c0e6d547d91cd50a30207421c3c3a63d71f195255c66401209\n",
      "  Building wheel for smac (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for smac: filename=smac-0.13.1-cp37-none-any.whl size=252182 sha256=f64d4d4c4250fea5926a83e4565dc8fe43bbd69adb8ab4f621ee2a9638011952\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/a6/af/9ec3c1ff517759ad1aad6babcbcf047dd5078c8b08fa4e63cc\n",
      "  Building wheel for lazy-import (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for lazy-import: filename=lazy_import-0.2.2-py2.py3-none-any.whl size=16486 sha256=8fb774f147485d0edc8c893ab6715ff7b1dd2b285ee1adfb7cbbb60895f9c621\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/b0/b5/8c7e6810aee14bc4ed4a542ce56e744126263bf4f4825a9094\n",
      "Successfully built auto-sklearn liac-arff pynisher smac lazy-import\n",
      "\u001b[31mERROR: distributed 2021.4.1 has requirement cloudpickle>=1.5.0, but you'll have cloudpickle 1.3.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: distributed, liac-arff, ConfigSpace, pynisher, pyrfr, lazy-import, smac, auto-sklearn\n",
      "  Found existing installation: distributed 1.25.3\n",
      "    Uninstalling distributed-1.25.3:\n",
      "      Successfully uninstalled distributed-1.25.3\n",
      "Successfully installed ConfigSpace-0.4.18 auto-sklearn-0.12.6 distributed-2021.4.1 lazy-import-0.2.2 liac-arff-2.5.0 pynisher-0.6.4 pyrfr-0.8.2 smac-0.13.1\n"
     ]
    }
   ],
   "source": [
    "!sudo pip3 install auto-sklearn\n",
    "# import auto-sklearn\n",
    "# %cd \"/content/drive/Shareddrives/xucc1993.HK(CRN.NGO)/CLF_of_ECG_signals/AutoML-AutoDL/asModels-predictClass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5511,
     "status": "ok",
     "timestamp": 1620900243435,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "yzuKDc2Tk8QD",
    "outputId": "461d21f3-a278-49bc-c12e-9c5d97313575"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dask\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/23/42159cdb5c9edac174296c9976e175742513883f0f7bc132cfc2a2480fab/dask-2021.4.1-py3-none-any.whl (952kB)\n",
      "\u001b[K     |████████████████████████████████| 952kB 8.7MB/s \n",
      "\u001b[?25hCollecting fsspec>=0.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 27.1MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: cloudpickle>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from dask) (1.3.0)\n",
      "Collecting partd>=0.3.10\n",
      "  Downloading https://files.pythonhosted.org/packages/41/94/360258a68b55f47859d72b2d0b2b3cfe0ca4fbbcb81b78812bd00ae86b7c/partd-1.2.0-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from dask) (0.11.1)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from dask) (3.13)\n",
      "Collecting locket\n",
      "  Downloading https://files.pythonhosted.org/packages/50/b8/e789e45b9b9c2db75e9d9e6ceb022c8d1d7e49b2c085ce8c05600f90a96b/locket-0.2.1-py2.py3-none-any.whl\n",
      "Installing collected packages: fsspec, locket, partd, dask\n",
      "  Found existing installation: dask 2.12.0\n",
      "    Uninstalling dask-2.12.0:\n",
      "      Successfully uninstalled dask-2.12.0\n",
      "Successfully installed dask-2021.4.1 fsspec-2021.4.0 locket-0.2.1 partd-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!sudo pip3 install dask --upgrade\n",
    "# !sudo pip3 install scikit-learn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HW4rcXUbFu8V"
   },
   "outputs": [],
   "source": [
    "!bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OSlQHARxV0G"
   },
   "source": [
    "### CLF_of_ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "executionInfo": {
     "elapsed": 3892,
     "status": "error",
     "timestamp": 1620887693344,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "EWmmBirAA8VV",
    "outputId": "7e412cdc-a13f-46d0-cbc1-5720fdbef0f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pyparsing.py:3190: FutureWarning: Possible set intersection at position 3\n",
      "  self.re = re.compile(self.reString)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-449db6178d72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoSklearnClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_left_for_this_task\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mper_run_time_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_models_on_disc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_configurations_via_metalearning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msmac_scenario_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'runcount_limit'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/autosklearn/estimators.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_test, y_test, feat_type, dataset_name)\u001b[0m\n\u001b[1;32m    660\u001b[0m                              \"\".format(\n\u001b[1;32m    661\u001b[0m                                     \u001b[0mtarget_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m                                     \u001b[0msupported_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m                                 )\n\u001b[1;32m    664\u001b[0m                              )\n",
      "\u001b[0;31mValueError\u001b[0m: Classification with data of type multiclass-multioutput is not supported. Supported types are ['binary', 'multiclass', 'multilabel-indicator']. You can find more information about scikit-learn data types in: https://scikit-learn.org/stable/modules/multiclass.html"
     ]
    }
   ],
   "source": [
    "# 划分X_train, X_val, y_train, y_val\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "# X_train, X_val, y_train, y_val = train_test_split(features, labels)\n",
    "\n",
    "X_train = pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/X_train.csv')\n",
    "X_val = pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/X_val.csv')\n",
    "y_train = pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/y_train.csv')\n",
    "y_val = pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/y_val.csv')\n",
    "\n",
    "\n",
    "# import autosklearn.classification\n",
    "from autosklearn.classification import AutoSklearnClassifier\n",
    "\n",
    "cls = AutoSklearnClassifier(time_left_for_this_task=60,per_run_time_limit=30,max_models_on_disc=5,initial_configurations_via_metalearning=0,smac_scenario_args={'runcount_limit': 1})\n",
    "\n",
    "cls.fit(X_train, y_train)\n",
    "\n",
    "print(automl.show_models())\n",
    "print(automl.sprint_statistics())\n",
    "\n",
    "predictions = cls.predict(X_val)\n",
    "print(predictions)\n",
    "import sklearn\n",
    "print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 777,
     "status": "ok",
     "timestamp": 1620813458224,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "15-y4_yFynAX",
    "outputId": "28c2a383-8b64-4e41-da69-d9a9cf565285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_for_Competition in Val_Dataset: 1100\n"
     ]
    }
   ],
   "source": [
    "def feval_abs_sum(preds, labels):  # 导入的都是稀疏标签\n",
    "  score_for_Competition = 0\n",
    "  for i in range(preds.shape[0]):\n",
    "    if preds[i] != np.array(labels)[i]:\n",
    "      score_for_Competition += 2\n",
    "  return score_for_Competition\n",
    "\n",
    "score_for_Competition = feval_abs_sum(predictions, y_val)\n",
    "34\n",
    "print('score_for_Competition in Val_Dataset:',score_for_Competition)\n",
    "# score_for_Competition in Val_Dataset: 1100,   Auto-sklearn不大行啊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A37OgGYZxW2H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zp_kV_1w-KQI"
   },
   "source": [
    "## AutoGluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpXtd1EKw-Qv"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123218,
     "status": "ok",
     "timestamp": 1620775369319,
     "user": {
      "displayName": "Gabriel Fernando",
      "photoUrl": "",
      "userId": "06273348089492808596"
     },
     "user_tz": -480
    },
    "id": "sxhZ6IS_-M_j",
    "outputId": "8c2b97d8-c6c4-41e5-e62a-d6c155643a38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autogluon\n",
      "  Downloading https://files.pythonhosted.org/packages/89/38/1669479ce6a4760cd99d2ef7ffd450213bbce65c91202339cf716917bcf8/autogluon-0.2.0-py3-none-any.whl\n",
      "Collecting autogluon.text==0.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/9b/b226cbb1574cb87149727b6b84baefd99e4b12210cecffdeb9c6a55d1e8b/autogluon.text-0.2.0-py3-none-any.whl (48kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 7.3MB/s \n",
      "\u001b[?25hCollecting autogluon.tabular[all]==0.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/9c/e879ef67abf232b8f582591d4ff774e60d41833332e49dc1a2d1b22e2d38/autogluon.tabular-0.2.0-py3-none-any.whl (250kB)\n",
      "\u001b[K     |████████████████████████████████| 256kB 41.3MB/s \n",
      "\u001b[?25hCollecting autogluon.core==0.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/4d/6e8e1eb195aa09c4f62dab87a4ff1d7cb5964369606e4fe91af0bf9c5ffa/autogluon.core-0.2.0-py3-none-any.whl (334kB)\n",
      "\u001b[K     |████████████████████████████████| 337kB 50.6MB/s \n",
      "\u001b[?25hCollecting autogluon.features==0.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/a9/8277149a2517bba1e5e563e47ef1ebbf5fbe3da51342da67d6eef0580522/autogluon.features-0.2.0-py3-none-any.whl (48kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 7.4MB/s \n",
      "\u001b[?25hCollecting autogluon.mxnet==0.2.0\n",
      "  Downloading https://files.pythonhosted.org/packages/aa/ff/8f14f35051297c47a2383e51c1b8030b3a4dca1de94ca05c8f1b0611c28e/autogluon.mxnet-0.2.0-py3-none-any.whl\n",
      "Collecting autogluon.extra==0.2.0\n",
      "  Downloading https://files.pythonhosted.org/packages/ca/a4/1fd328c8187f15897e9b4cb7ef193bfe58aeca03a9621216291ff4f9a008/autogluon.extra-0.2.0-py3-none-any.whl\n",
      "Collecting autogluon.vision==0.2.0\n",
      "  Downloading https://files.pythonhosted.org/packages/32/5c/3fc553331d580ffb24a9003ab0edfa1ee83f87f6efc4345a0060b4a90f4c/autogluon.vision-0.2.0-py3-none-any.whl\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from autogluon.text==0.2.0->autogluon) (3.6.4)\n",
      "Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from autogluon.text==0.2.0->autogluon) (1.19.5)\n",
      "Requirement already satisfied: pandas<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.text==0.2.0->autogluon) (1.1.5)\n",
      "Collecting autogluon-contrib-nlp==0.0.1b20210201\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/22/29037f78a5b05b2749a89e19497d2498b7ba200921d482ee9bd99293f398/autogluon_contrib_nlp-0.0.1b20210201-py3-none-any.whl (157kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 48.4MB/s \n",
      "\u001b[?25hCollecting scipy<1.7,>=1.5.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e8/43ffca541d2f208d516296950b25fe1084b35c2881f4d444c1346ca75815/scipy-1.6.3-cp37-cp37m-manylinux1_x86_64.whl (27.4MB)\n",
      "\u001b[K     |████████████████████████████████| 27.4MB 106kB/s \n",
      "\u001b[?25hCollecting scikit-learn<0.25,>=0.23.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/eb/a48f25c967526b66d5f1fa7a984594f0bf0a5afafa94a8c4dbc317744620/scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
      "\u001b[K     |████████████████████████████████| 22.3MB 91kB/s \n",
      "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from autogluon.text==0.2.0->autogluon) (3.0.0)\n",
      "Requirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.text==0.2.0->autogluon) (4.41.1)\n",
      "Requirement already satisfied: networkx<3.0,>=2.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.2.0->autogluon) (2.5.1)\n",
      "Collecting psutil<5.9,>=5.7.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/da/f7efdcf012b51506938553dbe302aecc22f3f43abd5cffa8320e8e0588d5/psutil-5.8.0-cp37-cp37m-manylinux2010_x86_64.whl (296kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 53.5MB/s \n",
      "\u001b[?25hCollecting fastai<3.0,>=2.0; extra == \"all\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/79/e8a87e4c20238e114671314426227db8647d2b42744eab79e0917c59865e/fastai-2.3.1-py3-none-any.whl (194kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 56.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch<2.0,>=1.0; extra == \"all\" in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.2.0->autogluon) (1.8.1+cu101)\n",
      "Collecting xgboost<1.4,>=1.3.2; extra == \"all\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/57/bf5026701c384decd2b995eb39d86587a103ba4eb26f8a9b1811db0896d3/xgboost-1.3.3-py3-none-manylinux2010_x86_64.whl (157.5MB)\n",
      "\u001b[K     |████████████████████████████████| 157.5MB 87kB/s \n",
      "\u001b[?25hCollecting lightgbm<4.0,>=3.0; extra == \"all\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/b2/fff8370f48549ce223f929fe8cab4ee6bf285a41f86037d91312b48ed95b/lightgbm-3.2.1-py3-none-manylinux1_x86_64.whl (2.0MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0MB 46.6MB/s \n",
      "\u001b[?25hCollecting catboost<0.26,>=0.24.0; extra == \"all\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/80/8e9c57ec32dfed6ba2922bc5c96462cbf8596ce1a6f5de532ad1e43e53fe/catboost-0.25.1-cp37-none-manylinux1_x86_64.whl (67.3MB)\n",
      "\u001b[K     |████████████████████████████████| 67.3MB 42kB/s \n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.2.0->autogluon) (3.2.2)\n",
      "Collecting ConfigSpace==0.4.18\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/c3/3c21e8d82a639fd821f538e6d7f830b654a6ce1fe52644be1a67f323f707/ConfigSpace-0.4.18.tar.gz (950kB)\n",
      "\u001b[K     |████████████████████████████████| 952kB 39.6MB/s \n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: dask>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.2.0->autogluon) (2.12.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.2.0->autogluon) (2.23.0)\n",
      "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.2.0->autogluon) (1.3)\n",
      "Collecting boto3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/2f/8cfc267a6bc33a50ea899462ca5d85bf583acb0880cb91e1e533a6271bd3/boto3-1.17.71-py2.py3-none-any.whl (131kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 57.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.2.0->autogluon) (0.29.22)\n",
      "Collecting graphviz<0.9.0,>=0.8.1\n",
      "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: dill==0.3.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.2.0->autogluon) (0.3.3)\n",
      "Collecting paramiko>=2.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/19/124e9287b43e6ff3ebb9cdea3e5e8e88475a873c05ccdf8b7e20d2c4201e/paramiko-2.7.2-py2.py3-none-any.whl (206kB)\n",
      "\u001b[K     |████████████████████████████████| 215kB 59.4MB/s \n",
      "\u001b[?25hCollecting distributed>=2.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/f8/ac2c18adde6477bca3881c4d3cfa74c7f4da7ee82f3c83c201aa3b9ca5ee/distributed-2021.4.1-py3-none-any.whl (696kB)\n",
      "\u001b[K     |████████████████████████████████| 706kB 53.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: tornado>=5.0.1 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.2.0->autogluon) (5.1.1)\n",
      "Collecting openml\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/cf/17b76b80820fb1466c20ed1d25595744647db8956a43140fdfa614897489/openml-0.12.1.tar.gz (115kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 54.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: Pillow<=8.1 in /usr/local/lib/python3.7/dist-packages (from autogluon.mxnet==0.2.0->autogluon) (7.1.2)\n",
      "Collecting gluoncv<0.11,>=0.10.1.post0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/2b/f1cc4e7a0a654c00dae9d870d0d1f653e5760ad85297306b63e6be527dcf/gluoncv-0.10.1.post0-py3-none-any.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 37.5MB/s \n",
      "\u001b[?25hCollecting d8<1.0,>=0.0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/5f/31/dbc20607856da42d84ba6a89e9146a100274a1e0ecd1814eb2c791d9d1d5/d8-0.0.2.post0-py3-none-any.whl\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.text==0.2.0->autogluon) (1.10.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.text==0.2.0->autogluon) (1.15.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.text==0.2.0->autogluon) (1.4.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.text==0.2.0->autogluon) (56.1.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.text==0.2.0->autogluon) (8.7.0)\n",
      "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.text==0.2.0->autogluon) (0.7.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.text==0.2.0->autogluon) (20.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0,>=1.0.0->autogluon.text==0.2.0->autogluon) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0,>=1.0.0->autogluon.text==0.2.0->autogluon) (2018.9)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.2.0->autogluon) (2019.12.20)\n",
      "Collecting sacrebleu\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 9.5MB/s \n",
      "\u001b[?25hCollecting sacremoses>=0.0.38\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001b[K     |████████████████████████████████| 901kB 48.1MB/s \n",
      "\u001b[?25hCollecting tokenizers==0.9.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/36/59e4a62254c5fcb43894c6b0e9403ec6f4238cc2422a003ed2e6279a1784/tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 41.6MB/s \n",
      "\u001b[?25hCollecting yacs>=0.1.6\n",
      "  Downloading https://files.pythonhosted.org/packages/38/4f/fe9a4d472aa867878ce3bb7efb16654c5d63672b86dc0e6e953a67018433/yacs-0.1.8-py3-none-any.whl\n",
      "Collecting contextvars\n",
      "  Downloading https://files.pythonhosted.org/packages/83/96/55b82d9f13763be9d672622e1b8106c85acb83edd7cc2fa5bc67cd9877e9/contextvars-2.4.tar.gz\n",
      "Collecting flake8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/80/35a0716e5d5101e643404dabd20f07f5528a21f3ef4032d31a49c913237b/flake8-3.9.2-py2.py3-none-any.whl (73kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 12.0MB/s \n",
      "\u001b[?25hCollecting sentencepiece==0.1.95\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 21.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.2.0->autogluon) (3.12.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25,>=0.23.2->autogluon.text==0.2.0->autogluon) (1.0.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx<3.0,>=2.3->autogluon.tabular[all]==0.2.0->autogluon) (4.4.2)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.0; extra == \"all\"->autogluon.tabular[all]==0.2.0->autogluon) (0.9.1+cu101)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.0; extra == \"all\"->autogluon.tabular[all]==0.2.0->autogluon) (20.9)\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.0; extra == \"all\"->autogluon.tabular[all]==0.2.0->autogluon) (19.3.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.0; extra == \"all\"->autogluon.tabular[all]==0.2.0->autogluon) (3.13)\n",
      "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.0; extra == \"all\"->autogluon.tabular[all]==0.2.0->autogluon) (2.2.4)\n",
      "Collecting fastcore<1.4,>=1.3.8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b0/f1fbf554e0bf3c76e1bdc3b82eedfe41fcf656479586be38c64421082b1b/fastcore-1.3.20-py3-none-any.whl (53kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 9.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.0; extra == \"all\"->autogluon.tabular[all]==0.2.0->autogluon) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2.0,>=1.0; extra == \"all\"->autogluon.tabular[all]==0.2.0->autogluon) (3.7.4.3)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm<4.0,>=3.0; extra == \"all\"->autogluon.tabular[all]==0.2.0->autogluon) (0.36.2)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost<0.26,>=0.24.0; extra == \"all\"->autogluon.tabular[all]==0.2.0->autogluon) (4.4.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->autogluon.core==0.2.0->autogluon) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->autogluon.core==0.2.0->autogluon) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->autogluon.core==0.2.0->autogluon) (0.10.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core==0.2.0->autogluon) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core==0.2.0->autogluon) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core==0.2.0->autogluon) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core==0.2.0->autogluon) (2020.12.5)\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd>=1.3->autogluon.core==0.2.0->autogluon) (0.16.0)\n",
      "Collecting botocore<1.21.0,>=1.20.71\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/7a/bcc46535f978318168fbc5d008c58141280939fea33dae12644b97ced403/botocore-1.20.71-py2.py3-none-any.whl (7.5MB)\n",
      "\u001b[K     |████████████████████████████████| 7.5MB 45.8MB/s \n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.5.0,>=0.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 12.9MB/s \n",
      "\u001b[?25hCollecting pynacl>=1.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/57/2f5e6226a674b2bcb6db531e8b383079b678df5b10cdaa610d6cf20d77ba/PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961kB)\n",
      "\u001b[K     |████████████████████████████████| 962kB 44.4MB/s \n",
      "\u001b[?25hCollecting cryptography>=2.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/26/7af637e6a7e87258b963f1731c5982fb31cd507f0d90d91836e446955d02/cryptography-3.4.7-cp36-abi3-manylinux2014_x86_64.whl (3.2MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 46.1MB/s \n",
      "\u001b[?25hCollecting bcrypt>=3.1.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/70/6d218afbe4c73538053c1016dd631e8f25fffc10cd01f5c272d7acf3c03d/bcrypt-3.2.0-cp36-abi3-manylinux2010_x86_64.whl (63kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 10.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.2.0->autogluon) (7.1.2)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.2.0->autogluon) (1.0.2)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.2.0->autogluon) (1.7.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.2.0->autogluon) (0.11.1)\n",
      "Collecting cloudpickle>=1.5.0\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/e3/898487e5dbeb612054cf2e0c188463acb358167fef749c53c8bb8918cea1/cloudpickle-1.6.0-py3-none-any.whl\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.2.0->autogluon) (2.3.0)\n",
      "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.2.0->autogluon) (2.0.0)\n",
      "Collecting liac-arff>=2.4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/6e/43/73944aa5ad2b3185c0f0ba0ee6f73277f2eb51782ca6ccf3e6793caf209a/liac-arff-2.5.0.tar.gz\n",
      "Collecting xmltodict\n",
      "  Downloading https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\n",
      "Collecting minio\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/c9/09761948bb0c73e08e803dfc616e2f3829a790349ba18dcffdf6a4608f2d/minio-7.0.3-py3-none-any.whl (75kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 12.3MB/s \n",
      "\u001b[?25hCollecting portalocker\n",
      "  Downloading https://files.pythonhosted.org/packages/68/33/cb524f4de298509927b90aa5ee34767b9a2b93e663cf354b2a3efa2b4acd/portalocker-2.3.0-py2.py3-none-any.whl\n",
      "Collecting tensorboardx\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 59.6MB/s \n",
      "\u001b[?25hCollecting decord\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/5e/e2be6a3a3a46275059574d9c6a1d422aa6c7c3cbf6614939b8a3c3f8f2d5/decord-0.5.2-py3-none-manylinux2010_x86_64.whl (14.1MB)\n",
      "\u001b[K     |████████████████████████████████| 14.1MB 223kB/s \n",
      "\u001b[?25hCollecting autocfg\n",
      "  Downloading https://files.pythonhosted.org/packages/95/f9/74e0a42cbc6d871c92288806e7812c7d2628c2a06557930dbab0a17438d2/autocfg-0.0.8-py3-none-any.whl\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gluoncv<0.11,>=0.10.1.post0->autogluon.extra==0.2.0->autogluon) (4.1.2.30)\n",
      "Collecting xxhash\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 52.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from d8<1.0,>=0.0.2->autogluon.vision==0.2.0->autogluon) (1.5.12)\n",
      "Collecting immutables>=0.9\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/46/d2feb89eb17c981208037cfeeadb854ce066a5a23d0aba616afd9dfa0c05/immutables-0.15-cp37-cp37m-manylinux1_x86_64.whl (101kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 14.4MB/s \n",
      "\u001b[?25hCollecting pycodestyle<2.8.0,>=2.7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/cc/227251b1471f129bc35e966bb0fceb005969023926d744139642d847b7ae/pycodestyle-2.7.0-py2.py3-none-any.whl (41kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 8.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from flake8->autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.2.0->autogluon) (3.10.1)\n",
      "Collecting mccabe<0.7.0,>=0.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
      "Collecting pyflakes<2.4.0,>=2.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/11/2a745612f1d3cbbd9c69ba14b1b43a35a2f5c3c81cd0124508c52c64307f/pyflakes-2.3.1-py2.py3-none-any.whl (68kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 11.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.0; extra == \"all\"->autogluon.tabular[all]==0.2.0->autogluon) (2.0.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.0; extra == \"all\"->autogluon.tabular[all]==0.2.0->autogluon) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.0; extra == \"all\"->autogluon.tabular[all]==0.2.0->autogluon) (0.8.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.0; extra == \"all\"->autogluon.tabular[all]==0.2.0->autogluon) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.0; extra == \"all\"->autogluon.tabular[all]==0.2.0->autogluon) (1.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.0; extra == \"all\"->autogluon.tabular[all]==0.2.0->autogluon) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.0; extra == \"all\"->autogluon.tabular[all]==0.2.0->autogluon) (3.0.5)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.0; extra == \"all\"->autogluon.tabular[all]==0.2.0->autogluon) (7.4.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.0; extra == \"all\"->autogluon.tabular[all]==0.2.0->autogluon) (0.4.1)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost<0.26,>=0.24.0; extra == \"all\"->autogluon.tabular[all]==0.2.0->autogluon) (1.3.3)\n",
      "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pynacl>=1.0.1->paramiko>=2.4->autogluon.core==0.2.0->autogluon) (1.14.5)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.6.0->autogluon.core==0.2.0->autogluon) (1.0.1)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->d8<1.0,>=0.0.2->autogluon.vision==0.2.0->autogluon) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->flake8->autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.2.0->autogluon) (3.4.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.4.1->pynacl>=1.0.1->paramiko>=2.4->autogluon.core==0.2.0->autogluon) (2.20)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->d8<1.0,>=0.0.2->autogluon.vision==0.2.0->autogluon) (1.3)\n",
      "Building wheels for collected packages: ConfigSpace\n",
      "  Building wheel for ConfigSpace (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ConfigSpace: filename=ConfigSpace-0.4.18-cp37-cp37m-linux_x86_64.whl size=2879815 sha256=5c20681cf09c2a145b991a83ecdf81bf13c16acc1683aa9771f7be043213d36f\n",
      "  Stored in directory: /root/.cache/pip/wheels/39/ea/40/d93931850f700427db0a84180829c709d30484c9475040c7bd\n",
      "Successfully built ConfigSpace\n",
      "Building wheels for collected packages: openml, contextvars, liac-arff\n",
      "  Building wheel for openml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for openml: filename=openml-0.12.1-cp37-none-any.whl size=132141 sha256=faf7923f20b6961129db10793c26b485bba6640c93399c076ad83c05f2805f84\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/1d/f9/a92b9c9d0f73772df4b9c2cdd367b706e595f67816dbefd032\n",
      "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for contextvars: filename=contextvars-2.4-cp37-none-any.whl size=7667 sha256=3c2ca9ed9ea9e6f7e6a977461db8e67a0ae775f6c550a75ab4bc06a3d4a44bb1\n",
      "  Stored in directory: /root/.cache/pip/wheels/a5/7d/68/1ebae2668bda2228686e3c1cf16f2c2384cea6e9334ad5f6de\n",
      "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for liac-arff: filename=liac_arff-2.5.0-cp37-none-any.whl size=11732 sha256=b9421b521b9584adc605267d5de5a5d0c24656b71786dedbe8a6a0f8ef495072\n",
      "  Stored in directory: /root/.cache/pip/wheels/77/8d/b4/8bfce5beea9a3496cc15b24961876adb7b6e2912ff09164179\n",
      "Successfully built openml contextvars liac-arff\n",
      "\u001b[31mERROR: distributed 2021.4.1 has requirement dask>=2021.03.0, but you'll have dask 2.12.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: sacrebleu 1.5.1 has requirement portalocker==2.0.0, but you'll have portalocker 2.3.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: botocore 1.20.71 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: portalocker, sacrebleu, sacremoses, tokenizers, yacs, immutables, contextvars, pycodestyle, mccabe, pyflakes, flake8, sentencepiece, autogluon-contrib-nlp, scipy, ConfigSpace, jmespath, botocore, s3transfer, boto3, graphviz, threadpoolctl, scikit-learn, pynacl, cryptography, bcrypt, paramiko, psutil, cloudpickle, distributed, autogluon.core, liac-arff, xmltodict, minio, openml, autogluon.mxnet, autogluon.text, autogluon.features, fastcore, fastai, xgboost, lightgbm, catboost, autogluon.tabular, tensorboardx, decord, autocfg, gluoncv, autogluon.extra, xxhash, d8, autogluon.vision, autogluon\n",
      "  Found existing installation: scipy 1.4.1\n",
      "    Uninstalling scipy-1.4.1:\n",
      "      Successfully uninstalled scipy-1.4.1\n",
      "  Found existing installation: graphviz 0.10.1\n",
      "    Uninstalling graphviz-0.10.1:\n",
      "      Successfully uninstalled graphviz-0.10.1\n",
      "  Found existing installation: scikit-learn 0.22.2.post1\n",
      "    Uninstalling scikit-learn-0.22.2.post1:\n",
      "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
      "  Found existing installation: psutil 5.4.8\n",
      "    Uninstalling psutil-5.4.8:\n",
      "      Successfully uninstalled psutil-5.4.8\n",
      "  Found existing installation: cloudpickle 1.3.0\n",
      "    Uninstalling cloudpickle-1.3.0:\n",
      "      Successfully uninstalled cloudpickle-1.3.0\n",
      "  Found existing installation: distributed 1.25.3\n",
      "    Uninstalling distributed-1.25.3:\n",
      "      Successfully uninstalled distributed-1.25.3\n",
      "  Found existing installation: fastai 1.0.61\n",
      "    Uninstalling fastai-1.0.61:\n",
      "      Successfully uninstalled fastai-1.0.61\n",
      "  Found existing installation: xgboost 0.90\n",
      "    Uninstalling xgboost-0.90:\n",
      "      Successfully uninstalled xgboost-0.90\n",
      "  Found existing installation: lightgbm 2.2.3\n",
      "    Uninstalling lightgbm-2.2.3:\n",
      "      Successfully uninstalled lightgbm-2.2.3\n",
      "Successfully installed ConfigSpace-0.4.18 autocfg-0.0.8 autogluon-0.2.0 autogluon-contrib-nlp-0.0.1b20210201 autogluon.core-0.2.0 autogluon.extra-0.2.0 autogluon.features-0.2.0 autogluon.mxnet-0.2.0 autogluon.tabular-0.2.0 autogluon.text-0.2.0 autogluon.vision-0.2.0 bcrypt-3.2.0 boto3-1.17.71 botocore-1.20.71 catboost-0.25.1 cloudpickle-1.6.0 contextvars-2.4 cryptography-3.4.7 d8-0.0.2.post0 decord-0.5.2 distributed-2021.4.1 fastai-2.3.1 fastcore-1.3.20 flake8-3.9.2 gluoncv-0.10.1.post0 graphviz-0.8.4 immutables-0.15 jmespath-0.10.0 liac-arff-2.5.0 lightgbm-3.2.1 mccabe-0.6.1 minio-7.0.3 openml-0.12.1 paramiko-2.7.2 portalocker-2.3.0 psutil-5.8.0 pycodestyle-2.7.0 pyflakes-2.3.1 pynacl-1.4.0 s3transfer-0.4.2 sacrebleu-1.5.1 sacremoses-0.0.45 scikit-learn-0.24.2 scipy-1.6.3 sentencepiece-0.1.95 tensorboardx-2.2 threadpoolctl-2.1.0 tokenizers-0.9.4 xgboost-1.3.3 xmltodict-0.12.0 xxhash-2.0.2 yacs-0.1.8\n"
     ]
    }
   ],
   "source": [
    "# !sudo pip3 install setuptools wheel\n",
    "# !sudo pip3 install -U \"mxnet<2.0.0\"\n",
    "!sudo pip3 install autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3219,
     "status": "ok",
     "timestamp": 1620777046501,
     "user": {
      "displayName": "Gabriel Fernando",
      "photoUrl": "",
      "userId": "06273348089492808596"
     },
     "user_tz": -480
    },
    "id": "vkgzLX_m_V17",
    "outputId": "ea3ee486-3e3e-4c4f-e03d-1748412893dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name: autogluon\n",
      "Version: 0.2.0\n",
      "Summary: AutoML for Text, Image, and Tabular Data\n",
      "Home-page: https://github.com/awslabs/autogluon\n",
      "Author: AutoGluon Community\n",
      "Author-email: None\n",
      "License: Apache-2.0\n",
      "Location: /usr/local/lib/python3.7/dist-packages\n",
      "Requires: autogluon.text, autogluon.mxnet, autogluon.features, autogluon.core, autogluon.extra, autogluon.tabular, autogluon.vision\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip3 show autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JBccBprGJo5e"
   },
   "outputs": [],
   "source": [
    "!python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITJFVICO_lpG"
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 810,
     "status": "ok",
     "timestamp": 1620699426129,
     "user": {
      "displayName": "Gabriel Fernando",
      "photoUrl": "",
      "userId": "06273348089492808596"
     },
     "user_tz": -480
    },
    "id": "XfeP2AXWIugs",
    "outputId": "7e7026dd-9f0c-4129-87d3-471c3508cd57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: shell: command not found\n"
     ]
    }
   ],
   "source": [
    "# !pip3 show dask\n",
    "# from dask.utils import stringify\n",
    "!bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7OzkflJJ_aix"
   },
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "train_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\n",
    "subsample_size = 500  # subsample subset of data for faster demo, try setting this to much larger values\n",
    "train_data = train_data.sample(n=subsample_size, random_state=0)\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYm4uBtL_tNk"
   },
   "outputs": [],
   "source": [
    "label = 'class'\n",
    "print(\"Summary of class variable: \\n\", train_data[label].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Foj5cpXi_vZX"
   },
   "outputs": [],
   "source": [
    "# save_path = 'agModels-predictClass'  # specifies folder to store trained models\n",
    "predictor = TabularPredictor(label=label).fit(train_data)  # , path=save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mmqR1vHJ_9XD"
   },
   "outputs": [],
   "source": [
    "test_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')\n",
    "y_test = test_data[label]  # values to predict\n",
    "test_data_nolab = test_data.drop(columns=[label])  # delete label column to prove we're not cheating\n",
    "test_data_nolab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-YfZsCkAMTp"
   },
   "outputs": [],
   "source": [
    "results = predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLsfrDlmASP_"
   },
   "outputs": [],
   "source": [
    "print(\"AutoGluon infers problem type is: \", predictor.problem_type)\n",
    "print(\"AutoGluon identified the following types of features:\")\n",
    "print(predictor.feature_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbMySkcZAWbG"
   },
   "outputs": [],
   "source": [
    "predictor.predict(test_data, model='LightGBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p241ybxRABqb"
   },
   "outputs": [],
   "source": [
    "predictor.leaderboard(test_data, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bA9PnbbeAIb4"
   },
   "outputs": [],
   "source": [
    "pred_probs = predictor.predict_proba(test_data_nolab)\n",
    "pred_probs.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7V_a5yAvPKOi"
   },
   "source": [
    "### CLF_of_ECG\n",
    "\n",
    "\n",
    "使用AutoML方法得到的结果评分为328。位次还可以，应该将这个数值看作AutoML的baseline，不必要在机器学习上花太多时间，毕竟当数据量达到一定程度，深度学习模型基本可以碾压机器学习模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3103278,
     "status": "ok",
     "timestamp": 1620705220420,
     "user": {
      "displayName": "Gabriel Fernando",
      "photoUrl": "",
      "userId": "06273348089492808596"
     },
     "user_tz": -480
    },
    "id": "tD9nTc6eQWj1",
    "outputId": "c69ae796-e6f2-4c3d-94cf-becff647b0be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: /content/drive/MyDrive/CLF_of_ECG_signals/Data/X_train.csv | Columns = 206 / 206 | Rows = 75000 -> 75000\n",
      "Loaded data from: /content/drive/MyDrive/CLF_of_ECG_signals/Data/y_train.csv | Columns = 2 / 2 | Rows = 75000 -> 75000\n",
      "Loaded data from: /content/drive/MyDrive/CLF_of_ECG_signals/Data/features_labels.csv | Columns = 208 / 208 | Rows = 100000 -> 100000\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20210511_030204/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20210511_030204/\"\n",
      "AutoGluon Version:  0.2.0\n",
      "Train Data Rows:    100000\n",
      "Train Data Columns: 207\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == float, but few unique label-values observed and label-values can be converted to int).\n",
      "\t4 unique label values:  [0.0, 2.0, 3.0, 1.0]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Train Data Class Count: 4\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    11780.44 MB\n",
      "\tTrain Data (Original)  Memory Usage: 165.6 MB (1.4% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['id', 's_0', 's_1', 's_2', 's_3', ...]\n",
      "\t\t('int', [])   :   1 | ['Unnamed: 0']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['id', 's_0', 's_1', 's_2', 's_3', ...]\n",
      "\t\t('int', [])   :   1 | ['Unnamed: 0']\n",
      "\t1.7s = Fit runtime\n",
      "\t207 features in original data used to generate 207 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 165.6 MB (1.4% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.04s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "Automatically generating train/validation split with holdout_frac=0.025, Train Rows: 97500, Val Rows: 2500\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.6072\t = Validation accuracy score\n",
      "\t0.19s\t = Training runtime\n",
      "\t10.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.5912\t = Validation accuracy score\n",
      "\t0.14s\t = Training runtime\n",
      "\t9.51s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.9924\t = Validation accuracy score\n",
      "\t104.11s\t = Training runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's multi_error: 0\tvalid_set's multi_error: 0.008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9924\t = Validation accuracy score\n",
      "\t269.3s\t = Training runtime\n",
      "\t0.56s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.9832\t = Validation accuracy score\n",
      "\t81.61s\t = Training runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.9836\t = Validation accuracy score\n",
      "\t227.73s\t = Training runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.9852\t = Validation accuracy score\n",
      "\t244.89s\t = Training runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.9628\t = Validation accuracy score\n",
      "\t106.97s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.9844\t = Validation accuracy score\n",
      "\t46.43s\t = Training runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.9848\t = Validation accuracy score\n",
      "\t41.34s\t = Training runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.98\t = Validation accuracy score\n",
      "\t1028.35s\t = Training runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: NeuralNetMXNet ...\n",
      "\t0.9932\t = Validation accuracy score\n",
      "\t779.5s\t = Training runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.9856\t = Validation accuracy score\n",
      "\t98.24s\t = Training runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.9948\t = Validation accuracy score\n",
      "\t0.71s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 3089.44s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20210511_030204/\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                  model  score_val  pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0   WeightedEnsemble_L2     0.9948       1.134994  1049.509962                0.003901           0.711680            2       True         14\n",
      "1        NeuralNetMXNet     0.9932       0.573963   779.497032                0.573963         779.497032            1       True         12\n",
      "2       NeuralNetFastAI     0.9924       0.110950   104.107766                0.110950         104.107766            1       True          3\n",
      "3            LightGBMXT     0.9924       0.557130   269.301250                0.557130         269.301250            1       True          4\n",
      "4         LightGBMLarge     0.9856       0.106029    98.238721                0.106029          98.238721            1       True         13\n",
      "5      RandomForestEntr     0.9852       0.206998   244.889860                0.206998         244.889860            1       True          7\n",
      "6        ExtraTreesEntr     0.9848       0.207398    41.340098                0.207398          41.340098            1       True         10\n",
      "7        ExtraTreesGini     0.9844       0.206226    46.431425                0.206226          46.431425            1       True          9\n",
      "8      RandomForestGini     0.9836       0.206634   227.733884                0.206634         227.733884            1       True          6\n",
      "9              LightGBM     0.9832       0.131136    81.605152                0.131136          81.605152            1       True          5\n",
      "10              XGBoost     0.9800       0.155610  1028.349480                0.155610        1028.349480            1       True         11\n",
      "11             CatBoost     0.9628       0.011319   106.974802                0.011319         106.974802            1       True          8\n",
      "12       KNeighborsUnif     0.6072      10.027151     0.194257               10.027151           0.194257            1       True          1\n",
      "13       KNeighborsDist     0.5912       9.513411     0.138659                9.513411           0.138659            1       True          2\n",
      "Number of models trained: 14\n",
      "Types of models trained:\n",
      "{'LGBModel', 'RFModel', 'KNNModel', 'CatBoostModel', 'XGBoostModel', 'WeightedEnsembleModel', 'TabularNeuralNetModel', 'XTModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', []) : 206 | ['id', 's_0', 's_1', 's_2', 's_3', ...]\n",
      "('int', [])   :   1 | ['Unnamed: 0']\n",
      "Plot summary of models saved to file: AutogluonModels/ag-20210511_030204/SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    " \n",
    "data_train_path = '/content/drive/MyDrive/CLF_of_ECG_signals/Data/features_labels.csv'\n",
    " \n",
    "model_save_path = '/content/drive/Shareddrives/xucc1993.HK(CRN.NGO)/CLF_of_ECG_signals/AutoML-AutoDL/agModels-predictClass'\n",
    " \n",
    "data_train = TabularDataset(data_train_path)\n",
    " \n",
    "label = 'label'\n",
    " \n",
    "predictor = TabularPredictor(label=label, path=model_save_path).fit(data_train)\n",
    " \n",
    "results = predictor.fit_summary()\n",
    " \n",
    "# print(predictor.feature_metadata)\n",
    " \n",
    "# predictor.predict(X_val, model='LightGBM')\n",
    " \n",
    "# predictor.leaderboard(test_data, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14073,
     "status": "ok",
     "timestamp": 1620804252541,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "fIQ3v-sViHIU",
    "outputId": "0c76ad7a-28e0-465c-872d-68646edb05b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/fsspec/__init__.py:47: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for spec in entry_points.get(\"fsspec.specs\", []):\n",
      "/usr/local/lib/python3.7/dist-packages/_pytest/mark/structures.py:426: DeprecationWarning: The usage of `cmp` is deprecated and will be removed on or after 2021-06-01.  Please use `eq` and `order` instead.\n",
      "  @attr.s(cmp=False, hash=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  \n",
      " 0        0.0\n",
      "1        2.0\n",
      "2        3.0\n",
      "3        0.0\n",
      "4        0.0\n",
      "        ... \n",
      "19995    0.0\n",
      "19996    0.0\n",
      "19997    2.0\n",
      "19998    0.0\n",
      "19999    0.0\n",
      "Name: label, Length: 20000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 加载已有模型\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "model_save_path = '/content/drive/Shareddrives/xucc1993.HK(CRN.NGO)/CLF_of_ECG_signals/AutoML-AutoDL/agModels-predictClass'\n",
    "\n",
    "predictor = TabularPredictor.load(model_save_path)  # unnecessary, just demonstrates how to load previously-trained predictor from file\n",
    "\n",
    "data_test_path = '/content/drive/MyDrive/CLF_of_ECG_signals/Data/features_X_test.csv'\n",
    "# id = 'id'\n",
    "data_test = TabularDataset(data_test_path)\n",
    "\n",
    "y_pred = predictor.predict(data_test)\n",
    "print(\"Predictions:  \\n\", y_pred)\n",
    "# perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 994,
     "status": "ok",
     "timestamp": 1620804578994,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "2AGXSt3Wd9bp",
    "outputId": "8992abc0-3006-4bee-9186-3b68bf0ae58f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# type(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2298,
     "status": "ok",
     "timestamp": 1620804605322,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "iKwbbf8uPfN-",
    "outputId": "1d12d453-06be-4c31-b930-f92c5c7b869f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "           id  label_0  label_1  label_2  label_3\n",
      "0      100000      1.0      0.0      0.0      0.0\n",
      "1      100001      0.0      0.0      1.0      0.0\n",
      "2      100002      0.0      0.0      0.0      1.0\n",
      "3      100003      1.0      0.0      0.0      0.0\n",
      "4      100004      1.0      0.0      0.0      0.0\n",
      "...       ...      ...      ...      ...      ...\n",
      "19995  119995      1.0      0.0      0.0      0.0\n",
      "19996  119996      1.0      0.0      0.0      0.0\n",
      "19997  119997      0.0      0.0      1.0      0.0\n",
      "19998  119998      1.0      0.0      0.0      0.0\n",
      "19999  119999      1.0      0.0      0.0      0.0\n",
      "\n",
      "[20000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "y_preds_subm = y_pred.values\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_preds_subm = onehot_encoder.fit_transform(y_preds_subm.reshape(y_preds_subm.shape[0],1))\n",
    "print(y_preds_subm)\n",
    "\n",
    "result=pd.read_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Data/sample_submit.csv')\n",
    "result['label_0']=y_preds_subm[:,0]\n",
    "result['label_1']=y_preds_subm[:,1]\n",
    "result['label_2']=y_preds_subm[:,2]\n",
    "result['label_3']=y_preds_subm[:,3]\n",
    "print(result)\n",
    "result.to_csv('/content/drive/MyDrive/CLF_of_ECG_signals/Result_Submission/submit.csv',index=False)\n",
    "\n",
    "# 最终提交得分为328，非常不错了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3117,
     "status": "ok",
     "timestamp": 1620804145533,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "G7dFjgRShkO9",
    "outputId": "ff879c12-1d45-44c3-d872-c42626520ea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: autogluon\n",
      "Version: 0.2.0\n",
      "Summary: AutoML for Text, Image, and Tabular Data\n",
      "Home-page: https://github.com/awslabs/autogluon\n",
      "Author: AutoGluon Community\n",
      "Author-email: None\n",
      "License: Apache-2.0\n",
      "Location: /usr/local/lib/python3.7/dist-packages\n",
      "Requires: autogluon.core, autogluon.vision, autogluon.features, autogluon.text, autogluon.extra, autogluon.tabular, autogluon.mxnet\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# !sudo pip3 show autogluon\n",
    "# !sudo pip3 install autogluon\n",
    "# !sudo pip3 install dask --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3ASDT8TyDHW"
   },
   "source": [
    "## AutoKeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEW821HayII1"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6078,
     "status": "ok",
     "timestamp": 1620814222286,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "kseSgIktyHdW",
    "outputId": "de92ffab-0af2-4d3c-e47a-9fd8c0cf1d24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autokeras\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/12/cf698586ccc8245f08d1843dcafb65b064a2e9e2923b889dc58e1019f099/autokeras-1.0.12-py3-none-any.whl (164kB)\n",
      "\r",
      "\u001b[K     |██                              | 10kB 18.1MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 20kB 21.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 30kB 17.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 40kB 15.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 51kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 61kB 10.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 71kB 9.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 81kB 10.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 92kB 10.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 102kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 112kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 122kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 133kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 143kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 153kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 163kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 174kB 7.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: tensorflow>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from autokeras) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from autokeras) (0.24.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from autokeras) (20.9)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from autokeras) (1.1.5)\n",
      "Collecting keras-tuner>=1.0.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/ec/1ef246787174b1e2bb591c95f29d3c1310070cad877824f907faba3dade9/keras-tuner-1.0.2.tar.gz (62kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 9.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (3.3.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (0.36.2)\n",
      "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (0.12.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (1.12)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (0.2.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (1.19.5)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (1.6.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (1.1.0)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (2.10.0)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (2.4.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (3.12.4)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (0.3.3)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (1.12.1)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (1.1.2)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (1.32.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (3.7.4.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3.0->autokeras) (2.4.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->autokeras) (1.6.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->autokeras) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->autokeras) (2.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->autokeras) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->autokeras) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->autokeras) (2.8.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (0.16.0)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (0.8.9)\n",
      "Collecting terminaltables\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
      "Collecting colorama\n",
      "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (2.23.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (0.4.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (3.3.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (56.1.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (1.28.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.2->autokeras) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.2->autokeras) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.2->autokeras) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.2->autokeras) (3.0.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (3.10.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (0.2.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (3.4.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3.0->autokeras) (0.4.8)\n",
      "Building wheels for collected packages: keras-tuner, terminaltables\n",
      "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-tuner: filename=keras_tuner-1.0.2-cp37-none-any.whl size=78938 sha256=5f98e61043ff6be3d2ec6399fa315629021e1ba8f699766f9c49e0c436fa0ecd\n",
      "  Stored in directory: /root/.cache/pip/wheels/bb/a1/8a/7c3de0efb3707a1701b36ebbfdbc4e67aedf6d4943a1f463d6\n",
      "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp37-none-any.whl size=15356 sha256=7151631a93875c67fea26e91e38abb19cd545aaf86f781033658f174a07ef2fc\n",
      "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
      "Successfully built keras-tuner terminaltables\n",
      "Installing collected packages: terminaltables, colorama, keras-tuner, autokeras\n",
      "Successfully installed autokeras-1.0.12 colorama-0.4.4 keras-tuner-1.0.2 terminaltables-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!sudo pip3 install autokeras\n",
    "# !pip3 show autokeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3669,
     "status": "ok",
     "timestamp": 1620814890230,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "EbHgT7r5GFEo",
    "outputId": "f2dc2937-957c-4c3e-8dcf-58be6328c0ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emcee\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/f4/00151f5f843088337c6a53edd6cbb2df340f1044d23080c662f95219cc3f/emcee-3.0.2-py2.py3-none-any.whl (41kB)\n",
      "\r",
      "\u001b[K     |███████▉                        | 10kB 16.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 20kB 24.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 30kB 28.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▎| 40kB 19.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 51kB 4.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from emcee) (1.19.5)\n",
      "Installing collected packages: emcee\n",
      "Successfully installed emcee-3.0.2\n"
     ]
    }
   ],
   "source": [
    "!sudo pip3 install emcee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gonrIsnyWk-"
   },
   "source": [
    "### CLF_of_ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 893,
     "status": "ok",
     "timestamp": 1620815222678,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "JwD_6ivAyWCQ",
    "outputId": "db3187ae-17c8-4a4c-95a2-73f26e9052e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> (75000, 205)\n",
      "<class 'pandas.core.frame.DataFrame'> (25000, 205)\n",
      "<class 'pandas.core.series.Series'> (75000,)\n",
      "<class 'pandas.core.series.Series'> (25000,)\n"
     ]
    }
   ],
   "source": [
    "# 划分X_train, X_val, y_train, y_val\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels)\n",
    " \n",
    "# 导入模型前预处理\n",
    "# X_train = X_train.values.reshape(X_train.shape[0],X_train.shape[1],1)\n",
    "# X_val = X_val.values.reshape(X_val.shape[0],X_val.shape[1],1)\n",
    "# y_train = y_train.values.reshape(y_train.shape[0],1)\n",
    "# y_val = y_val.values.reshape(y_val.shape[0],1)\n",
    " \n",
    "print(type(X_train),X_train.shape)\n",
    "print(type(X_val),X_val.shape)\n",
    "print(type(y_train),y_train.shape)\n",
    "print(type(y_val),y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TMrt675EIv7"
   },
   "outputs": [],
   "source": [
    "\"\"\"GPU设置为按需增长\"\"\"\n",
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    " \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #有多个GPU时可以指定只使用第几号GPU\n",
    "config = tf.ConfigProto()\n",
    "config.allow_soft_placement=True #允许动态放置张量和操作符\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4 #最多使用40%GPU内存\n",
    "config.gpu_options.allow_growth=True   #初始化时不全部占满GPU显存, 按需分配 \n",
    "sess = tf.Session(config = config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHHjnbn1Kcp4"
   },
   "outputs": [],
   "source": [
    "data_features_labels_path = '/content/drive/MyDrive/CLF_of_ECG_signals/Data/features_labels.csv'\n",
    "data_train_features_labels_path = '/content/drive/MyDrive/CLF_of_ECG_signals/Data/train_features_labels.csv'\n",
    "data_val_features_labels_path = '/content/drive/MyDrive/CLF_of_ECG_signals/Data/val_features_labels.csv'\n",
    "\n",
    "import pandas as pd\n",
    "data_features_labels = pd.read_csv(data_features_labels_path)\n",
    "shape_0 = data_features_labels.shape[0]\n",
    "shape_1 = data_features_labels.shape[1]\n",
    "data_train_features_labels = data_features_labels.iloc[:int(0.75*shape_0),:]\n",
    "data_train_features_labels.to_csv(data_train_features_labels_path)\n",
    "\n",
    "data_val_features_labels = data_features_labels.iloc[int(0.75*shape_0):,:]\n",
    "data_val_features_labels.to_csv(data_val_features_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 812,
     "status": "ok",
     "timestamp": 1620834139734,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "tSwtBqNXwUiN",
    "outputId": "8bea90ef-f995-4a7e-f205-fdf8202ee576"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/Shareddrives/xucc1993.HK(CRN.NGO)/CLF_of_ECG_signals/AutoML-AutoDL/akModels-predictClass\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/Shareddrives/xucc1993.HK(CRN.NGO)/CLF_of_ECG_signals/AutoML-AutoDL/akModels-predictClass\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 740062,
     "status": "ok",
     "timestamp": 1620834901205,
     "user": {
      "displayName": "Chuanchao Xu",
      "photoUrl": "",
      "userId": "08016207472332924278"
     },
     "user_tz": -480
    },
    "id": "QswHIVPvECvJ",
    "outputId": "df5e04bb-0e9b-4114-a376-4998b51a9ed5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 03m 28s]\n",
      "val_accuracy: 1.0\n",
      "\n",
      "Best val_accuracy So Far: 1.0\n",
      "Total elapsed time: 00h 06m 54s\n",
      "Epoch 1/5\n",
      "[WARNING] [2021-05-12 15:52:16,666:tensorflow] AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fdf9e96e680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fdf9e96e680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "2344/2344 [==============================] - 24s 10ms/step - loss: 0.0000e+00 - accuracy: 0.3240\n",
      "Epoch 2/5\n",
      "2344/2344 [==============================] - 23s 10ms/step - loss: 0.0000e+00 - accuracy: 0.2537\n",
      "Epoch 3/5\n",
      "2344/2344 [==============================] - 23s 10ms/step - loss: 0.0000e+00 - accuracy: 0.2501\n",
      "Epoch 4/5\n",
      "2344/2344 [==============================] - 23s 10ms/step - loss: 0.0000e+00 - accuracy: 0.2470\n",
      "Epoch 5/5\n",
      "2344/2344 [==============================] - 23s 10ms/step - loss: 0.0000e+00 - accuracy: 0.2509\n",
      "[WARNING] [2021-05-12 15:54:16,459:tensorflow] AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fdf9ee4a710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fdf9ee4a710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,565:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9ee4a710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9ee4a710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,586:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9dfe0f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9dfe0f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,605:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e502950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e502950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,624:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9ced69e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9ced69e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,638:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e52cb90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e52cb90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,659:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa1d8c680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa1d8c680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,673:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9dfe0c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9dfe0c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,695:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9ced6e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9ced6e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,709:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9dfe0950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9dfe0950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,729:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9cee2dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9cee2dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,743:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9cee2ef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9cee2ef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,764:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9ced6170> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9ced6170> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,780:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9ced6560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9ced6560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,800:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9cee28c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9cee28c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,820:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c51add0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c51add0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,846:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9cee2200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9cee2200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,859:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9cee2830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9cee2830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,879:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9ced6200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9ced6200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,892:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdfa1d8cb00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdfa1d8cb00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,919:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9dfe0d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9dfe0d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,933:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdfa1d8cb00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdfa1d8cb00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,951:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e45ce60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e45ce60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,965:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e45ccb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e45ccb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:16,986:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9cee2b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9cee2b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,001:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9ced63b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9ced63b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,028:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e45ce60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e45ce60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,042:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e52c290> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e52c290> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,074:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c51a320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c51a320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,090:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e52c290> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e52c290> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,109:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e45c440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e45c440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,130:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c5a48c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c5a48c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,150:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e45c680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e45c680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,165:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9ee4a710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9ee4a710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,187:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c5a4b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c5a4b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,209:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9ced63b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9ced63b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,229:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e45cf80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e45cf80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,246:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c55ab00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c55ab00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,268:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c5a44d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c5a44d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,284:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c51a9e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c51a9e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,305:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c55a950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c55a950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,320:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9ced63b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9ced63b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,345:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c5a4dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c5a4dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,359:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c5a44d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c5a44d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,379:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c55a710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c55a710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,393:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9d5fcd40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9d5fcd40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,414:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c55ad40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c55ad40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,428:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e45c320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e45c320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,453:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c5a48c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c5a48c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,468:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c5a4680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c5a4680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,489:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9fc00b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9fc00b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,507:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9fc00a70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9fc00a70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,529:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9d5fcb00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9d5fcb00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,548:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9d5fc5f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9d5fc5f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,569:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9fc00320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9fc00320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,584:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdfa1d8ca70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdfa1d8ca70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,606:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c55a680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c55a680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,629:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9ced63b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9ced63b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,650:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9fc00320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9fc00320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,666:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdfa1b75dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdfa1b75dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,693:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9fc00b00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9fc00b00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,707:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c55a680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c55a680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,729:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa1b75320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa1b75320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,744:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9ced63b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9ced63b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,765:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9fc00200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9fc00200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,779:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e738b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e738b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,801:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9fc00e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9fc00e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,815:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdfa1b758c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdfa1b758c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,835:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e738c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e738c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,850:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9d5fccb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9d5fccb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,869:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9fc00440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9fc00440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,883:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9cee2c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9cee2c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,904:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa1905c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa1905c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,917:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9cee2c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9cee2c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,938:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa1b75e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa1b75e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,959:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9fc00200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9fc00200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:17,984:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa1905b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa1905b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:18,001:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9fc00a70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9fc00a70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:18,021:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e738710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e738710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:18,042:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9d7ae680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9d7ae680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:18,063:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e738710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e738710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:18,077:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdfa19050e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdfa19050e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:18,098:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9d7aecb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9d7aecb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:18,114:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9fc00320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9fc00320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:18,134:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa1905f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa1905f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:18,151:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c55a710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c55a710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:18,171:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e7389e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e7389e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:18,186:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e7389e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e7389e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:18,206:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa19ecb90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa19ecb90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:18,221:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e738710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e738710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:18,242:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c55acb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c55acb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:18,256:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c55acb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c55acb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:18,276:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c4279e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c4279e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:18,292:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c4277a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c4277a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:18,312:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9d7aeef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9d7aeef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:18,328:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9d7ae5f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9d7ae5f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:18,351:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c427d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c427d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:18,372:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9d5fc9e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9d5fc9e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:18,393:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa19ece60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa19ece60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,332:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9de54560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9de54560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,356:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e954440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e954440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,372:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9de54710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9de54710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,393:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e9548c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e9548c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,407:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e954200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e954200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,434:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e9618c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e9618c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,450:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9de54dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9de54dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,471:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e9610e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e9610e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,487:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e961050> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e961050> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,507:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e955680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e955680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,522:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e954830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e954830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,544:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e961ef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e961ef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,559:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e961200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e961200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,579:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e9554d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e9554d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,596:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e955c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e955c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,617:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c6d4dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c6d4dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,633:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e9543b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e9543b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,657:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e961f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e961f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,672:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e954830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e954830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,692:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c6c18c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c6c18c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,710:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e9545f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e9545f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,732:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c6c1dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c6c1dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,752:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e954f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e954f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,773:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c6d4680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c6d4680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,788:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c6d4320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c6d4320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,810:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c6c1ef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c6c1ef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,832:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c6c1b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c6c1b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,855:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa0692f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa0692f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,871:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e961f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e961f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,895:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c6d4a70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9c6d4a70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,912:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e961f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e961f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,935:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa0692c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa0692c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,950:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e961f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e961f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,972:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9cd56d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9cd56d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:23,988:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e961f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e961f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,009:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa0692c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa0692c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,025:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e955560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e955560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,047:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9cd56d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9cd56d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,061:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9cd32b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9cd32b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,083:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e961f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e961f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,100:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e961f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e961f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,128:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9cd324d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9cd324d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,150:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e961f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e961f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,171:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e349dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e349dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,190:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e349c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e349c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,209:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9cd32560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9cd32560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,227:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9cd324d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9cd324d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,249:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e3493b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e3493b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,267:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c6d4680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c6d4680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,288:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9d466d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9d466d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,304:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c6c1710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9c6c1710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,324:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e349440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e349440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,345:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e349e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e349e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,367:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9d466f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9d466f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,382:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e349cb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e349cb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,405:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf99fa5dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf99fa5dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,420:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9cd56b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9cd56b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,441:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e349f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e349f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,459:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9d466b00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9d466b00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,481:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf99fa5f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf99fa5f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,498:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9b757950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9b757950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,519:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9cd56e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9cd56e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,537:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e954320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e954320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,559:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9b757a70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9b757a70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,575:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9cd56e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9cd56e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,598:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9cd32e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9cd32e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,617:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdfa198ec20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdfa198ec20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,638:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e349440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e349440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,654:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9b757200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9b757200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,677:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa198ea70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa198ea70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,692:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdfa198e5f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdfa198e5f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,713:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa19797a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa19797a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,729:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9d466560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9d466560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,752:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa198ecb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa198ecb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,768:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9b7579e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9b7579e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,789:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa19797a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa19797a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,808:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9b7574d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9b7574d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,831:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9bcfa9e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9bcfa9e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,847:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9bcfadd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9bcfadd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,869:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa19790e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa19790e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,886:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf99fa5680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf99fa5680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,909:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9bcfaa70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9bcfaa70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,926:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9cd32e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9cd32e60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,945:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e68bf80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e68bf80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,960:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e954170> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e954170> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,983:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9bcfa7a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9bcfa7a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:24,998:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9cd56b00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9cd56b00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:25,023:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e68bdd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e68bdd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:25,038:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf99fa5680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf99fa5680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:25,068:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9dc1e8c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9dc1e8c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:25,085:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9dc1ee60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9dc1ee60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:25,108:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e68bf80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9e68bf80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:25,126:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e68b320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9e68b320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:25,156:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa1979f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdfa1979f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:25,171:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdfa1979f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdfa1979f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:25,193:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9deedef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9deedef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:25,209:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9b7575f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.save_fn at 0x7fdf9b7575f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:25,229:tensorflow] AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9dc1e950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _trace_save_and_restore_function.<locals>.restore_fn at 0x7fdf9dc1e950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:30,916:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9cbc1200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:30,927:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9cbc5cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:30,939:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9ce3dc20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:30,950:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9ce4cb90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:30,960:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9ce4c680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:30,974:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9cbb1cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:30,984:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf99b61680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:30,994:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9ce545f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,005:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9a213a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,014:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9ce54e60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,023:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9cbc1950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,032:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9cbb1320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,042:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9a1f94d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,051:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9a22f950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,062:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9ce3db00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,072:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9cbc1170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,083:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9a213cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,096:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9a21d3b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,104:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9a21d7a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,113:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9a213320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,121:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9cbc55f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,132:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9a1f9a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,140:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9a201290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,148:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9a1f9e60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,159:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9cbc1440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,168:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9a22fb00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,177:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9a201cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,187:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf99c1b170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,197:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9a201710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,206:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9ce54680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,215:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9a1f9830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,230:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf99c1bb90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,242:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf99c1b680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,251:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9d9f2200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,260:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9a201a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,268:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9ce4c170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,279:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf99c100e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,289:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9cbc7a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,297:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9a21d200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:31,308:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9a219290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:32,101:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf99c10b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:32,112:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf99bfff80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:32,123:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf99c10290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:32,134:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9ce6bd40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:32,143:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9ce4c440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:32,152:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf99c0e9e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:32,162:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf99c0e7a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:32,170:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf99c0c320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:32,181:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf99c1b9e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:32,685:tensorflow] AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fdfa1be3830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fdfa1be3830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[WARNING] [2021-05-12 15:54:49,590:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9d6fbe60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,598:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9defab90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,609:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9df1d680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,622:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9d71a950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,630:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9d722b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,643:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9defa950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,652:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9defadd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,661:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9d6f0170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,673:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9df04200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,681:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9defb8c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,692:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9d702950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,701:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9d6fba70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,709:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9df04a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,718:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9d71a9e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,728:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9ceac170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,736:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9cea3ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,745:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9d6f98c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,754:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9df1d5f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,761:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9ceacb00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,770:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9ce90a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,779:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9ce90050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,788:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9df1d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,797:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9defae60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,810:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9cea59e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,818:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9cea3cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,827:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9df04680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,835:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9cea5ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,845:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9dc494d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,856:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9dc51ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,867:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9ce90dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,875:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9ceac0e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,884:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9cea3d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,895:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9dc5fa70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,905:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9dc5f8c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,912:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9d71a4d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,923:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9d702680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,933:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9dc660e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,946:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9dc68e60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,960:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9dc517a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,969:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9df17200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,978:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9dc66680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,987:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9dc60ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:49,996:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9dc5f440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:50,006:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9df1db90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:50,016:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9dc68560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:50,025:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9dcfa3b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:50,032:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9dd27ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:50,044:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9dcfaa70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:50,053:tensorflow] 11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdf9dc51b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[WARNING] [2021-05-12 15:54:50,554:tensorflow] AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fdf9da5bb00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fdf9da5bb00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: smac attempted to use a functionality that requires module skopt, but it couldn't be loaded. Please install skopt and retry.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "782/782 [==============================] - 13s 15ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "[0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import autokeras as ak\n",
    "\n",
    "data_train_features_labels_path = '/content/drive/MyDrive/CLF_of_ECG_signals/Data/train_features_labels.csv'\n",
    "data_val_features_labels_path = '/content/drive/MyDrive/CLF_of_ECG_signals/Data/val_features_labels.csv'\n",
    "\n",
    "# data_train_features_labels_path = tf.keras.utils.get_file(data_train_features_labels_path, )\n",
    "# data_val_features_labels_path = tf.keras.utils.get_file(data_val_features_labels_path, )\n",
    "\n",
    "# Initialize the structured data classifier.\n",
    "clf = ak.StructuredDataClassifier(\n",
    "    overwrite=True, max_trials=2\n",
    ")  # It tries 3 different models.\n",
    "# Feed the structured data classifier with training data.\n",
    "clf.fit(\n",
    "    # The path to the train.csv file.\n",
    "    data_train_features_labels_path,\n",
    "    # The name of the label column.\n",
    "    \"label\",\n",
    "    epochs=5,\n",
    ")\n",
    "# Predict with the best model.\n",
    "predicted_y = clf.predict(data_val_features_labels_path)\n",
    "# Evaluate the best model with testing data.\n",
    "print(clf.evaluate(data_val_features_labels_path, \"label\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbkeVOuwOERj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N06qLiSWyadM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "心跳信号分类_XCC.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "tianchi_metadata": {
   "competitions": [],
   "datasets": [
    {
     "id": "94250",
     "title": "零基础入门数据挖掘-心跳信号分类预测"
    },
    {
     "id": "94490",
     "title": "零基础入门数据挖掘-心跳信号分类预测"
    }
   ],
   "description": "",
   "notebookId": "185355",
   "source": "dsw"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "309.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "144px",
    "left": "996px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "17566945d6ab40829d27a2c02d70928e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a386842f8c04f018f1a6bc82671b03d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_532ecb5e555343449e2afef2f7c29971",
      "max": 16,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c5bd0614b2df48728cb82154f6560497",
      "value": 16
     }
    },
    "2058430a95174fd698ed6719a0dc4dec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3b215c73f1014eb9b62fd8bf54a80a6c",
       "IPY_MODEL_1a386842f8c04f018f1a6bc82671b03d",
       "IPY_MODEL_f7095d4380494a59a32950ef7ecfd50b"
      ],
      "layout": "IPY_MODEL_b9ad6debbbb04e3283f8947b4b36f646"
     }
    },
    "23604b8b0bdf48a281736e7b45fbc1c0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "242ab962063b43c0aa70095148e5476e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3b215c73f1014eb9b62fd8bf54a80a6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_17566945d6ab40829d27a2c02d70928e",
      "placeholder": "​",
      "style": "IPY_MODEL_242ab962063b43c0aa70095148e5476e",
      "value": "Summarize dataset: 100%"
     }
    },
    "471313f08acc429ea93aa0614117d019": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7220a2a95b8f423f90b2c71631aae85e",
      "placeholder": "​",
      "style": "IPY_MODEL_a3464d70e65740aabb37d9b4daaf2b41",
      "value": " 1/1 [00:00&lt;00:00,  1.59it/s]"
     }
    },
    "532ecb5e555343449e2afef2f7c29971": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "577ac48c48a740c9a2470507bb6c3a60": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58e5bab52f22484a91c48700156d7cb8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d5ebbb51bd741bcbed8d15a9983bf20": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e64d698c6c1457b9ea0754c5969567f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66cac038e50d4f2291bbd63409de6b05": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d5ebbb51bd741bcbed8d15a9983bf20",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7e96301d6b80451fa455bd143aa6f015",
      "value": 1
     }
    },
    "685edc46ad494e97babff3247f47858c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ca991303adf42318898ea2054235705",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6a281ccafcee46e9bddd670357e1fab9",
      "value": 1
     }
    },
    "6a281ccafcee46e9bddd670357e1fab9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7220a2a95b8f423f90b2c71631aae85e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ccc142146e847ae92869c5a6a3493a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7e96301d6b80451fa455bd143aa6f015": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7fda10bb8e1748f99aeb6fb6df5836f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8ca991303adf42318898ea2054235705": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "932ee9a6254547e483b877e6f6ee3f37": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "95320707fc8c4c1c85ca85509b222d0b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9579edb7b78f4ab595edf541942e2da7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "959b835107294c0992ef46127c7bea96": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_58e5bab52f22484a91c48700156d7cb8",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9579edb7b78f4ab595edf541942e2da7",
      "value": 1
     }
    },
    "9ce8864d9c19454692dedb787a4276a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9e4a003ecfa8410d9dc4abb882da22c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ddcf9de53741443baed426267ff6ecf1",
      "placeholder": "​",
      "style": "IPY_MODEL_7ccc142146e847ae92869c5a6a3493a7",
      "value": "Render HTML: 100%"
     }
    },
    "a3464d70e65740aabb37d9b4daaf2b41": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a85ffdd82f984c5694b7060afcb639e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac42e4c91e8c4962871b5c5ffea1e382": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b23942e6aaa04c0a92688689c6e7e8d2",
      "placeholder": "​",
      "style": "IPY_MODEL_932ee9a6254547e483b877e6f6ee3f37",
      "value": "Export report to file: 100%"
     }
    },
    "b23942e6aaa04c0a92688689c6e7e8d2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b35a6dedfd2d4c36bd59d2159515d988": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9ad6debbbb04e3283f8947b4b36f646": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bfda2581b5f64bc2b02c1932011cc239": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5bd0614b2df48728cb82154f6560497": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c9767270ac404f6d99a4c1465b69f36e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e064dce707a1430fa4dded5095c71204",
       "IPY_MODEL_959b835107294c0992ef46127c7bea96",
       "IPY_MODEL_fb0d4c2ca5a5431bbbd3515f70df71a0"
      ],
      "layout": "IPY_MODEL_cf4be2286ea84901873eccdd9a22dda9"
     }
    },
    "cb7885884585433ba2e95b16a76f2bf8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf4be2286ea84901873eccdd9a22dda9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ddcf9de53741443baed426267ff6ecf1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e064dce707a1430fa4dded5095c71204": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bfda2581b5f64bc2b02c1932011cc239",
      "placeholder": "​",
      "style": "IPY_MODEL_a85ffdd82f984c5694b7060afcb639e0",
      "value": "Generate report structure: 100%"
     }
    },
    "e7582ea428f44afa906452124c43ddb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_577ac48c48a740c9a2470507bb6c3a60",
      "placeholder": "​",
      "style": "IPY_MODEL_9ce8864d9c19454692dedb787a4276a5",
      "value": " 1/1 [00:00&lt;00:00,  2.71it/s]"
     }
    },
    "f7095d4380494a59a32950ef7ecfd50b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95320707fc8c4c1c85ca85509b222d0b",
      "placeholder": "​",
      "style": "IPY_MODEL_7fda10bb8e1748f99aeb6fb6df5836f3",
      "value": " 16/16 [00:25&lt;00:00,  1.04s/it, Completed]"
     }
    },
    "f8a653e1d4c54ee8bf86bd07c866bb65": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ac42e4c91e8c4962871b5c5ffea1e382",
       "IPY_MODEL_685edc46ad494e97babff3247f47858c",
       "IPY_MODEL_471313f08acc429ea93aa0614117d019"
      ],
      "layout": "IPY_MODEL_cb7885884585433ba2e95b16a76f2bf8"
     }
    },
    "fb0d4c2ca5a5431bbbd3515f70df71a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23604b8b0bdf48a281736e7b45fbc1c0",
      "placeholder": "​",
      "style": "IPY_MODEL_5e64d698c6c1457b9ea0754c5969567f",
      "value": " 1/1 [00:02&lt;00:00,  2.84s/it]"
     }
    },
    "fe4b0d91c3454918a9e45b7c69dc7c62": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9e4a003ecfa8410d9dc4abb882da22c0",
       "IPY_MODEL_66cac038e50d4f2291bbd63409de6b05",
       "IPY_MODEL_e7582ea428f44afa906452124c43ddb7"
      ],
      "layout": "IPY_MODEL_b35a6dedfd2d4c36bd59d2159515d988"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
